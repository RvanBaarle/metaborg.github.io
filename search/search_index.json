{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Spoofax Language Workbench \u00b6 Spoofax is a platform for developing textual (domain-specific) programming languages. The platform provides the following ingredients: Meta-languages for high-level declarative language definition An interactive environment for developing languages using these meta-languages Code generators that produces parsers, type checkers, compilers, interpreters, and other tools from language definitions Generation of full-featured Eclipse editor plugins from language definitions An API for programmatically combining the components of a language implementation With Spoofax you can focus on the essence of language definition and ignore irrelevant implementation details. Get started by downloading and installing Spoofax or build it from source .","title":"Spoofax"},{"location":"#the-spoofax-language-workbench","text":"Spoofax is a platform for developing textual (domain-specific) programming languages. The platform provides the following ingredients: Meta-languages for high-level declarative language definition An interactive environment for developing languages using these meta-languages Code generators that produces parsers, type checkers, compilers, interpreters, and other tools from language definitions Generation of full-featured Eclipse editor plugins from language definitions An API for programmatically combining the components of a language implementation With Spoofax you can focus on the essence of language definition and ignore irrelevant implementation details. Get started by downloading and installing Spoofax or build it from source .","title":"The Spoofax Language Workbench"},{"location":"getting-started/","text":"Getting started \u00b6 The quickest way to get started with Spoofax by downloading an instance of Eclipse with the latest release. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, use Homebrew on macOS, or download and build Spoofax from source. Installation \u00b6 The recommended way to get started with Spoofax is to download an Eclipse instance with the latest Spoofax plugin. The plugin also includes the Spoofax meta-languages. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, or download and build Spoofax from source. Choose the Eclipse Bundle installation (recommended) or the Eclipse Plugin installation: Eclipse Bundle Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Download Eclipse with Spoofax without an embedded JRE . Nightly releases . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew ( macOS) On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date. Quick Start \u00b6 Once installed, create a new Spoofax project: Right-click the Package Explorer , choose New \u2192 Project , and select Spoofax Language project from the Spoofax category. Provide a name for your new language and click Finish . Select the created language project and press Ctrl + Alt + B ( Cmd + Alt + B on macOS) to build the project. Create a new file with the extension registered to your language to test it. Follow one of the tutorials to learn more. Finding the filename extension of your language If you didn't explicitly specify a filename extension for your language, it is derived from the language name. You can find the filename extension for your language in editor/Main.esv at the extensions property.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"The quickest way to get started with Spoofax by downloading an instance of Eclipse with the latest release. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, use Homebrew on macOS, or download and build Spoofax from source.","title":"Getting started"},{"location":"getting-started/#installation","text":"The recommended way to get started with Spoofax is to download an Eclipse instance with the latest Spoofax plugin. The plugin also includes the Spoofax meta-languages. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, or download and build Spoofax from source. Choose the Eclipse Bundle installation (recommended) or the Eclipse Plugin installation: Eclipse Bundle Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Download Eclipse with Spoofax without an embedded JRE . Nightly releases . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew ( macOS) On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date.","title":"Installation"},{"location":"getting-started/#quick-start","text":"Once installed, create a new Spoofax project: Right-click the Package Explorer , choose New \u2192 Project , and select Spoofax Language project from the Spoofax category. Provide a name for your new language and click Finish . Select the created language project and press Ctrl + Alt + B ( Cmd + Alt + B on macOS) to build the project. Create a new file with the extension registered to your language to test it. Follow one of the tutorials to learn more. Finding the filename extension of your language If you didn't explicitly specify a filename extension for your language, it is derived from the language name. You can find the filename extension for your language in editor/Main.esv at the extensions property.","title":"Quick Start"},{"location":"nightly/","text":"Nightly Releases \u00b6 Use the nightly (development) releases of Spoofax only if you want to be on the cutting-edge of Spoofax development. Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the From Source installation: Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: http://buildfarm.metaborg.org/job/metaborg/job/spoofax-releng/job/master/lastSuccessfulBuild/artifact/dist/spoofax/eclipse/site/ Installation instructions . From Source Use Git to clone the Spoofax Github repository : HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Installation instructions .","title":"Nightly Releases"},{"location":"nightly/#nightly-releases","text":"Use the nightly (development) releases of Spoofax only if you want to be on the cutting-edge of Spoofax development. Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the From Source installation: Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: http://buildfarm.metaborg.org/job/metaborg/job/spoofax-releng/job/master/lastSuccessfulBuild/artifact/dist/spoofax/eclipse/site/ Installation instructions . From Source Use Git to clone the Spoofax Github repository : HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Installation instructions .","title":"Nightly Releases"},{"location":"stable/","text":"Stable Releases \u00b6 This page lists the stable releases of Spoofax. While this version is recommended for most users, there is also the nightly version. Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the Homebrew installation ( macOS only): Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date.","title":"Stable Releases"},{"location":"stable/#stable-releases","text":"This page lists the stable releases of Spoofax. While this version is recommended for most users, there is also the nightly version. Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the Homebrew installation ( macOS only): Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date.","title":"Stable Releases"},{"location":"background/","text":"Background \u00b6 This section contains information on the ideas, architecture, and design decisions behind Spoofax. For the Spoofax language reference, see the References section. Documentation No background yet.","title":"Background"},{"location":"background/#background","text":"This section contains information on the ideas, architecture, and design decisions behind Spoofax. For the Spoofax language reference, see the References section. Documentation No background yet.","title":"Background"},{"location":"background/documentation/","text":"Documentation \u00b6 This page explains the documentation's technology and structure, and how you can contribute. Technology \u00b6 This documentation uses MkDocs , a fast and simple static site generated that's geared towards building project documentation from Markdown files. In particular, this website uses MkDocs Material , which provides a clean look, easy customization, and many features for technical documentation. Structure \u00b6 The structure of this documentation follows the Grand Unified Theory of Documentation where documentation is split into four categories: Tutorials : oriented to learning , enabling newcomers to get started through a lesson, analogous to teaching a child how to cook. How-Tos : oriented to a particular goal , showing how to solve a specific problem through a series of steps, analogous to a recipe in a cookbook. Reference : oriented to information , describing the machinery through dry description, analogous to an encyclopaedia article. Background : oriented to understanding , explaining through discursive explanation, analogous to an article on culinary social history. Contributing \u00b6 Contributing to the documentation is easy. Quick changes and fixing typos can be done by clicking the button in the top-right corner of a page, and editing and saving the underlying Markdown file. More considerable contributions can be made by cloning this repository locally, and editing the Markdown files there. The easiest way to get a live preview (automatically reloading) of your changes, is by installing Docker and executing make from the root directory. This will serve the latest changes to localhost:8000 . MkDocs Reference Extensions Reference Adding Pages \u00b6 The first page mentioned in nav under a section should be some index.md (without a title), and will be used as the index page (home page) for that section. When you add a new page, don't forget to add it to the nav element in the mkdocs.yml file, or it will not show up. Links \u00b6 Links to other Markdown pages should be written as relative links. For example, to link to tutorials from the background/index.md page, write the relative link including the Markdown file: ```markdown [Tutorials](../tutorials/index.md) ``` Absolute Links are Not Supported Absolute links are not supported, and while they may work locally, they break in production. Citations \u00b6 To cite a paper or work, first ensure the citation is in a bibliography ( .bib ) file in the /bibliographies/ directory. For example, in the bibliographies/spoofax.bib file, we find: @inproceedings { KatsV10 , title = {The {Spoofax} language workbench: rules for declarative specification of languages and {IDEs}} , author = {Lennart C. L. Kats and Eelco Visser} , year = {2010} , doi = {10.1145/1869459.1869497} , url = {https://doi.org/10.1145/1869459.1869497} , pages = {444-463} , booktitle = {Proceedings of the 25th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010} , } Adding References To add a reference, add it on Researchr to the Spoofax bibliography . Then on the command-line, invoke the following to regenerate the spoofax.bib file: make bib Do not change the spoofax.bib file manually, it is generated and updated through Researchr . Then reference the work like this: The Spoofax language workbench[@KatsV10] is vital to declarative language development. Finally, add a place for the bibliography footnotes to be added (usually at the end of the file) by adding the following line to the file: \\bibliography The line will be rendered as: The Spoofax language workbench 1 is vital to declarative language development. And the references will be at the bottom of this page. If the citation appears rendered as Spoofax language workbench[^1] , then you might have forgotten to add a place for the bibliography. Technical Details \u00b6 The structure of the documentation repository is as follows (hover over any of the files to see its description): \ud83d\udce6 / \u2523 \ud83d\udcc1 .github \u2523 \ud83d\udcc2 docs \u2503 \u2523 \ud83d\udcc2 assets \u2503 \u2503 \u2523 \ud83d\udcdc favicon.png \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-dark.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-light.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero.svg \u2503 \u2503 \u2523 \ud83d\udcdc logo.svg \u2503 \u2503 \u2517 \ud83d\udcdc styles.css \u2503 \u2523 \ud83d\udcc2 background \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2523 \ud83d\udcc2 howtos \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2523 \ud83d\udcc2 reference \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2523 \ud83d\udcc2 tutorials \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2517 \ud83d\udcdc index.md \u2523 \ud83d\udcc1 overrides \u2503 \u2523 \ud83d\udcdc index.html \u2503 \u2517 \ud83d\udcdc main.html \u2523 \ud83d\udcdc .gitignore \u2523 \ud83d\udcdc Dockerfile \u2523 \ud83d\udcdc LICENSE \u2523 \ud83d\udcdc Makefile \u2523 \ud83d\udcdc mkdocs_requirements.txt \u2523 \ud83d\udcdc mkdocs.yml \u2517 \ud83d\udcdc README.md Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9","title":"Documentation"},{"location":"background/documentation/#documentation","text":"This page explains the documentation's technology and structure, and how you can contribute.","title":"Documentation"},{"location":"background/documentation/#technology","text":"This documentation uses MkDocs , a fast and simple static site generated that's geared towards building project documentation from Markdown files. In particular, this website uses MkDocs Material , which provides a clean look, easy customization, and many features for technical documentation.","title":"Technology"},{"location":"background/documentation/#structure","text":"The structure of this documentation follows the Grand Unified Theory of Documentation where documentation is split into four categories: Tutorials : oriented to learning , enabling newcomers to get started through a lesson, analogous to teaching a child how to cook. How-Tos : oriented to a particular goal , showing how to solve a specific problem through a series of steps, analogous to a recipe in a cookbook. Reference : oriented to information , describing the machinery through dry description, analogous to an encyclopaedia article. Background : oriented to understanding , explaining through discursive explanation, analogous to an article on culinary social history.","title":"Structure"},{"location":"background/documentation/#contributing","text":"Contributing to the documentation is easy. Quick changes and fixing typos can be done by clicking the button in the top-right corner of a page, and editing and saving the underlying Markdown file. More considerable contributions can be made by cloning this repository locally, and editing the Markdown files there. The easiest way to get a live preview (automatically reloading) of your changes, is by installing Docker and executing make from the root directory. This will serve the latest changes to localhost:8000 . MkDocs Reference Extensions Reference","title":"Contributing"},{"location":"background/documentation/#adding-pages","text":"The first page mentioned in nav under a section should be some index.md (without a title), and will be used as the index page (home page) for that section. When you add a new page, don't forget to add it to the nav element in the mkdocs.yml file, or it will not show up.","title":"Adding Pages"},{"location":"background/documentation/#links","text":"Links to other Markdown pages should be written as relative links. For example, to link to tutorials from the background/index.md page, write the relative link including the Markdown file: ```markdown [Tutorials](../tutorials/index.md) ``` Absolute Links are Not Supported Absolute links are not supported, and while they may work locally, they break in production.","title":"Links"},{"location":"background/documentation/#citations","text":"To cite a paper or work, first ensure the citation is in a bibliography ( .bib ) file in the /bibliographies/ directory. For example, in the bibliographies/spoofax.bib file, we find: @inproceedings { KatsV10 , title = {The {Spoofax} language workbench: rules for declarative specification of languages and {IDEs}} , author = {Lennart C. L. Kats and Eelco Visser} , year = {2010} , doi = {10.1145/1869459.1869497} , url = {https://doi.org/10.1145/1869459.1869497} , pages = {444-463} , booktitle = {Proceedings of the 25th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010} , } Adding References To add a reference, add it on Researchr to the Spoofax bibliography . Then on the command-line, invoke the following to regenerate the spoofax.bib file: make bib Do not change the spoofax.bib file manually, it is generated and updated through Researchr . Then reference the work like this: The Spoofax language workbench[@KatsV10] is vital to declarative language development. Finally, add a place for the bibliography footnotes to be added (usually at the end of the file) by adding the following line to the file: \\bibliography The line will be rendered as: The Spoofax language workbench 1 is vital to declarative language development. And the references will be at the bottom of this page. If the citation appears rendered as Spoofax language workbench[^1] , then you might have forgotten to add a place for the bibliography.","title":"Citations"},{"location":"background/documentation/#technical-details","text":"The structure of the documentation repository is as follows (hover over any of the files to see its description): \ud83d\udce6 / \u2523 \ud83d\udcc1 .github \u2523 \ud83d\udcc2 docs \u2503 \u2523 \ud83d\udcc2 assets \u2503 \u2503 \u2523 \ud83d\udcdc favicon.png \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-dark.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-light.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero.svg \u2503 \u2503 \u2523 \ud83d\udcdc logo.svg \u2503 \u2503 \u2517 \ud83d\udcdc styles.css \u2503 \u2523 \ud83d\udcc2 background \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2523 \ud83d\udcc2 howtos \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2523 \ud83d\udcc2 reference \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2523 \ud83d\udcc2 tutorials \u2503 \u2503 \u2517 \ud83d\udcdc index.md \u2503 \u2517 \ud83d\udcdc index.md \u2523 \ud83d\udcc1 overrides \u2503 \u2523 \ud83d\udcdc index.html \u2503 \u2517 \ud83d\udcdc main.html \u2523 \ud83d\udcdc .gitignore \u2523 \ud83d\udcdc Dockerfile \u2523 \ud83d\udcdc LICENSE \u2523 \ud83d\udcdc Makefile \u2523 \ud83d\udcdc mkdocs_requirements.txt \u2523 \ud83d\udcdc mkdocs.yml \u2517 \ud83d\udcdc README.md Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9","title":"Technical Details"},{"location":"background/bibliography/stratego/","text":"A Stratego Bibliography \u00b6 The original publication on Stratego appeared in ICFP'98 1 and introduced named rewrite rules and a language of strategy combinators. The paper also introduced contextual terms. These where eventually replaced by dynamic rewrite rules 2 . core language 3 System descriptions Stratego 0.5 4 , Stratego/XT 0.16 [@Bravenboer], Stratego/XT 0.17 [@Bravenboer] gradual typing 5 References \u00b6 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Martin Bravenboer, Arthur van Dam, Karina Olmos, and Eelco Visser. Program transformation with scoped dynamic rewrite rules. Fundamenta Informaticae , 69(1-2):123\u2013178, 2006. URL: https://content.iospress.com/articles/fundamenta-informaticae/fi69-1-2-06 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9 Eelco Visser. Stratego: a language for program transformation based on rewriting strategies. In Aart Middeldorp, editor, Rewriting Techniques and Applications, 12 th International Conference, RTA 2001, Utrecht, The Netherlands, May 22-24, 2001, Proceedings , volume 2051 of Lecture Notes in Computer Science, 357\u2013362. Springer, 2001. URL: https://doi.org/10.1007/3-540-45127-7_27 , doi:10.1007/3-540-45127-7_27 . \u21a9 Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9","title":"A Stratego Bibliography"},{"location":"background/bibliography/stratego/#a-stratego-bibliography","text":"The original publication on Stratego appeared in ICFP'98 1 and introduced named rewrite rules and a language of strategy combinators. The paper also introduced contextual terms. These where eventually replaced by dynamic rewrite rules 2 . core language 3 System descriptions Stratego 0.5 4 , Stratego/XT 0.16 [@Bravenboer], Stratego/XT 0.17 [@Bravenboer] gradual typing 5","title":"A Stratego Bibliography"},{"location":"background/bibliography/stratego/#references","text":"Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Martin Bravenboer, Arthur van Dam, Karina Olmos, and Eelco Visser. Program transformation with scoped dynamic rewrite rules. Fundamenta Informaticae , 69(1-2):123\u2013178, 2006. URL: https://content.iospress.com/articles/fundamenta-informaticae/fi69-1-2-06 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9 Eelco Visser. Stratego: a language for program transformation based on rewriting strategies. In Aart Middeldorp, editor, Rewriting Techniques and Applications, 12 th International Conference, RTA 2001, Utrecht, The Netherlands, May 22-24, 2001, Proceedings , volume 2051 of Lecture Notes in Computer Science, 357\u2013362. Springer, 2001. URL: https://doi.org/10.1007/3-540-45127-7_27 , doi:10.1007/3-540-45127-7_27 . \u21a9 Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9","title":"References"},{"location":"background/statix/","text":"Statix Background \u00b6 rule selection open/closed world reasoning ( try /DWF/DLeq) desugaring of functional rules Internal representation of scope graphs Query Scheduling/Permission to Extend","title":"Statix Background"},{"location":"background/statix/#statix-background","text":"rule selection open/closed world reasoning ( try /DWF/DLeq) desugaring of functional rules Internal representation of scope graphs Query Scheduling/Permission to Extend","title":"Statix Background"},{"location":"howtos/","text":"How-To's \u00b6 These are some How-To's that help you to get to a specific goal or result with Spoofax. For hands-on tutorials on learning Spoofax, see the Tutorials section. For the Spoofax languages references, see the References section. Installation and Build \u00b6 Install the Eclipse with Spoofax Plugin Bundle Install the Spoofax Eclipse Plugin Manually Install Spoofax from Source","title":"How-To's"},{"location":"howtos/#how-tos","text":"These are some How-To's that help you to get to a specific goal or result with Spoofax. For hands-on tutorials on learning Spoofax, see the Tutorials section. For the Spoofax languages references, see the References section.","title":"How-To's"},{"location":"howtos/#installation-and-build","text":"Install the Eclipse with Spoofax Plugin Bundle Install the Spoofax Eclipse Plugin Manually Install Spoofax from Source","title":"Installation and Build"},{"location":"howtos/install-eclipse-bundle/","text":"Install the Eclipse with Spoofax Plugin Bundle \u00b6 Install an Eclipse instance with the latest stable release of the Spoofax plugin pre-installed for your platform: Eclipse with JRE (recommended) Eclipse bundle including the Spoofax plugin with embedded Java Runtime Environment (JRE) (recommended): + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Eclipse Eclipse bundle including the Spoofax plugin ( no embedded JRE ): macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Nightly releases . Troubleshooting \u00b6 macOS: \"Eclipse\" cannot be opened because the developer could not be verified \u00b6 macOS puts unverified binaries in 'quarantine' and disallows their execution. To remove the com.apple.quarantine attribute, do: xattr -rc Eclipse.app Eclipse does not start, or complains about missing Java \u00b6 Download the Eclipse bundle with embedded JRE . Otherwise, ensure you have a distribution of Java installed. Then in eclipse.ini , add a -vm line at the top of the file, followed by the path to the Java installation. For example, with SDKMan! on macOS: -vm /Users/myusername/.sdkman/candidates/java/current/jre/lib/jli/libjli.dylib","title":"Install the Eclipse with Spoofax Plugin Bundle"},{"location":"howtos/install-eclipse-bundle/#install-the-eclipse-with-spoofax-plugin-bundle","text":"Install an Eclipse instance with the latest stable release of the Spoofax plugin pre-installed for your platform: Eclipse with JRE (recommended) Eclipse bundle including the Spoofax plugin with embedded Java Runtime Environment (JRE) (recommended): + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Eclipse Eclipse bundle including the Spoofax plugin ( no embedded JRE ): macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Nightly releases .","title":"Install the Eclipse with Spoofax Plugin Bundle"},{"location":"howtos/install-eclipse-bundle/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"howtos/install-eclipse-bundle/#macos-eclipse-cannot-be-opened-because-the-developer-could-not-be-verified","text":"macOS puts unverified binaries in 'quarantine' and disallows their execution. To remove the com.apple.quarantine attribute, do: xattr -rc Eclipse.app","title":" macOS: \"Eclipse\" cannot be opened because the developer could not be verified"},{"location":"howtos/install-eclipse-bundle/#eclipse-does-not-start-or-complains-about-missing-java","text":"Download the Eclipse bundle with embedded JRE . Otherwise, ensure you have a distribution of Java installed. Then in eclipse.ini , add a -vm line at the top of the file, followed by the path to the Java installation. For example, with SDKMan! on macOS: -vm /Users/myusername/.sdkman/candidates/java/current/jre/lib/jli/libjli.dylib","title":"Eclipse does not start, or complains about missing Java"},{"location":"howtos/install-eclipse-plugin-manually/","text":"Install the Spoofax Eclipse Plugin Manually \u00b6 Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer. In Eclipse, go to menu Help \u2192 Install New Software . In the Work with: text area, type: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ ( Nightly releases ). Uncheck Group items by category to make the plugin visible. Check Spoofax Eclipse meta-tooling , Spoofax Eclipse meta-tooling M2E integration and Spoofax Eclipse runtime . Click Install and go through the remaining steps. Restart Eclipse.","title":"Install the Spoofax Eclipse Plugin Manually"},{"location":"howtos/install-eclipse-plugin-manually/#install-the-spoofax-eclipse-plugin-manually","text":"Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer. In Eclipse, go to menu Help \u2192 Install New Software . In the Work with: text area, type: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ ( Nightly releases ). Uncheck Group items by category to make the plugin visible. Check Spoofax Eclipse meta-tooling , Spoofax Eclipse meta-tooling M2E integration and Spoofax Eclipse runtime . Click Install and go through the remaining steps. Restart Eclipse.","title":"Install the Spoofax Eclipse Plugin Manually"},{"location":"howtos/install-from-source/","text":"Install Spoofax from Source \u00b6 Perform a manual build and installation of cutting-edge Spoofax from source, by first cloning the Git repository: HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Then: Using a terminal, navigate to the root of the spoofax-releng repository. (Optional.) Generate a new Maven ~/.m2/settings.xml with the Spoofax repository information. ./b gen-mvn-settings This will overwrite your existing ~/.m2/settings.xml file! Invoke the following command to build Spoofax and its submodules and meta-languages: ./b build all (Optional.) Generate a new Eclipse instance with the Spoofax plugin embedded into it: ./b gen-eclipse --destination Spoofax.app","title":"Install Spoofax from Source"},{"location":"howtos/install-from-source/#install-spoofax-from-source","text":"Perform a manual build and installation of cutting-edge Spoofax from source, by first cloning the Git repository: HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Then: Using a terminal, navigate to the root of the spoofax-releng repository. (Optional.) Generate a new Maven ~/.m2/settings.xml with the Spoofax repository information. ./b gen-mvn-settings This will overwrite your existing ~/.m2/settings.xml file! Invoke the following command to build Spoofax and its submodules and meta-languages: ./b build all (Optional.) Generate a new Eclipse instance with the Spoofax plugin embedded into it: ./b gen-eclipse --destination Spoofax.app","title":"Install Spoofax from Source"},{"location":"howtos/editor-services/rename-refactoring/","text":"Add Rename Refactoring to an Existing Project \u00b6 Rename Refactoring is the ability for the user to select a reference or declaration and rename it to across the whole program while not introducing errors and not touching syntactically equal names. Renaming in Statix \u00b6 To enable the Rename Refactoring for an existing Spoofax Language project that uses Statix, create an action that calls the rename-action strategy from the statixruntime library. The parameters are explained in the reference . For example: module renaming imports statixruntime statix / runtime / renaming pp analysis rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id ) Renaming in NaBL2 \u00b6 There also exists a version of the Rename refactoring that works with languages using NaBL2. It can be added with a Stratego module like this: module renaming imports nabl2 / runtime pp analysis rules rename-menu-action = nabl2-rename-action ( construct-textual-change , editor-analyze , id ) Menu Action \u00b6 The rename refactoring is triggered from an entry in the Spoofax menu. To add it to an existing project a menu like the following can be implemented in an ESV file : module Refactoring menus menu : \"Refactoring\" action : \"Rename\" = re name - menu - action See Also \u00b6 Reference: Rename Refactoring","title":"Add Rename Refactoring to an Existing Project"},{"location":"howtos/editor-services/rename-refactoring/#add-rename-refactoring-to-an-existing-project","text":"Rename Refactoring is the ability for the user to select a reference or declaration and rename it to across the whole program while not introducing errors and not touching syntactically equal names.","title":"Add Rename Refactoring to an Existing Project"},{"location":"howtos/editor-services/rename-refactoring/#renaming-in-statix","text":"To enable the Rename Refactoring for an existing Spoofax Language project that uses Statix, create an action that calls the rename-action strategy from the statixruntime library. The parameters are explained in the reference . For example: module renaming imports statixruntime statix / runtime / renaming pp analysis rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id )","title":"Renaming in Statix"},{"location":"howtos/editor-services/rename-refactoring/#renaming-in-nabl2","text":"There also exists a version of the Rename refactoring that works with languages using NaBL2. It can be added with a Stratego module like this: module renaming imports nabl2 / runtime pp analysis rules rename-menu-action = nabl2-rename-action ( construct-textual-change , editor-analyze , id )","title":"Renaming in NaBL2"},{"location":"howtos/editor-services/rename-refactoring/#menu-action","text":"The rename refactoring is triggered from an entry in the Spoofax menu. To add it to an existing project a menu like the following can be implemented in an ESV file : module Refactoring menus menu : \"Refactoring\" action : \"Rename\" = re name - menu - action","title":"Menu Action"},{"location":"howtos/editor-services/rename-refactoring/#see-also","text":"Reference: Rename Refactoring","title":"See Also"},{"location":"howtos/stratego/debug-stratego/","text":"How to Debug Stratego Programs \u00b6 Types \u00b6 With \u00b6 Debug \u00b6 debug ( ! \"\" )","title":"How to Debug Stratego Programs"},{"location":"howtos/stratego/debug-stratego/#how-to-debug-stratego-programs","text":"","title":"How to Debug Stratego Programs"},{"location":"howtos/stratego/debug-stratego/#types","text":"","title":"Types"},{"location":"howtos/stratego/debug-stratego/#with","text":"","title":"With"},{"location":"howtos/stratego/debug-stratego/#debug","text":"debug ( ! \"\" )","title":"Debug"},{"location":"howtos/stratego/exchange-terms/","text":"How to Exchange Terms \u00b6 The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer.","title":"How to Exchange Terms"},{"location":"howtos/stratego/exchange-terms/#how-to-exchange-terms","text":"The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer.","title":"How to Exchange Terms"},{"location":"howtos/stratego/inspect-terms/","text":"How To Inspect Terms \u00b6 As a Stratego programmer you will be looking a lot at raw ATerms. Stratego pioneers did this by opening an ATerm file in emacs and trying to get a sense of the structure by parenthesis highlighting and inserting newlines here and there. These days your life is much more pleasant through pretty-printing ATerms, which adds layout to a term to make it readable. For example, parsing the following program let function fact(n : int) : int = if n < 1 then 1 else (n * fact(n - 1)) in printint(fact(10)) end produces the following ATerm: Let([FunDecs([FunDec(\"fact\",[FArg(\"n\",Tp(Tid(\"int\")))],Tp(Tid(\"int\")), If(Lt(Var(\"n\"),Int(\"1\")),Int(\"1\"),Seq([Times(Var(\"n\"),Call(Var(\"fact\"), [Minus(Var(\"n\"),Int(\"1\"))]))])))])],[Call(Var(\"printint\"),[Call(Var( \"fact\"),[Int(\"10\")])])]) By pretty-printing the term we get a much more readable term: Let( [ FunDecs( [ FunDec( \"fact\" , [FArg(\"n\", Tp(Tid(\"int\")))] , Tp(Tid(\"int\")) , If( Lt(Var(\"n\"), Int(\"1\")) , Int(\"1\") , Seq([ Times(Var(\"n\"), Call(Var(\"fact\"), [Minus(Var(\"n\"), Int(\"1\"))])) ]) ) ) ] ) ] , [ Call(Var(\"printint\"), [Call(Var(\"fact\"), [Int(\"10\")])]) ] ) In Spoofax/Eclipse, you will find that in some contexts ATerms are automatically pretty-printed, whereas in others they are simply printed linearly. However, you can obtain assistance with perceiving the structure of any ATerm by writing it into a file with the \".aterm\" extension and opening it in the Spoofax Editor in Eclipse. On the right there will be a convenient Outline Navigator which allows you to select any node in the ATerm and see the entire subtree below it highlighted in the editor.","title":"How To Inspect Terms"},{"location":"howtos/stratego/inspect-terms/#how-to-inspect-terms","text":"As a Stratego programmer you will be looking a lot at raw ATerms. Stratego pioneers did this by opening an ATerm file in emacs and trying to get a sense of the structure by parenthesis highlighting and inserting newlines here and there. These days your life is much more pleasant through pretty-printing ATerms, which adds layout to a term to make it readable. For example, parsing the following program let function fact(n : int) : int = if n < 1 then 1 else (n * fact(n - 1)) in printint(fact(10)) end produces the following ATerm: Let([FunDecs([FunDec(\"fact\",[FArg(\"n\",Tp(Tid(\"int\")))],Tp(Tid(\"int\")), If(Lt(Var(\"n\"),Int(\"1\")),Int(\"1\"),Seq([Times(Var(\"n\"),Call(Var(\"fact\"), [Minus(Var(\"n\"),Int(\"1\"))]))])))])],[Call(Var(\"printint\"),[Call(Var( \"fact\"),[Int(\"10\")])])]) By pretty-printing the term we get a much more readable term: Let( [ FunDecs( [ FunDec( \"fact\" , [FArg(\"n\", Tp(Tid(\"int\")))] , Tp(Tid(\"int\")) , If( Lt(Var(\"n\"), Int(\"1\")) , Int(\"1\") , Seq([ Times(Var(\"n\"), Call(Var(\"fact\"), [Minus(Var(\"n\"), Int(\"1\"))])) ]) ) ) ] ) ] , [ Call(Var(\"printint\"), [Call(Var(\"fact\"), [Int(\"10\")])]) ] ) In Spoofax/Eclipse, you will find that in some contexts ATerms are automatically pretty-printed, whereas in others they are simply printed linearly. However, you can obtain assistance with perceiving the structure of any ATerm by writing it into a file with the \".aterm\" extension and opening it in the Spoofax Editor in Eclipse. On the right there will be a convenient Outline Navigator which allows you to select any node in the ATerm and see the entire subtree below it highlighted in the editor.","title":"How To Inspect Terms"},{"location":"howtos/stratego/run-stratego-programs/","text":"How to Run Stratego Programs \u00b6","title":"How to Run Stratego Programs"},{"location":"howtos/stratego/run-stratego-programs/#how-to-run-stratego-programs","text":"","title":"How to Run Stratego Programs"},{"location":"references/","text":"References \u00b6 This are the Spoofax and meta-language references. For more background information on the ideas, architecture, and design decisions behind Spoofax and its meta-languages, see the Background section. The reference section should explain language constructs (syntax, statics, dynamics) and be structured following a taxonomy of the language Table of Contents \u00b6 SDF3 (Jasper) Statix (Aron) FlowSpec (Matthijs, Jeff) Stratego (Eelco, Jeff) PIE (Ivo, Gabri\u00ebl) MkDocs (Dani\u00ebl) bibtex syntax highlighting ESV / editor services Reviewing Peter Toine","title":"References"},{"location":"references/#references","text":"This are the Spoofax and meta-language references. For more background information on the ideas, architecture, and design decisions behind Spoofax and its meta-languages, see the Background section. The reference section should explain language constructs (syntax, statics, dynamics) and be structured following a taxonomy of the language","title":"References"},{"location":"references/#table-of-contents","text":"SDF3 (Jasper) Statix (Aron) FlowSpec (Matthijs, Jeff) Stratego (Eelco, Jeff) PIE (Ivo, Gabri\u00ebl) MkDocs (Dani\u00ebl) bibtex syntax highlighting ESV / editor services Reviewing Peter Toine","title":"Table of Contents"},{"location":"references/config/","text":"Language Configuration \u00b6 metaborg.yaml","title":"Language Configuration"},{"location":"references/config/#language-configuration","text":"metaborg.yaml","title":"Language Configuration"},{"location":"references/editor-services/","text":"Editor Services \u00b6 Most editor services are configured in an ESV file . This way the following editor services can be defined: Action Menus Analysis File Extensions Hover Tooltips On-Save Handlers Outline View Parsing Reference Resolution Stratego Strategies Syntax Highlighting Additionally, the following editor services are configured in a different way: Rename Refactoring","title":"Editor Services"},{"location":"references/editor-services/#editor-services","text":"Most editor services are configured in an ESV file . This way the following editor services can be defined: Action Menus Analysis File Extensions Hover Tooltips On-Save Handlers Outline View Parsing Reference Resolution Stratego Strategies Syntax Highlighting Additionally, the following editor services are configured in a different way: Rename Refactoring","title":"Editor Services"},{"location":"references/editor-services/analysis/","text":"Analysis \u00b6 The analyzer strategy is used to perform static analyses such as name and type analysis, on the AST that a parser produces. An analysis context provides a project-wide store to facilitate multi-file analysis and incrementality. There are four ways to configure the analysis, which set the analyzer strategy with the observer and context keys in an ESV file . language context : $Context observer : $Strategy No Analysis \u00b6 To completely disable analysis, do not set an observer and set the context to none: language context : none Stratego \u00b6 Stratego-based analysis allows you to implement your analysis in Stratego: language context : legacy observer : editor-analyze The identifier after the colon refers to the Stratego strategy that performs the analysis. It must take as input a 3-tuple (ast, path, projectPath) . As output it must produce a 4-tuple (ast, error*, warning*, note*) . The following Stratego code is an example of a strategy that implements this signature: editor-analyze : ( ast , path , projectPath ) - > ( ast ' , errors , warnings , notes ) with ast ' := < analyze > ast ; errors := < collect-all ( check-error )> ast ' ; warnings := < collect-all ( check-warning )> ast ' ; notes := < collect-all ( check-note )> ast ' Statix \u00b6 To use Statix as the meta-language for name and type analysis, use the editor-analyze strategy defined in trans/analysis.str , annotate it with the (constraint) modifier, and set no context: language observer : editor-analyze ( constraint ) By default, the Statix analyzer works in single-file mode and does not consider multi-file name resolution. To enable that, add the (multifile) modifier: language observer : editor-analyze ( constraint ) ( multifile )","title":"Analysis"},{"location":"references/editor-services/analysis/#analysis","text":"The analyzer strategy is used to perform static analyses such as name and type analysis, on the AST that a parser produces. An analysis context provides a project-wide store to facilitate multi-file analysis and incrementality. There are four ways to configure the analysis, which set the analyzer strategy with the observer and context keys in an ESV file . language context : $Context observer : $Strategy","title":"Analysis"},{"location":"references/editor-services/analysis/#no-analysis","text":"To completely disable analysis, do not set an observer and set the context to none: language context : none","title":"No Analysis"},{"location":"references/editor-services/analysis/#stratego","text":"Stratego-based analysis allows you to implement your analysis in Stratego: language context : legacy observer : editor-analyze The identifier after the colon refers to the Stratego strategy that performs the analysis. It must take as input a 3-tuple (ast, path, projectPath) . As output it must produce a 4-tuple (ast, error*, warning*, note*) . The following Stratego code is an example of a strategy that implements this signature: editor-analyze : ( ast , path , projectPath ) - > ( ast ' , errors , warnings , notes ) with ast ' := < analyze > ast ; errors := < collect-all ( check-error )> ast ' ; warnings := < collect-all ( check-warning )> ast ' ; notes := < collect-all ( check-note )> ast '","title":"Stratego"},{"location":"references/editor-services/analysis/#statix","text":"To use Statix as the meta-language for name and type analysis, use the editor-analyze strategy defined in trans/analysis.str , annotate it with the (constraint) modifier, and set no context: language observer : editor-analyze ( constraint ) By default, the Statix analyzer works in single-file mode and does not consider multi-file name resolution. To enable that, add the (multifile) modifier: language observer : editor-analyze ( constraint ) ( multifile )","title":"Statix"},{"location":"references/editor-services/esv/","text":"ESV \u00b6 The Editor Service (ESV) language is a declarative meta-language for configuring the editor services of a language. For example, the following ESV code fragment configures the syntax highlighting for a language, based on the types of tokens: module color colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic Structure \u00b6 ESV files end with the .esv extension, and are by convention placed in the editor/ folder of a language project. Each ESV file defines a module for the file, followed by import statements and then the main configuration sections. Each section consists of a number of keys and values. Main File By convention, the main ESV file of a language project must live at editor/Main.esv (default) or editor/main.esv . Other ESV files can be (transitively) imported from the main ESV file. Module Definition \u00b6 An ESV file starts with a module definition at the top of the file: module $ModuleName The module name is the filename of the ESV file without the exttension, and relative to the editor/ directory. For example, the module editor/mylang/Syntax.esv would have the following module name: module mylang/Syntax Module names can only contains the alphanumeric characters and dash, underscore, and period, and use the forward slash ( / ) as the path separator. Module names cannot be in parent directories, so ../Syntax is not allowed. Imports \u00b6 The imports section is an optional section immediately following the module definition. When specified it is given as: imports $Imports For example, to import editor/Syntax.esv and editor/Analysis.esv : imports Syntax Analysis Imports are transitive. At most one imports section is permitted. When specified, the imports section cannot be empty. Configuration Sections \u00b6 The main body of an ESV file consists of any number of configuration sections. An example of a configuration section is: language line comment : \"//\" block comment : \"/*\" \"*/\" The configuration sections are hard-coded in the ESV language, but mostly use a consistent syntax for the keys and values. The following configuration sections are currently defined: colorer Syntax Highlighting language Language File Extensions Parsing Analysis On-Save Handlers Stratego Strategies menus Action menus references Hover Tooltips Reference Resolutions views Outline View The following sections have been deprecated: analysis builders completions folding outliner refactorings","title":"ESV"},{"location":"references/editor-services/esv/#esv","text":"The Editor Service (ESV) language is a declarative meta-language for configuring the editor services of a language. For example, the following ESV code fragment configures the syntax highlighting for a language, based on the types of tokens: module color colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic","title":"ESV"},{"location":"references/editor-services/esv/#structure","text":"ESV files end with the .esv extension, and are by convention placed in the editor/ folder of a language project. Each ESV file defines a module for the file, followed by import statements and then the main configuration sections. Each section consists of a number of keys and values. Main File By convention, the main ESV file of a language project must live at editor/Main.esv (default) or editor/main.esv . Other ESV files can be (transitively) imported from the main ESV file.","title":"Structure"},{"location":"references/editor-services/esv/#module-definition","text":"An ESV file starts with a module definition at the top of the file: module $ModuleName The module name is the filename of the ESV file without the exttension, and relative to the editor/ directory. For example, the module editor/mylang/Syntax.esv would have the following module name: module mylang/Syntax Module names can only contains the alphanumeric characters and dash, underscore, and period, and use the forward slash ( / ) as the path separator. Module names cannot be in parent directories, so ../Syntax is not allowed.","title":"Module Definition"},{"location":"references/editor-services/esv/#imports","text":"The imports section is an optional section immediately following the module definition. When specified it is given as: imports $Imports For example, to import editor/Syntax.esv and editor/Analysis.esv : imports Syntax Analysis Imports are transitive. At most one imports section is permitted. When specified, the imports section cannot be empty.","title":"Imports"},{"location":"references/editor-services/esv/#configuration-sections","text":"The main body of an ESV file consists of any number of configuration sections. An example of a configuration section is: language line comment : \"//\" block comment : \"/*\" \"*/\" The configuration sections are hard-coded in the ESV language, but mostly use a consistent syntax for the keys and values. The following configuration sections are currently defined: colorer Syntax Highlighting language Language File Extensions Parsing Analysis On-Save Handlers Stratego Strategies menus Action menus references Hover Tooltips Reference Resolutions views Outline View The following sections have been deprecated: analysis builders completions folding outliner refactorings","title":"Configuration Sections"},{"location":"references/editor-services/file-extensions/","text":"Language File Extensions \u00b6 The file extensions that the editor should recognize as files belonging to the language definition, are configured in the language section extensions key of an ESV file . They are specified without a leading dot: language extensions : ent Multiple extensions can be set with a comma-separated list: language extensions : ent , entity , entities This will assign for example foo.ent , foo.entity , and foo.entities to the language.","title":"Language File Extensions"},{"location":"references/editor-services/file-extensions/#language-file-extensions","text":"The file extensions that the editor should recognize as files belonging to the language definition, are configured in the language section extensions key of an ESV file . They are specified without a leading dot: language extensions : ent Multiple extensions can be set with a comma-separated list: language extensions : ent , entity , entities This will assign for example foo.ent , foo.entity , and foo.entities to the language.","title":"Language File Extensions"},{"location":"references/editor-services/hover/","text":"Hover Tooltips \u00b6 Hover tooltips show a textual tooltip with extra information, when hovering part of the text. Hover tooltips are created by a Stratego strategy, but are configured in an ESV file under the references section: references hover _ : $Strategy For example: references hover _ : editor-hover The identifier after the colon refers to the Stratego strategy that creates the hover tooltip. The Stratego strategy takes an AST node, and either fails if no tooltip should be produced, or returns a tooltip string. The string may contain a few simple HTML tag to style the output. The following tags are supported: <br/> \u2014 line break <b>text</b> \u2014 bold <i>text</i> \u2014 italic <pre>code</pre> \u2014 preformatted (code) text Unrecognized HTML tags are stripped from the hover tooltip. Escape angled brackets and ampersands to show them verbatim in the tooltip.","title":"Hover Tooltips"},{"location":"references/editor-services/hover/#hover-tooltips","text":"Hover tooltips show a textual tooltip with extra information, when hovering part of the text. Hover tooltips are created by a Stratego strategy, but are configured in an ESV file under the references section: references hover _ : $Strategy For example: references hover _ : editor-hover The identifier after the colon refers to the Stratego strategy that creates the hover tooltip. The Stratego strategy takes an AST node, and either fails if no tooltip should be produced, or returns a tooltip string. The string may contain a few simple HTML tag to style the output. The following tags are supported: <br/> \u2014 line break <b>text</b> \u2014 bold <i>text</i> \u2014 italic <pre>code</pre> \u2014 preformatted (code) text Unrecognized HTML tags are stripped from the hover tooltip. Escape angled brackets and ampersands to show them verbatim in the tooltip.","title":"Hover Tooltips"},{"location":"references/editor-services/menus/","text":"Action Menus \u00b6 Menus are used to bind actions of your language, such as transformations, to a menu in the IDE. Menus are defined using the menu keyword under a menus section in an ESV file , and can themselves contain submenus, actions, and separators. menu : $String $MenuOptions $MenuContribs Menu Contributions \u00b6 A menu has zero or more $MenuContrib , which are: action , submenu , or separator . Actions \u00b6 Actions (sometimes called builders ) are defined under a menu or submenu with syntax: action : $String = $StrategoCall $MenuOptions Submenus \u00b6 Submenus allow grouping of actions in nested menus. Their syntax is: sub menu : $String $MenuOptions $MenuContribs end Separators \u00b6 Separators allow inserting a separator in a menu list using the syntax: separator Menu Options \u00b6 The menu options specify the behavior of the menu item. The following modifiers are supported: Modifier Description (source) Action is performed on the parsed AST instead of the default analyzed AST. (openeditor) The result should be opened in a new editor. (realtime) (meta) Example \u00b6 An example menu: menus menu : \"Generate\" action : \"To normal form\" = to-normal-form ( source ) sub menu : \"To Java\" action : \"Abstract\" = to-java-abstract ( openeditor ) action : \"Concrete\" = to-java-concrete end","title":"Action Menus"},{"location":"references/editor-services/menus/#action-menus","text":"Menus are used to bind actions of your language, such as transformations, to a menu in the IDE. Menus are defined using the menu keyword under a menus section in an ESV file , and can themselves contain submenus, actions, and separators. menu : $String $MenuOptions $MenuContribs","title":"Action Menus"},{"location":"references/editor-services/menus/#menu-contributions","text":"A menu has zero or more $MenuContrib , which are: action , submenu , or separator .","title":"Menu Contributions"},{"location":"references/editor-services/menus/#actions","text":"Actions (sometimes called builders ) are defined under a menu or submenu with syntax: action : $String = $StrategoCall $MenuOptions","title":"Actions"},{"location":"references/editor-services/menus/#submenus","text":"Submenus allow grouping of actions in nested menus. Their syntax is: sub menu : $String $MenuOptions $MenuContribs end","title":"Submenus"},{"location":"references/editor-services/menus/#separators","text":"Separators allow inserting a separator in a menu list using the syntax: separator","title":"Separators"},{"location":"references/editor-services/menus/#menu-options","text":"The menu options specify the behavior of the menu item. The following modifiers are supported: Modifier Description (source) Action is performed on the parsed AST instead of the default analyzed AST. (openeditor) The result should be opened in a new editor. (realtime) (meta)","title":"Menu Options"},{"location":"references/editor-services/menus/#example","text":"An example menu: menus menu : \"Generate\" action : \"To normal form\" = to-normal-form ( source ) sub menu : \"To Java\" action : \"Abstract\" = to-java-abstract ( openeditor ) action : \"Concrete\" = to-java-concrete end","title":"Example"},{"location":"references/editor-services/on-save/","text":"On-Save Handlers \u00b6 The on-save handler (also known as the compiler strategy) is used to transform files when they are saved in an editor. In an IDE, when a new project is opened, the compiler strategy is also executed on each file in the project, as well as when files change in the background. In a command-line batch compiler setting, it is used to transform all files. The compiler strategy is configured in an ESV file with the on save key: language on save : $Strategy The identifier after the colon refers to the Stratego strategy that performs the transformation. This strategy must have the exact same signature as the one for actions . For example: language on save : compile-file","title":"On-Save Handlers"},{"location":"references/editor-services/on-save/#on-save-handlers","text":"The on-save handler (also known as the compiler strategy) is used to transform files when they are saved in an editor. In an IDE, when a new project is opened, the compiler strategy is also executed on each file in the project, as well as when files change in the background. In a command-line batch compiler setting, it is used to transform all files. The compiler strategy is configured in an ESV file with the on save key: language on save : $Strategy The identifier after the colon refers to the Stratego strategy that performs the transformation. This strategy must have the exact same signature as the one for actions . For example: language on save : compile-file","title":"On-Save Handlers"},{"location":"references/editor-services/outline/","text":"Outline View \u00b6 An outline is a summary of the structure of a file, shown in a separate view next to a textual editor. An outline is created by a Stratego strategy, but is configured in an ESV file under the views section: views outline view : $Strategy expand to level : $Int The Stratego strategy specified as $Strategy must have the following signature: signature constructors Node : Label * Children - > Node rules editor-outline : ( node , position , ast , path , project-path ) - > outline Where the input is the default tuple used for builders , and the result is a list of Node terms, each carrying a label and a (possibly empty) list of child nodes. Preserve origins on the node's label to allow navigating to the corresponding code from the outline. For example: views outline view : editor-outline expand to level : 3 This configures the editor-outline Stratego strategy to be used to create outlines, and that outline nodes should be expanded 3 levels deep by default.","title":"Outline View"},{"location":"references/editor-services/outline/#outline-view","text":"An outline is a summary of the structure of a file, shown in a separate view next to a textual editor. An outline is created by a Stratego strategy, but is configured in an ESV file under the views section: views outline view : $Strategy expand to level : $Int The Stratego strategy specified as $Strategy must have the following signature: signature constructors Node : Label * Children - > Node rules editor-outline : ( node , position , ast , path , project-path ) - > outline Where the input is the default tuple used for builders , and the result is a list of Node terms, each carrying a label and a (possibly empty) list of child nodes. Preserve origins on the node's label to allow navigating to the corresponding code from the outline. For example: views outline view : editor-outline expand to level : 3 This configures the editor-outline Stratego strategy to be used to create outlines, and that outline nodes should be expanded 3 levels deep by default.","title":"Outline View"},{"location":"references/editor-services/parsing/","text":"Parsing \u00b6 Parsing language files in an editor is configured in the language section of an ESV file . The syntax is as follows: language table : $Path start symbols : $Sorts line comment : $String block comment : $String * $String fences : $Fences For example: language table : target/metaborg/sdf . tbl start symbols : File line comment : \"//\" block comment : \"/*\" * \"*/\" fences : [ ] ( ) { } Parse Table \u00b6 The parse table of your language is set with the table key. By default, the parse table of an SDF specification is always produced at target/metaborg/sdf.tbl . It is only necessary to change this configuration when a custom parse table is used. Start Symbols \u00b6 The start symbols key determine which start symbols to use when an editor is opened. This must be a subset of the start symbols defined in the SDF3 specification of your language. Multiple start symbols can be set with a comma-separated list: language start symbols : Start , Program Comments \u00b6 The syntax for comments is: language line comment : $String block comment : $String * $String For example, Java comments are specified as: language line comment : \"//\" block comment : \"/*\" * \"*/\" The line comment key determines how single-line comments are created. It is used by editors to toggle the comment for a single line. For example, in Eclipse, pressing Ctrl + / ( Cmd + / on macOS), respectively comments or uncomments the line. The block comment key determines how multi-line comments are created. It is used when a whole block needs to be commented or uncommented. A block comment is described by the two strings denoting the start and end symbols of the block comment respectively. Fences \u00b6 Fences for bracket matching are set as follows: language fences : $Fences The fences key determines which symbols to use and match for bracket matching. A single fence is defined by a starting and closing symbol. Multiple fences can be set with a space-separated list. Fences are used to do bracket matching in text editors. For example, the default fences in a new Spoofax language project are: language fences : [ ] ( ) { } Multi-Character Fences Fences can contain multiple characters, but some implementations may not handle bracket matching with multiple fence characters. For example, Eclipse does not handle this case and ignores multi-character fences.","title":"Parsing"},{"location":"references/editor-services/parsing/#parsing","text":"Parsing language files in an editor is configured in the language section of an ESV file . The syntax is as follows: language table : $Path start symbols : $Sorts line comment : $String block comment : $String * $String fences : $Fences For example: language table : target/metaborg/sdf . tbl start symbols : File line comment : \"//\" block comment : \"/*\" * \"*/\" fences : [ ] ( ) { }","title":"Parsing"},{"location":"references/editor-services/parsing/#parse-table","text":"The parse table of your language is set with the table key. By default, the parse table of an SDF specification is always produced at target/metaborg/sdf.tbl . It is only necessary to change this configuration when a custom parse table is used.","title":"Parse Table"},{"location":"references/editor-services/parsing/#start-symbols","text":"The start symbols key determine which start symbols to use when an editor is opened. This must be a subset of the start symbols defined in the SDF3 specification of your language. Multiple start symbols can be set with a comma-separated list: language start symbols : Start , Program","title":"Start Symbols"},{"location":"references/editor-services/parsing/#comments","text":"The syntax for comments is: language line comment : $String block comment : $String * $String For example, Java comments are specified as: language line comment : \"//\" block comment : \"/*\" * \"*/\" The line comment key determines how single-line comments are created. It is used by editors to toggle the comment for a single line. For example, in Eclipse, pressing Ctrl + / ( Cmd + / on macOS), respectively comments or uncomments the line. The block comment key determines how multi-line comments are created. It is used when a whole block needs to be commented or uncommented. A block comment is described by the two strings denoting the start and end symbols of the block comment respectively.","title":"Comments"},{"location":"references/editor-services/parsing/#fences","text":"Fences for bracket matching are set as follows: language fences : $Fences The fences key determines which symbols to use and match for bracket matching. A single fence is defined by a starting and closing symbol. Multiple fences can be set with a space-separated list. Fences are used to do bracket matching in text editors. For example, the default fences in a new Spoofax language project are: language fences : [ ] ( ) { } Multi-Character Fences Fences can contain multiple characters, but some implementations may not handle bracket matching with multiple fence characters. For example, Eclipse does not handle this case and ignores multi-character fences.","title":"Fences"},{"location":"references/editor-services/reference-resolution/","text":"Reference Resolution \u00b6 Reference resolution takes an AST node containing a reference, and tries to resolve it to its definition. The resolution is performed by a Stratego strategy, but is configured in an ESV file under the references section: references reference _ : $Strategy The identifier after the colon refers to the Stratego strategy that performs the resolution. The Stratego strategy takes an AST node, and either fails if it could not be resolved, or returns an AST node that has an origin location pointing to the definition site. For example: references reference _ : editor-resolve","title":"Reference Resolution"},{"location":"references/editor-services/reference-resolution/#reference-resolution","text":"Reference resolution takes an AST node containing a reference, and tries to resolve it to its definition. The resolution is performed by a Stratego strategy, but is configured in an ESV file under the references section: references reference _ : $Strategy The identifier after the colon refers to the Stratego strategy that performs the resolution. The Stratego strategy takes an AST node, and either fails if it could not be resolved, or returns an AST node that has an origin location pointing to the definition site. For example: references reference _ : editor-resolve","title":"Reference Resolution"},{"location":"references/editor-services/renaming/","text":"Rename Refactoring \u00b6 Spoofax provides an automated rename refactoring as an editor service for every language developed with it that has the static semantics defined with Statix or NaBL2. Strategy \u00b6 Rename refactoring is enabled by default for new Spoofax language projects. This works by registering the rename-action strategy from the statixruntime library as an action in a menu. This strategy takes three parameters: a layout-preserving pretty-printing strategy ( construct-textual-change by default), the editor analyze strategy ( editor-analyze by default), and a strategy that should succeed when renaming in multi-file mode. The default rename refactoring strategy looks like this: rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , fail ) To enable multi-file mode, change the last argument to id : rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id ) Statix \u00b6 For the renaming to work correctly in all cases when using Statix, terms that represent a declaration of a program entity, such as a function or a variable, need to set the @decl property on the name of the entity. For example, when declaring a type: declareType ( scope , name , T ) : - scope -> Type { name } with typeOfDecl T , @ name . decl := name , typeOfDecl of Type { name } in scope |-> [ ( _ , ( _ , T )) ] . See Also \u00b6 How-To: Add Rename Refactoring to an Existing Project","title":"Rename Refactoring"},{"location":"references/editor-services/renaming/#rename-refactoring","text":"Spoofax provides an automated rename refactoring as an editor service for every language developed with it that has the static semantics defined with Statix or NaBL2.","title":"Rename Refactoring"},{"location":"references/editor-services/renaming/#strategy","text":"Rename refactoring is enabled by default for new Spoofax language projects. This works by registering the rename-action strategy from the statixruntime library as an action in a menu. This strategy takes three parameters: a layout-preserving pretty-printing strategy ( construct-textual-change by default), the editor analyze strategy ( editor-analyze by default), and a strategy that should succeed when renaming in multi-file mode. The default rename refactoring strategy looks like this: rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , fail ) To enable multi-file mode, change the last argument to id : rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id )","title":"Strategy"},{"location":"references/editor-services/renaming/#statix","text":"For the renaming to work correctly in all cases when using Statix, terms that represent a declaration of a program entity, such as a function or a variable, need to set the @decl property on the name of the entity. For example, when declaring a type: declareType ( scope , name , T ) : - scope -> Type { name } with typeOfDecl T , @ name . decl := name , typeOfDecl of Type { name } in scope |-> [ ( _ , ( _ , T )) ] .","title":"Statix"},{"location":"references/editor-services/renaming/#see-also","text":"How-To: Add Rename Refactoring to an Existing Project","title":"See Also"},{"location":"references/editor-services/stratego/","text":"Stratego \u00b6 The Java JAR and CTree files that will be loaded into the Stratego runtime for your language can be configured with the provider key in an ESV file : language provider : $Path The path is a path to a .jar or .ctree file, relative to the root of the project. For example: language provider : target/metaborg/stratego . ctree The extension of the provider should match the format in the metaborg.yaml file of your language. Multiple files can be set by setting the key multiple times: ``esv language provider : target/metaborg/stratego.ctree provider : target/custom1.jar provider : target/custom2.ctree ```","title":"Stratego"},{"location":"references/editor-services/stratego/#stratego","text":"The Java JAR and CTree files that will be loaded into the Stratego runtime for your language can be configured with the provider key in an ESV file : language provider : $Path The path is a path to a .jar or .ctree file, relative to the root of the project. For example: language provider : target/metaborg/stratego . ctree The extension of the provider should match the format in the metaborg.yaml file of your language. Multiple files can be set by setting the key multiple times: ``esv language provider : target/metaborg/stratego.ctree provider : target/custom1.jar provider : target/custom2.ctree ```","title":"Stratego"},{"location":"references/editor-services/syntax-highlighting/","text":"Syntax Highlighting \u00b6 Token-based syntax highlighting is configured in a colorer section of an ESV file . Such a section can contain style definitions and styling rules. Style Definitions \u00b6 Style definitions bind an identifier to a style for later reuse, using the syntax: $ID = $Style Styles \u00b6 A style specifies a combination of a foreground color, optional background color, and optional font style. Colors are specified as Red-Green-Blue values ranging from 0 (none) to 255 (full). The possible font attributes are: Font attribute Description (none) Normal font. bold Bold font. italic Italic font. bold italic Bond and italic font. italic bold Same as bold italic . For example, the following style definitions bind the red , green , and blue colors: colorer red = 255 0 0 green = 0 255 0 blue = 0 0 255 An optional background color can be set by adding another RGB value: colorer redWithGreenBackground = 255 0 0 0 255 0 The font attributes can be used to make the font bold or italic: colorer redWithBold = 255 0 0 bold redWithItalic = 255 0 0 italic redWithGreenBackgroundWithBoldItalic = 255 0 0 0 255 0 bold italic Style Rules \u00b6 Style rules assign a style to matched tokens with syntax: $Matcher : $Style Or assigns a previously defined style definition: $Matcher : $Ref The left hand side of style rules matches a token, whereas the right hand side assigns a style by referring to a previously defined style definition, or by directly assigning a style. For example, the following matches a token type and references a style definition: colorer operator : black whereas the following matches a token with a sort and constructor, and directly assigns a style: colorer ClassBodyDec . MethodDec : 0 255 0 Matchers \u00b6 There are several ways in which the matcher on the left-hand side of a style rule can be specified: by type, by sort, by constructor, or by sort and constructor. Match by Sort and Constructor \u00b6 The combination of a token sort and constructor can be matched by specifying the $Sort.$Constructor . For example: colorer ClassBodyDec.MethodDec : yellow ClassBodyDec.FieldDec : red Match by Constructor \u00b6 It is also possible to match constructors, regardless of their token sorts, using _ in place of the sort name. For example: colorer _ . Str : blue _ . StrCong : blue _ . QStr : blue _ . QDollar : blue _ . QBr : gray Match by Sort \u00b6 Additionally, it is possible to match any constructor for a specific sort. For this, just specify the name of the sort, $Sort . For example: colorer ID : darkblue TYPEID : blue JQTYPEID : blue PQTYPEID : blue FUNCID : 153 51 0 JFUNCID : 153 51 0 STRING : 177 47 2 Match by Type \u00b6 Finally, the following built-in token types can be matched on: identifier \u2014 matches identifiers, found by lexical non-terminals without numbers; keyword \u2014 matches keywords, found by terminals in the syntax definition; layout \u2014 matches layout, such as whitespace and comments, found by layout definition; number \u2014 matches numbers, found by lexical non-terminals with numbers; operator \u2014 matches operations, found by terminals that contain just symbols (no characters); string \u2014 matches strings, found by lexical non-terminals that include quotation marks; unknown \u2014 matches tokens which the parser was unable to infer a type for. var error For example, the following code defines a simple highlighting with token types: colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic","title":"Syntax Highlighting"},{"location":"references/editor-services/syntax-highlighting/#syntax-highlighting","text":"Token-based syntax highlighting is configured in a colorer section of an ESV file . Such a section can contain style definitions and styling rules.","title":"Syntax Highlighting"},{"location":"references/editor-services/syntax-highlighting/#style-definitions","text":"Style definitions bind an identifier to a style for later reuse, using the syntax: $ID = $Style","title":"Style Definitions"},{"location":"references/editor-services/syntax-highlighting/#styles","text":"A style specifies a combination of a foreground color, optional background color, and optional font style. Colors are specified as Red-Green-Blue values ranging from 0 (none) to 255 (full). The possible font attributes are: Font attribute Description (none) Normal font. bold Bold font. italic Italic font. bold italic Bond and italic font. italic bold Same as bold italic . For example, the following style definitions bind the red , green , and blue colors: colorer red = 255 0 0 green = 0 255 0 blue = 0 0 255 An optional background color can be set by adding another RGB value: colorer redWithGreenBackground = 255 0 0 0 255 0 The font attributes can be used to make the font bold or italic: colorer redWithBold = 255 0 0 bold redWithItalic = 255 0 0 italic redWithGreenBackgroundWithBoldItalic = 255 0 0 0 255 0 bold italic","title":"Styles"},{"location":"references/editor-services/syntax-highlighting/#style-rules","text":"Style rules assign a style to matched tokens with syntax: $Matcher : $Style Or assigns a previously defined style definition: $Matcher : $Ref The left hand side of style rules matches a token, whereas the right hand side assigns a style by referring to a previously defined style definition, or by directly assigning a style. For example, the following matches a token type and references a style definition: colorer operator : black whereas the following matches a token with a sort and constructor, and directly assigns a style: colorer ClassBodyDec . MethodDec : 0 255 0","title":"Style Rules"},{"location":"references/editor-services/syntax-highlighting/#matchers","text":"There are several ways in which the matcher on the left-hand side of a style rule can be specified: by type, by sort, by constructor, or by sort and constructor.","title":"Matchers"},{"location":"references/editor-services/syntax-highlighting/#match-by-sort-and-constructor","text":"The combination of a token sort and constructor can be matched by specifying the $Sort.$Constructor . For example: colorer ClassBodyDec.MethodDec : yellow ClassBodyDec.FieldDec : red","title":"Match by Sort and Constructor"},{"location":"references/editor-services/syntax-highlighting/#match-by-constructor","text":"It is also possible to match constructors, regardless of their token sorts, using _ in place of the sort name. For example: colorer _ . Str : blue _ . StrCong : blue _ . QStr : blue _ . QDollar : blue _ . QBr : gray","title":"Match by Constructor"},{"location":"references/editor-services/syntax-highlighting/#match-by-sort","text":"Additionally, it is possible to match any constructor for a specific sort. For this, just specify the name of the sort, $Sort . For example: colorer ID : darkblue TYPEID : blue JQTYPEID : blue PQTYPEID : blue FUNCID : 153 51 0 JFUNCID : 153 51 0 STRING : 177 47 2","title":"Match by Sort"},{"location":"references/editor-services/syntax-highlighting/#match-by-type","text":"Finally, the following built-in token types can be matched on: identifier \u2014 matches identifiers, found by lexical non-terminals without numbers; keyword \u2014 matches keywords, found by terminals in the syntax definition; layout \u2014 matches layout, such as whitespace and comments, found by layout definition; number \u2014 matches numbers, found by lexical non-terminals with numbers; operator \u2014 matches operations, found by terminals that contain just symbols (no characters); string \u2014 matches strings, found by lexical non-terminals that include quotation marks; unknown \u2014 matches tokens which the parser was unable to infer a type for. var error For example, the following code defines a simple highlighting with token types: colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic","title":"Match by Type"},{"location":"references/flowspec/Stratego_API/","text":"Stratego API \u00b6 Strategies for interfacing with FlowSpec from Stratego. Execution, configuration, extracting results. Setup \u00b6 Execution \u00b6 Querying \u00b6","title":"Stratego API"},{"location":"references/flowspec/Stratego_API/#stratego-api","text":"Strategies for interfacing with FlowSpec from Stratego. Execution, configuration, extracting results.","title":"Stratego API"},{"location":"references/flowspec/Stratego_API/#setup","text":"","title":"Setup"},{"location":"references/flowspec/Stratego_API/#execution","text":"","title":"Execution"},{"location":"references/flowspec/Stratego_API/#querying","text":"","title":"Querying"},{"location":"references/flowspec/glossary/","text":"Glossary \u00b6","title":"Glossary"},{"location":"references/flowspec/glossary/#glossary","text":"","title":"Glossary"},{"location":"references/flowspec/introduction/","text":"Introduction \u00b6 Programs that are syntactically well-formed are not necessarily valid programs. Programming languages typically impose additional context-sensitive requirements on programs that cannot be captured in a syntax definition. Languages use data and control flow to check certain extra properties that fall outside of names and type systems. The FlowSpec \u2018Flow Analysis Specification Language\u2019 supports the specification of rules to define the static control flow of a language, and data flow analysis over that control flow. FlowSpec supports flow-sensitive intra-procedural data flow analysis. Control Flow Graphs \u00b6 Control-flow represents the execution order of a program. Depending on the input given to the program, or other things the program may observe of its execution environment (e.g. network communication, or a source of noise used to generate pseudo-random numbers), a program may execute a different trace of instructions. Since in general programs may not terminate at all, and humans are not very adapt at reasoning about possible infinities, we use a finite representation of possibly infinite program traces using control-flow graphs. Control-flow graphs are similarly finite as program text and are usually very similar, giving rise to a visual representation of the program. Loops in the program are represented as cycles in the control-flow graph, conditional code is represented by a split in control-flow which is merged again automatically after the conditional code. Data Flow Analysis over Control Flow Graphs \u00b6 Data-flow analysis propagates information either forward or backward along the control-flow graph. This can be information that approximates the data that is handled by the program, or the way in which the program interacts with memory, or something else altogether. Examples of data-flow analysis include constant analysis which checks when variables that are used in the program are guaranteed to have the same value regardless of the execution circumstances of the program, or live variables analysis which identifies if values in variables are actually observable by the program.","title":"Introduction"},{"location":"references/flowspec/introduction/#introduction","text":"Programs that are syntactically well-formed are not necessarily valid programs. Programming languages typically impose additional context-sensitive requirements on programs that cannot be captured in a syntax definition. Languages use data and control flow to check certain extra properties that fall outside of names and type systems. The FlowSpec \u2018Flow Analysis Specification Language\u2019 supports the specification of rules to define the static control flow of a language, and data flow analysis over that control flow. FlowSpec supports flow-sensitive intra-procedural data flow analysis.","title":"Introduction"},{"location":"references/flowspec/introduction/#control-flow-graphs","text":"Control-flow represents the execution order of a program. Depending on the input given to the program, or other things the program may observe of its execution environment (e.g. network communication, or a source of noise used to generate pseudo-random numbers), a program may execute a different trace of instructions. Since in general programs may not terminate at all, and humans are not very adapt at reasoning about possible infinities, we use a finite representation of possibly infinite program traces using control-flow graphs. Control-flow graphs are similarly finite as program text and are usually very similar, giving rise to a visual representation of the program. Loops in the program are represented as cycles in the control-flow graph, conditional code is represented by a split in control-flow which is merged again automatically after the conditional code.","title":"Control Flow Graphs"},{"location":"references/flowspec/introduction/#data-flow-analysis-over-control-flow-graphs","text":"Data-flow analysis propagates information either forward or backward along the control-flow graph. This can be information that approximates the data that is handled by the program, or the way in which the program interacts with memory, or something else altogether. Examples of data-flow analysis include constant analysis which checks when variables that are used in the program are guaranteed to have the same value regardless of the execution circumstances of the program, or live variables analysis which identifies if values in variables are actually observable by the program.","title":"Data Flow Analysis over Control Flow Graphs"},{"location":"references/flowspec/references/","text":"References \u00b6","title":"References"},{"location":"references/flowspec/references/#references","text":"","title":"References"},{"location":"references/flowspec/structure/","text":"Modules \u00b6 A module is defined by a single flowspec file. A module can contain several sections, for defining control-flow, data flow, types, and functions. Modules can import other modules. module $module-id imports $module-ref* $section* Terms and Patterns \u00b6 FlowSpec defines various data types, including terms, tuples, sets, and maps. These can be constructed by the user, or introduced by matching on the AST. term = ctor-id \"(\" {term \",\"}* \")\" | \"(\" {term \",\"}* \")\" | \"{\" {term \",\"}* \"}\" | \"{\" term \"|\" {term \",\"}* \"}\" | \"{\" {(term \"|->\" term) \",\"}* \"}\" | \"{\" term \"|->\" term \"|\" {(term \"|->\" term) \",\"}* \"}\" Control flow and data flow rules can use patterns to define which rules apply to which AST nodes. pattern = ctor-id \"(\" {pattern \",\"}* \")\" | \"(\" {pattern \",\"}* \")\" | var-id \"@\" pattern | \"_\" | var-id Control Flow \u00b6 The control-flow section contains the rules that define the control-flow for the subject language. control-flow rules $control-flow-rule* Control Flow Rules \u00b6 A control-flow rule consists of a pattern and a corresponding list of control-flow chains. control-flow-rule = \"root\"? pattern \"=\" {cfg-chain \",\"}+ | \"node\" pattern cfg-chain = {cfg-chain-elem \"->\"}+ cfg-chain-elem = \"entry\" | \"exit\" | variable | \"node\" variable | \"this\" Example. Module that specifies how the control-flow for the Add AST node goes from the lhs, the rhs, and then to the Add itself. It also specifies that Int must have a node in the control-flow graph. module control control-flow rules node Int(_) Add(l, r) = entry -> l -> r -> this -> exit Root Rules \u00b6 A root of the control-flow defines the start and end nodes of a control-flow graph. You can have multiple control-flow graphs in the same AST, but not nested ones. Each control-flow graph has a unique start and end node. A root control-flow rule introduces the start and end node. In other control-flow rules these nodes can be referred to for abrupt termination. cfg-chain-elem = ... | \"start\" | \"end\" Example. Module that defines control-flow for a procedure, and the return statement that goes straight to the end of the procedure. module control control-flow rules root Procedure(args, _, body) = start -> args -> body -> end Return(_) = entry -> this -> end Data Flow \u00b6 Properties \u00b6 The data flow section contains definitions of the properties to compute, and the rules that define how these properties are computed. properties property-definition* A property has a name, and a corresponding lattice type. The result after analysis will be a lattice of this type for each node in the control-flow graph. property-definition = name \":\" lattice Example. Lattice definition for a constant-value analysis. properties values: Map[name, Value] Rules \u00b6 The data flow rules specify how data should flow across the control-flow graph. property rules property-rule* property-rule = name \"(\" prop-pattern \")\" \"=\" expr prop-pattern = name \"->\" pattern | pattern \"->\" name | pattern \".\" \"start\" | pattern \".\" \"end\" Example. A simple specification for a constant-value analysis. property rules values(_.end) = Map[string, Value].bottom values(prev -> VarDec(n, _, Int(i))) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Const(i)} values(prev -> VarDec(n, _, _)) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Top()} values(prev -> _) = values(prev) Lattices \u00b6 Lattices are the main data type used in data-flow analysis, because of their desirable properties. Properties (the analysis results) must always be of type lattice. FlowSpec contains some builtin lattice types, but users can also specify their own. lattices lattice-definition* Lattice definitions must include the following: the underlying datatype, a join operator (either least-upper bound or greatest-lower bound), a top, and a bottom. name where type = type lub([name], name) = expr top = expr bottom = expr Types \u00b6 Aside from lattices, algebraic datatypes can be defined for use within lattices definitions. Users can directly match these datatypes, or construct new values. types type-definition* An algebraic datatype consists of a constructor and zero or more arguments. name = (\"|\" ctor-id \"(\" {type \",\"}* \")\")+ Example. The definition for an algebraic type used in constant value analysis. types ConstProp = | Top() | Const(int) | Bottom() Functions \u00b6 Functions make it possible to reuse functionality and avoid duplication of logic. functions function-definition* name([{(name \":\" type) \",\"}+]) = expr Expressions \u00b6 Integers \u00b6 Integer literals are written with an optional minus sign followed by one or more decimals. Supported integer operations are: Addition [ + ] Subtraction [ - ] Multiplication [ * ] Division [ / ] Modulo [ % ] Negate [ - ] Comparison [ < , <= , > , >= , == , != ] Booleans \u00b6 Boolean literals true and false are available as well as the usual boolean operations: And [ && ] Or [ || ] Not [ ! ] Sets and Maps \u00b6 Set and map literals are both denoted with curly braces. A set literal contains a comma-separated list of elements: {elem1, elem2, elem3} . A map literal contains a comma-separated list of bindings of the form key |-> value: { key1 |-> value1, key2 |-> value2 } . Operations on sets and maps include Union [ \\/ ] Intersection [ /\\ ] Set/map minus [ \\ ] Containment/lookup [ in ] There are also comprehensions of the form { new | old <- set, conditions } or { newkey |-> newvalue | oldkey |-> oldvalue <- map, condition } , where new elements or bindings are gathered based on old ones from a set or map, as long as the boolean condition expressions hold. Such a condition expression may also be a match expression without a body for the arms. This is commonly used to filter maps or sets. Match \u00b6 Pattern matching can be done with a match expression: match expr with | pattern1 => expr2 | pattern2 => expr2 , where expr are expressions and pattern are patterns. Terms and patterns are defined at the start of the reference. Variables and References \u00b6 Pattern matching can introduce variables. Other references include values in the lattice, such as MaySet.bottom or MustSet.top . Functions and Lattice Operations \u00b6 User defined functions are invoked with functionname(arg1, arg2) . Lattice operations can be similarly invoked, requiring the type name: MaySet.lub(s1, s2) . Property Lookup \u00b6 Property lookup is similar to a function call, although property lookup only ever has a single argument. Term Positions \u00b6 FlowSpec provides a builtin function that returns the position of a term: position(term) . This can be used to differentiate two terms from an AST that are otherwise the same. Lexical Grammar \u00b6 Identifiers \u00b6 Most identifiers in FlowSpec fall into one of two categories, which we will refer to as: Lowercase identifiers, that start with a lowercase character, and must match the regular expression [a-z][a-zA-Z0-9]*. Uppercase identifiers, that start with an uppercase character, and must match the regular expression [A-Z][a-zA-Z0-9]*. Comments \u00b6 Comments in FlowSpec follow C-style comments: // ... single line ... for single-line comments /* ... multiple lines ... */ for multi-line comments Multi-line comments can be nested, and run until the end of the file when the closing */ is omitted.","title":"Structure"},{"location":"references/flowspec/structure/#modules","text":"A module is defined by a single flowspec file. A module can contain several sections, for defining control-flow, data flow, types, and functions. Modules can import other modules. module $module-id imports $module-ref* $section*","title":"Modules"},{"location":"references/flowspec/structure/#terms-and-patterns","text":"FlowSpec defines various data types, including terms, tuples, sets, and maps. These can be constructed by the user, or introduced by matching on the AST. term = ctor-id \"(\" {term \",\"}* \")\" | \"(\" {term \",\"}* \")\" | \"{\" {term \",\"}* \"}\" | \"{\" term \"|\" {term \",\"}* \"}\" | \"{\" {(term \"|->\" term) \",\"}* \"}\" | \"{\" term \"|->\" term \"|\" {(term \"|->\" term) \",\"}* \"}\" Control flow and data flow rules can use patterns to define which rules apply to which AST nodes. pattern = ctor-id \"(\" {pattern \",\"}* \")\" | \"(\" {pattern \",\"}* \")\" | var-id \"@\" pattern | \"_\" | var-id","title":"Terms and Patterns"},{"location":"references/flowspec/structure/#control-flow","text":"The control-flow section contains the rules that define the control-flow for the subject language. control-flow rules $control-flow-rule*","title":"Control Flow"},{"location":"references/flowspec/structure/#control-flow-rules","text":"A control-flow rule consists of a pattern and a corresponding list of control-flow chains. control-flow-rule = \"root\"? pattern \"=\" {cfg-chain \",\"}+ | \"node\" pattern cfg-chain = {cfg-chain-elem \"->\"}+ cfg-chain-elem = \"entry\" | \"exit\" | variable | \"node\" variable | \"this\" Example. Module that specifies how the control-flow for the Add AST node goes from the lhs, the rhs, and then to the Add itself. It also specifies that Int must have a node in the control-flow graph. module control control-flow rules node Int(_) Add(l, r) = entry -> l -> r -> this -> exit","title":"Control Flow Rules"},{"location":"references/flowspec/structure/#root-rules","text":"A root of the control-flow defines the start and end nodes of a control-flow graph. You can have multiple control-flow graphs in the same AST, but not nested ones. Each control-flow graph has a unique start and end node. A root control-flow rule introduces the start and end node. In other control-flow rules these nodes can be referred to for abrupt termination. cfg-chain-elem = ... | \"start\" | \"end\" Example. Module that defines control-flow for a procedure, and the return statement that goes straight to the end of the procedure. module control control-flow rules root Procedure(args, _, body) = start -> args -> body -> end Return(_) = entry -> this -> end","title":"Root Rules"},{"location":"references/flowspec/structure/#data-flow","text":"","title":"Data Flow"},{"location":"references/flowspec/structure/#properties","text":"The data flow section contains definitions of the properties to compute, and the rules that define how these properties are computed. properties property-definition* A property has a name, and a corresponding lattice type. The result after analysis will be a lattice of this type for each node in the control-flow graph. property-definition = name \":\" lattice Example. Lattice definition for a constant-value analysis. properties values: Map[name, Value]","title":"Properties"},{"location":"references/flowspec/structure/#rules","text":"The data flow rules specify how data should flow across the control-flow graph. property rules property-rule* property-rule = name \"(\" prop-pattern \")\" \"=\" expr prop-pattern = name \"->\" pattern | pattern \"->\" name | pattern \".\" \"start\" | pattern \".\" \"end\" Example. A simple specification for a constant-value analysis. property rules values(_.end) = Map[string, Value].bottom values(prev -> VarDec(n, _, Int(i))) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Const(i)} values(prev -> VarDec(n, _, _)) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Top()} values(prev -> _) = values(prev)","title":"Rules"},{"location":"references/flowspec/structure/#lattices","text":"Lattices are the main data type used in data-flow analysis, because of their desirable properties. Properties (the analysis results) must always be of type lattice. FlowSpec contains some builtin lattice types, but users can also specify their own. lattices lattice-definition* Lattice definitions must include the following: the underlying datatype, a join operator (either least-upper bound or greatest-lower bound), a top, and a bottom. name where type = type lub([name], name) = expr top = expr bottom = expr","title":"Lattices"},{"location":"references/flowspec/structure/#types","text":"Aside from lattices, algebraic datatypes can be defined for use within lattices definitions. Users can directly match these datatypes, or construct new values. types type-definition* An algebraic datatype consists of a constructor and zero or more arguments. name = (\"|\" ctor-id \"(\" {type \",\"}* \")\")+ Example. The definition for an algebraic type used in constant value analysis. types ConstProp = | Top() | Const(int) | Bottom()","title":"Types"},{"location":"references/flowspec/structure/#functions","text":"Functions make it possible to reuse functionality and avoid duplication of logic. functions function-definition* name([{(name \":\" type) \",\"}+]) = expr","title":"Functions"},{"location":"references/flowspec/structure/#expressions","text":"","title":"Expressions"},{"location":"references/flowspec/structure/#integers","text":"Integer literals are written with an optional minus sign followed by one or more decimals. Supported integer operations are: Addition [ + ] Subtraction [ - ] Multiplication [ * ] Division [ / ] Modulo [ % ] Negate [ - ] Comparison [ < , <= , > , >= , == , != ]","title":"Integers"},{"location":"references/flowspec/structure/#booleans","text":"Boolean literals true and false are available as well as the usual boolean operations: And [ && ] Or [ || ] Not [ ! ]","title":"Booleans"},{"location":"references/flowspec/structure/#sets-and-maps","text":"Set and map literals are both denoted with curly braces. A set literal contains a comma-separated list of elements: {elem1, elem2, elem3} . A map literal contains a comma-separated list of bindings of the form key |-> value: { key1 |-> value1, key2 |-> value2 } . Operations on sets and maps include Union [ \\/ ] Intersection [ /\\ ] Set/map minus [ \\ ] Containment/lookup [ in ] There are also comprehensions of the form { new | old <- set, conditions } or { newkey |-> newvalue | oldkey |-> oldvalue <- map, condition } , where new elements or bindings are gathered based on old ones from a set or map, as long as the boolean condition expressions hold. Such a condition expression may also be a match expression without a body for the arms. This is commonly used to filter maps or sets.","title":"Sets and Maps"},{"location":"references/flowspec/structure/#match","text":"Pattern matching can be done with a match expression: match expr with | pattern1 => expr2 | pattern2 => expr2 , where expr are expressions and pattern are patterns. Terms and patterns are defined at the start of the reference.","title":"Match"},{"location":"references/flowspec/structure/#variables-and-references","text":"Pattern matching can introduce variables. Other references include values in the lattice, such as MaySet.bottom or MustSet.top .","title":"Variables and References"},{"location":"references/flowspec/structure/#functions-and-lattice-operations","text":"User defined functions are invoked with functionname(arg1, arg2) . Lattice operations can be similarly invoked, requiring the type name: MaySet.lub(s1, s2) .","title":"Functions and Lattice Operations"},{"location":"references/flowspec/structure/#property-lookup","text":"Property lookup is similar to a function call, although property lookup only ever has a single argument.","title":"Property Lookup"},{"location":"references/flowspec/structure/#term-positions","text":"FlowSpec provides a builtin function that returns the position of a term: position(term) . This can be used to differentiate two terms from an AST that are otherwise the same.","title":"Term Positions"},{"location":"references/flowspec/structure/#lexical-grammar","text":"","title":"Lexical Grammar"},{"location":"references/flowspec/structure/#identifiers","text":"Most identifiers in FlowSpec fall into one of two categories, which we will refer to as: Lowercase identifiers, that start with a lowercase character, and must match the regular expression [a-z][a-zA-Z0-9]*. Uppercase identifiers, that start with an uppercase character, and must match the regular expression [A-Z][a-zA-Z0-9]*.","title":"Identifiers"},{"location":"references/flowspec/structure/#comments","text":"Comments in FlowSpec follow C-style comments: // ... single line ... for single-line comments /* ... multiple lines ... */ for multi-line comments Multi-line comments can be nested, and run until the end of the file when the closing */ is omitted.","title":"Comments"},{"location":"references/flowspec/testing/","text":"Testing \u00b6","title":"Testing"},{"location":"references/flowspec/testing/#testing","text":"","title":"Testing"},{"location":"references/pipelines/","text":"Pipelines for Interactive Environments \u00b6 Pipelines for interactive Environments (PIE) is the build system for Spoofax 3. PIE consists of two parts: a Java framework, a Java runtime and the PIE Domain Specific Language (DSL). This reference documentation is for the PIE DSL and will only provide some high level information about the framework and runtime to provide context. PIE uses tasks to compose pipelines. Each task has 0 or more inputs and one output. Each task can depend on files or on other tasks. Tasks can be marked as explicitly observed to indicate that we want the output of these tasks to stay up to date. The PIE runtime executes tasks incrementally, which means that it only executes tasks that are no longer up to date and that are required for a task which is explicitly observed. Tasks can be written in Java, but this involves a lot of boilerplate. Tasks can also be written in the PIE DSL. The PIE DSL is specifically made for PIE, so it has little boilerplate. Tasks written in the PIE DSL are compiled to Java. The PIE DSL \u00b6 PIE models a pipeline as tasks that call each other. The PIE DSL calls these tasks \"functions\", because each task has inputs and an output. A PIE DSL program consists of one or more files. File structure \u00b6 module fully:qualified:moduleName import fully:qualified:name:of:another:module import org:example:multipleDefs:{func1, func2 as other, aDataTypeAsWell} import org:example:languages:{java, cpp, sql}:spoofax:{parse, analyze, compile} data coolDataType = foreign java org.example.MyFirstJavaClass { func aMethod(int) -> bool } func greetWorld() -> string = \"Hello world!\" PIE DSL files contain a module statement, imports, and data and function definitions. The module statement declares the fully qualified name of the module. Imports are optional and import datatypes and function from other modules. They can import multiple functions or datatypes at the same time, and they can rename elements. Data and function definitions define functions and datatypes. Directory structure and module system \u00b6 PIE files have the extension .pie . Each PIE file forms a module. Modules can define functions and datatypes, and can import functions and datatypes from other modules. It is recommended to use the same name for the module as the path and filename, but this is not required. As such, the PIE DSL does not place any restrictions on paths and file names besides the standard restrictions for Spoofax languages. The module system is described in Modules . Types and data definitions \u00b6 The PIE DSL is a statically typed language. There are a few built-in types, such as int and path . Built-in types use lowercase characters. Custom datatypes can currently only be imported from Java as foreign definitions. The types in the PIE DSL are described in Types . The PIE DSL also supports generic datatypes. These follow Java semantics. The semantics of generics can be found in Generics . Function definitions \u00b6 Functions express task definitions. Functions consist of a head and an implementation. func $FuncHead = $FuncImpl func greet(name: string) -> string = \"Hello ${name}!\" func doSomethingDifficult() -> path = foreign org.example.DoSomethingDifficult func callJavaStaticFunction() -> bool = foreign java fully.qualified.java.ClassName#staticMethodName func createCustomType() -> CustomType = foreign java constructor org.example.CustomType The function head describes the signature of the function: the name, the input parameter types and the output type. All functions can be called the same way regardless of their implementation. The function implementation describes the way a function is implemented. A function can be implemented in PIE by providing an expression, as can be seen with greet Expressions are described in Expressions . A function can also be implemented in Java. The three ways this can be done are shown in the example as well. A complete overview of functions is given in Functions . Misc information. \u00b6 Java and C use the function called main with a certain signature as the entry point to the program. A PIE program does not have a set entry point. The entry point is whatever function is called from the PIE runtime.","title":"Pipelines for Interactive Environments"},{"location":"references/pipelines/#pipelines-for-interactive-environments","text":"Pipelines for interactive Environments (PIE) is the build system for Spoofax 3. PIE consists of two parts: a Java framework, a Java runtime and the PIE Domain Specific Language (DSL). This reference documentation is for the PIE DSL and will only provide some high level information about the framework and runtime to provide context. PIE uses tasks to compose pipelines. Each task has 0 or more inputs and one output. Each task can depend on files or on other tasks. Tasks can be marked as explicitly observed to indicate that we want the output of these tasks to stay up to date. The PIE runtime executes tasks incrementally, which means that it only executes tasks that are no longer up to date and that are required for a task which is explicitly observed. Tasks can be written in Java, but this involves a lot of boilerplate. Tasks can also be written in the PIE DSL. The PIE DSL is specifically made for PIE, so it has little boilerplate. Tasks written in the PIE DSL are compiled to Java.","title":"Pipelines for Interactive Environments"},{"location":"references/pipelines/#the-pie-dsl","text":"PIE models a pipeline as tasks that call each other. The PIE DSL calls these tasks \"functions\", because each task has inputs and an output. A PIE DSL program consists of one or more files.","title":"The PIE DSL"},{"location":"references/pipelines/#file-structure","text":"module fully:qualified:moduleName import fully:qualified:name:of:another:module import org:example:multipleDefs:{func1, func2 as other, aDataTypeAsWell} import org:example:languages:{java, cpp, sql}:spoofax:{parse, analyze, compile} data coolDataType = foreign java org.example.MyFirstJavaClass { func aMethod(int) -> bool } func greetWorld() -> string = \"Hello world!\" PIE DSL files contain a module statement, imports, and data and function definitions. The module statement declares the fully qualified name of the module. Imports are optional and import datatypes and function from other modules. They can import multiple functions or datatypes at the same time, and they can rename elements. Data and function definitions define functions and datatypes.","title":"File structure"},{"location":"references/pipelines/#directory-structure-and-module-system","text":"PIE files have the extension .pie . Each PIE file forms a module. Modules can define functions and datatypes, and can import functions and datatypes from other modules. It is recommended to use the same name for the module as the path and filename, but this is not required. As such, the PIE DSL does not place any restrictions on paths and file names besides the standard restrictions for Spoofax languages. The module system is described in Modules .","title":"Directory structure and module system"},{"location":"references/pipelines/#types-and-data-definitions","text":"The PIE DSL is a statically typed language. There are a few built-in types, such as int and path . Built-in types use lowercase characters. Custom datatypes can currently only be imported from Java as foreign definitions. The types in the PIE DSL are described in Types . The PIE DSL also supports generic datatypes. These follow Java semantics. The semantics of generics can be found in Generics .","title":"Types and data definitions"},{"location":"references/pipelines/#function-definitions","text":"Functions express task definitions. Functions consist of a head and an implementation. func $FuncHead = $FuncImpl func greet(name: string) -> string = \"Hello ${name}!\" func doSomethingDifficult() -> path = foreign org.example.DoSomethingDifficult func callJavaStaticFunction() -> bool = foreign java fully.qualified.java.ClassName#staticMethodName func createCustomType() -> CustomType = foreign java constructor org.example.CustomType The function head describes the signature of the function: the name, the input parameter types and the output type. All functions can be called the same way regardless of their implementation. The function implementation describes the way a function is implemented. A function can be implemented in PIE by providing an expression, as can be seen with greet Expressions are described in Expressions . A function can also be implemented in Java. The three ways this can be done are shown in the example as well. A complete overview of functions is given in Functions .","title":"Function definitions"},{"location":"references/pipelines/#misc-information","text":"Java and C use the function called main with a certain signature as the entry point to the program. A PIE program does not have a set entry point. The entry point is whatever function is called from the PIE runtime.","title":"Misc information."},{"location":"references/pipelines/expressions/","text":"Expressions \u00b6 This section describes expressions in the PIE DSL. Todo Write documentation","title":"Expressions"},{"location":"references/pipelines/expressions/#expressions","text":"This section describes expressions in the PIE DSL. Todo Write documentation","title":"Expressions"},{"location":"references/pipelines/functions/","text":"Functions \u00b6 This section describes functions in the PIE DSL. Note A task is a function with some special semantics in regards to runtime behavior. The PIE DSL does not differentiate between functions and tasks. In the DSL, both are called functions. Todo Write documentation","title":"Functions"},{"location":"references/pipelines/functions/#functions","text":"This section describes functions in the PIE DSL. Note A task is a function with some special semantics in regards to runtime behavior. The PIE DSL does not differentiate between functions and tasks. In the DSL, both are called functions. Todo Write documentation","title":"Functions"},{"location":"references/pipelines/generics/","text":"Generics \u00b6 This section describes generics in the PIE DSL. In a nutshell, it just follows the Java semantics. Todo Write documentation","title":"Generics"},{"location":"references/pipelines/generics/#generics","text":"This section describes generics in the PIE DSL. In a nutshell, it just follows the Java semantics. Todo Write documentation","title":"Generics"},{"location":"references/pipelines/modules/","text":"Module system \u00b6 This section describes the module system of the PIE DSL. Todo document the module system","title":"Module system"},{"location":"references/pipelines/modules/#module-system","text":"This section describes the module system of the PIE DSL. Todo document the module system","title":"Module system"},{"location":"references/pipelines/types/","text":"Types \u00b6 There are several built-in types in the PIE DSL. The PIE DSL also allows defining custom types. Todo write about the types in PIE The type system \u00b6 Built-in types \u00b6 unit \u00b6 bool \u00b6 int \u00b6 string \u00b6 path \u00b6 null \u00b6 top \u00b6 bottom \u00b6 Nullable types \u00b6 Lists \u00b6 (todo: also discuss empty lists) Tuples \u00b6 Suppliers \u00b6 Function types \u00b6 Datatypes \u00b6 Wildcards \u00b6 Custom datatypes \u00b6","title":"Types"},{"location":"references/pipelines/types/#types","text":"There are several built-in types in the PIE DSL. The PIE DSL also allows defining custom types. Todo write about the types in PIE","title":"Types"},{"location":"references/pipelines/types/#the-type-system","text":"","title":"The type system"},{"location":"references/pipelines/types/#built-in-types","text":"","title":"Built-in types"},{"location":"references/pipelines/types/#unit","text":"","title":"unit"},{"location":"references/pipelines/types/#bool","text":"","title":"bool"},{"location":"references/pipelines/types/#int","text":"","title":"int"},{"location":"references/pipelines/types/#string","text":"","title":"string"},{"location":"references/pipelines/types/#path","text":"","title":"path"},{"location":"references/pipelines/types/#null","text":"","title":"null"},{"location":"references/pipelines/types/#top","text":"","title":"top"},{"location":"references/pipelines/types/#bottom","text":"","title":"bottom"},{"location":"references/pipelines/types/#nullable-types","text":"","title":"Nullable types"},{"location":"references/pipelines/types/#lists","text":"(todo: also discuss empty lists)","title":"Lists"},{"location":"references/pipelines/types/#tuples","text":"","title":"Tuples"},{"location":"references/pipelines/types/#suppliers","text":"","title":"Suppliers"},{"location":"references/pipelines/types/#function-types","text":"","title":"Function types"},{"location":"references/pipelines/types/#datatypes","text":"","title":"Datatypes"},{"location":"references/pipelines/types/#wildcards","text":"","title":"Wildcards"},{"location":"references/pipelines/types/#custom-datatypes","text":"","title":"Custom datatypes"},{"location":"references/statix/","text":"Statix \u00b6 Meta-language for Specification of Static Semantics.","title":"Statix"},{"location":"references/statix/#statix","text":"Meta-language for Specification of Static Semantics.","title":"Statix"},{"location":"references/statix/basic-constraints/","text":"Basic Constraints \u00b6 True \u00b6 False \u00b6 Conjunction \u00b6 Equality \u00b6 Disequality \u00b6 briefly describe disequality with free variables Exists \u00b6 Try \u00b6 AST Identifiers \u00b6 AST Property \u00b6 Arithmetic Constraints \u00b6 Note Statix has a special syntactic category for arithmetic expressions. Therefore, arithmetic expressions cannot be used at regular term positions. Messages \u00b6","title":"Basic Constraints"},{"location":"references/statix/basic-constraints/#basic-constraints","text":"","title":"Basic Constraints"},{"location":"references/statix/basic-constraints/#true","text":"","title":"True"},{"location":"references/statix/basic-constraints/#false","text":"","title":"False"},{"location":"references/statix/basic-constraints/#conjunction","text":"","title":"Conjunction"},{"location":"references/statix/basic-constraints/#equality","text":"","title":"Equality"},{"location":"references/statix/basic-constraints/#disequality","text":"briefly describe disequality with free variables","title":"Disequality"},{"location":"references/statix/basic-constraints/#exists","text":"","title":"Exists"},{"location":"references/statix/basic-constraints/#try","text":"","title":"Try"},{"location":"references/statix/basic-constraints/#ast-identifiers","text":"","title":"AST Identifiers"},{"location":"references/statix/basic-constraints/#ast-property","text":"","title":"AST Property"},{"location":"references/statix/basic-constraints/#arithmetic-constraints","text":"Note Statix has a special syntactic category for arithmetic expressions. Therefore, arithmetic expressions cannot be used at regular term positions.","title":"Arithmetic Constraints"},{"location":"references/statix/basic-constraints/#messages","text":"","title":"Messages"},{"location":"references/statix/concepts/","text":"Language Concepts \u00b6 In this section, a brief description of the main concepts of the Statix language is provided. Terms \u00b6 The data model that underlies all Statix specifications is algebraic data. Besides several built-in primitives, such as integer and string literals, users can build composite terms using term constructors, tuples and lists. Statix is a sorted logic, in the sense that all runtime data should adhere to a multi-sorted signature. Constraints \u00b6 Key to the Statix design philosophy is to view a type-checking problem as a constraint problem. When solving the constraint problem, a minimal model is inferred from the constraints. This model represents a principal typing for the original program. In order to express such constraint problems, a versatile set of built-in constraints is provided by the Statix language. For more information on constraints, see the Basic Constraints section. Rules \u00b6 Besides using built-in constraints, users can define their own constraints using constraint handling rules. Rules consist of a head and a body. The head specifies the arguments to the constraint, and (optionally) a guard, which indicates when to apply the rule. The body is a regular constraint, which, when proven, asserts that the constraint holds. More detailed information about user-defined constraints can be found in the Rules section. Scope Graphs \u00b6 Since Statix is especially designed for type-checking, and type-checking is heavily intertwined with name binding, special support for name binding is integrated in the language. Name binding is modelled using scope graphs , in which scopes are represented as nodes, visibility is modelled using labelled edges between nodes, and declarations using special terminal nodes that are associated with a particular datum. References are modelled using scope graph queries . For more information on scope graph construction and querying, see sections Scope Graph Constraints and Queries , respectively.","title":"Language Concepts"},{"location":"references/statix/concepts/#language-concepts","text":"In this section, a brief description of the main concepts of the Statix language is provided.","title":"Language Concepts"},{"location":"references/statix/concepts/#terms","text":"The data model that underlies all Statix specifications is algebraic data. Besides several built-in primitives, such as integer and string literals, users can build composite terms using term constructors, tuples and lists. Statix is a sorted logic, in the sense that all runtime data should adhere to a multi-sorted signature.","title":"Terms"},{"location":"references/statix/concepts/#constraints","text":"Key to the Statix design philosophy is to view a type-checking problem as a constraint problem. When solving the constraint problem, a minimal model is inferred from the constraints. This model represents a principal typing for the original program. In order to express such constraint problems, a versatile set of built-in constraints is provided by the Statix language. For more information on constraints, see the Basic Constraints section.","title":"Constraints"},{"location":"references/statix/concepts/#rules","text":"Besides using built-in constraints, users can define their own constraints using constraint handling rules. Rules consist of a head and a body. The head specifies the arguments to the constraint, and (optionally) a guard, which indicates when to apply the rule. The body is a regular constraint, which, when proven, asserts that the constraint holds. More detailed information about user-defined constraints can be found in the Rules section.","title":"Rules"},{"location":"references/statix/concepts/#scope-graphs","text":"Since Statix is especially designed for type-checking, and type-checking is heavily intertwined with name binding, special support for name binding is integrated in the language. Name binding is modelled using scope graphs , in which scopes are represented as nodes, visibility is modelled using labelled edges between nodes, and declarations using special terminal nodes that are associated with a particular datum. References are modelled using scope graph queries . For more information on scope graph construction and querying, see sections Scope Graph Constraints and Queries , respectively.","title":"Scope Graphs"},{"location":"references/statix/modules/","text":"Modules \u00b6 A Statix Specification is organised as a collection of modules. Each module corresponds to a file with a .stx extension. Module Structure \u00b6 The structure of a Statix module looks as follows: module $ ModuleName $ Section * Each module declares its name, and subsequently contains a number of sections. The module name should coincide with the relative path of the module with respect to the closest source root. Todo Link to documentation on source roots. Imports \u00b6 In an imports section, definitions from other modules can be brought in scope. imports $ ModuleName * Modules can only be imported with their fully qualified name. That is, for each $ModuleName in an imports section, a module with exactly the same name must exist. Imports of sorts, constructors and predicates are transitive, while imports of labels and relations are non-transitive. Furthermore, overloading by type, shadowing of top-level definitions, and duplicate imports of specification entities are not allowed. Signatures \u00b6 In a signature section, type definitions are located. signature $ Signature * Examples of signatures are: sort and constructor declarations or label and relation declarations. Each of these will be explained in the appropriate subsection. Rules \u00b6 In a rules section, the rules of a specification are defined. For more information on rules, see the Rules section. rules $ RuleDeclaration *","title":"Modules"},{"location":"references/statix/modules/#modules","text":"A Statix Specification is organised as a collection of modules. Each module corresponds to a file with a .stx extension.","title":"Modules"},{"location":"references/statix/modules/#module-structure","text":"The structure of a Statix module looks as follows: module $ ModuleName $ Section * Each module declares its name, and subsequently contains a number of sections. The module name should coincide with the relative path of the module with respect to the closest source root. Todo Link to documentation on source roots.","title":"Module Structure"},{"location":"references/statix/modules/#imports","text":"In an imports section, definitions from other modules can be brought in scope. imports $ ModuleName * Modules can only be imported with their fully qualified name. That is, for each $ModuleName in an imports section, a module with exactly the same name must exist. Imports of sorts, constructors and predicates are transitive, while imports of labels and relations are non-transitive. Furthermore, overloading by type, shadowing of top-level definitions, and duplicate imports of specification entities are not allowed.","title":"Imports"},{"location":"references/statix/modules/#signatures","text":"In a signature section, type definitions are located. signature $ Signature * Examples of signatures are: sort and constructor declarations or label and relation declarations. Each of these will be explained in the appropriate subsection.","title":"Signatures"},{"location":"references/statix/modules/#rules","text":"In a rules section, the rules of a specification are defined. For more information on rules, see the Rules section. rules $ RuleDeclaration *","title":"Rules"},{"location":"references/statix/queries/","text":"Queries \u00b6 Filters \u00b6 Shadowing \u00b6 Result pattern \u00b6","title":"Queries"},{"location":"references/statix/queries/#queries","text":"","title":"Queries"},{"location":"references/statix/queries/#filters","text":"","title":"Filters"},{"location":"references/statix/queries/#shadowing","text":"","title":"Shadowing"},{"location":"references/statix/queries/#result-pattern","text":"","title":"Result pattern"},{"location":"references/statix/rules/","text":"Rules \u00b6 User-defined constraints and their rules make up the main part of a Statix specification. In this section, we describe the definition and usage of user-defined constraints and their rules. Constraint Definitions \u00b6 In order to define a custom constraint, its type must be declared first. A constraint can be declared in a rules section, or in a constraints subsection of a signature section. A constraint is declared by specifying its name and argument type. For more information on types, please refer to the Terms section. Note that the name of the constraint must be unique within a specification. $ ConstraintName : { $ Type \"*\" } * Note In this reference manual, we consistently use the term 'constraint declaration' for the introduction of new user-defined constraints. However, in practise, these are sometimes also referred to as 'predicate' or just simply 'constraint'. When a constraint declaration is provided this way, it can be used as a constraint by providing concrete arguments, separated by comma's. $ ConstraintName ({ $ Term \",\" } * ) The sorts of the argument terms should be equal to the sorts in the constraint declaration. Rule Definitions \u00b6 When solving a user-defined constraint, a rule for that constraint is unfolded in order to infer a model satisfying the constraint. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) : - $ Constraint . The part before the turnstile ( :- ) is often referred to as the head of the rule, while the $Constraint after the turnstile is denoted as body . When applying a rule, each head pattern (which is just a term) will be matched with its corresponding actual argument. Statically, the sorts of the terms in $Patterns are type-checked based on the constraint declaration. Any variables in patterns are implicitly introduced in the scope of the rule. Patterns can be non-linear. That is, a variable may occur multiple times in a pattern. Operationally, the subterms at these positions are then required to be structurally equal. Note that multiple rules for a single constraint can, and often will, be provided. For each constraint, the rule that is used for simplification is determined by the guard of the rule. This guard is derived from the head pattern: a rule can only be applied when the constraint arguments match the patterns. During constraint solving, Statix will try at most one rule for each constraint. The appropriate rule is selected by applying the following heuristics in order: 1. Rules with a smaller domain are preferred over rules with a larger domain. 2. When pairwise comparing rules, the rule for which, in left-to-right order, a more specific pattern is encountered first is preferred over the other. For all cases where these heuristics do not decide which rule to use for a constraint, compile time \"Overlapping patterns\" errors will be emitted. The $RuleName is just a name that can be used for documentation purposes. It cannot be referenced from any position in the specification, and may be omitted altogether. Axiom rules \u00b6 In some cases, a constraint trivially holds for particular inputs. For such constraints, an axiom rule can be specified. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ). This rule is similar to a regular rule, but lacks a body. When applying such a rule, no new constraints are introduced, reflecting the fact that the constraint trivially holds for these arguments. Functional Rules \u00b6 Some user-defined constraints can be thought of more naturally as a function: a constraint where a particular term is inferred by the constraint, rather than validated. Statix allows to write constraints in a functional idiom as follows: First, a constraint declaration for such 'functional constraints' must be provided as follows: $ ConstraintName : { $ Type \"*\" } * -> $ Type In addition to the regular list of input sorts, a sort for the output term is provided to the constraint declaration. Rule definitions for a functional constraint look as follows: [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) = $ Term : - $ Constraint . Compared to predicative rule definitions as introduced earlier in this section, an additional term after an equality-sign is appended to the rule head. This term denotes the output term (the term inferred by the rule). A functional constraint can be used in a term position, as opposed to a constraint position for predicative rules. Otherwise, their syntax is the same. $ ConstraintName ({ $ Term \",\" } * ) Semantically, the output term of applying the constraint is substituted at the position of the application of the functional predicate. Note When we want to make the distinction between these two forms of constraints explicit, we usually refer to either groups with 'predicative constraint declarations' and 'predicative constraints', versus 'functional constraint declarations' and 'functional constraints', respectively. Info Every specification with functional predicates is normalized to a form with only regular predicates. To show the normal form of a specification in Eclipse, use the Spoofax > Syntax > Format normalized AST menu action. Mapping rules \u00b6 Another common pattern in Statix is defining a predicate that instantiates a predicate for all elements in a list. Statix allows derive such mapping rules using the maps keyword as follows: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) A lift specifier ( $Lift ) can be one of the following: * : The identity lift . This lift specifier indicates that this argument is passed to the mapped constraint unchanged. list(*) : The list lift : This lift specifier indicates that the mapped constraint will be instantiated for each element in the list at that argument position. Each constraint defined with maps , must contain at least one list lift. Otherwise, the mapping would be a no-op. ({$Lift \",\"}+) : The tuple lift : This lift specifier indicates that arguments are extracted from a tuple. For each tuple argument, a corresponding lifting is applied afterwards. The type of $MappingConstraintName is inferred by inverse application of the lift specifiers to the type of $MappedConstraintName . Therefore, no explicit declaration of the type of the mapping constraint is required. Similar to predicative constraints, functional mapping constraints can be derived: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) = $ Lift In addition to lift specifiers of the input arguments, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred terms from the mapped constraints are aggregated and returned by the mapping constraint. Example. A common example where mapping rules are used is when type-checking a list of declarations. A specification snippet for that could look as follows: rules declOk : scope * Decl declsOk maps declOk ( * , list ( * )) // rules for declOk In this snippet, the declsOk constraint instantiates declOk for each declaration in a list of declaration. Its inferred type is scope * list(Decl) . When mapping functional constraints, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred values of the mapped constraint are returned by the mapping constraint. When using multiple list lifts in the input, the resulting constraint will zip the arguments. This implicitly requires the input lists to be of equal length. The creation of a cartesian product can be achieved by repeated application of the maps construct for each argument. Info Similar to functional constraints, constraints derived using the maps construct are normalized to regular predicative constraints. This normalization can be inspected using the Spoofax > Syntax > Format normalized AST menu action. Injections of Namespaces and Relations \u00b6 For convenience, it is possible to declare namespaces, namespace queries (both deprecated) and relations in a rules section as well. rules namespace Var : string resolve Var filter P * I * relation var : string -> TYPE","title":"Rules"},{"location":"references/statix/rules/#rules","text":"User-defined constraints and their rules make up the main part of a Statix specification. In this section, we describe the definition and usage of user-defined constraints and their rules.","title":"Rules"},{"location":"references/statix/rules/#constraint-definitions","text":"In order to define a custom constraint, its type must be declared first. A constraint can be declared in a rules section, or in a constraints subsection of a signature section. A constraint is declared by specifying its name and argument type. For more information on types, please refer to the Terms section. Note that the name of the constraint must be unique within a specification. $ ConstraintName : { $ Type \"*\" } * Note In this reference manual, we consistently use the term 'constraint declaration' for the introduction of new user-defined constraints. However, in practise, these are sometimes also referred to as 'predicate' or just simply 'constraint'. When a constraint declaration is provided this way, it can be used as a constraint by providing concrete arguments, separated by comma's. $ ConstraintName ({ $ Term \",\" } * ) The sorts of the argument terms should be equal to the sorts in the constraint declaration.","title":"Constraint Definitions"},{"location":"references/statix/rules/#rule-definitions","text":"When solving a user-defined constraint, a rule for that constraint is unfolded in order to infer a model satisfying the constraint. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) : - $ Constraint . The part before the turnstile ( :- ) is often referred to as the head of the rule, while the $Constraint after the turnstile is denoted as body . When applying a rule, each head pattern (which is just a term) will be matched with its corresponding actual argument. Statically, the sorts of the terms in $Patterns are type-checked based on the constraint declaration. Any variables in patterns are implicitly introduced in the scope of the rule. Patterns can be non-linear. That is, a variable may occur multiple times in a pattern. Operationally, the subterms at these positions are then required to be structurally equal. Note that multiple rules for a single constraint can, and often will, be provided. For each constraint, the rule that is used for simplification is determined by the guard of the rule. This guard is derived from the head pattern: a rule can only be applied when the constraint arguments match the patterns. During constraint solving, Statix will try at most one rule for each constraint. The appropriate rule is selected by applying the following heuristics in order: 1. Rules with a smaller domain are preferred over rules with a larger domain. 2. When pairwise comparing rules, the rule for which, in left-to-right order, a more specific pattern is encountered first is preferred over the other. For all cases where these heuristics do not decide which rule to use for a constraint, compile time \"Overlapping patterns\" errors will be emitted. The $RuleName is just a name that can be used for documentation purposes. It cannot be referenced from any position in the specification, and may be omitted altogether.","title":"Rule Definitions"},{"location":"references/statix/rules/#axiom-rules","text":"In some cases, a constraint trivially holds for particular inputs. For such constraints, an axiom rule can be specified. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ). This rule is similar to a regular rule, but lacks a body. When applying such a rule, no new constraints are introduced, reflecting the fact that the constraint trivially holds for these arguments.","title":"Axiom rules"},{"location":"references/statix/rules/#functional-rules","text":"Some user-defined constraints can be thought of more naturally as a function: a constraint where a particular term is inferred by the constraint, rather than validated. Statix allows to write constraints in a functional idiom as follows: First, a constraint declaration for such 'functional constraints' must be provided as follows: $ ConstraintName : { $ Type \"*\" } * -> $ Type In addition to the regular list of input sorts, a sort for the output term is provided to the constraint declaration. Rule definitions for a functional constraint look as follows: [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) = $ Term : - $ Constraint . Compared to predicative rule definitions as introduced earlier in this section, an additional term after an equality-sign is appended to the rule head. This term denotes the output term (the term inferred by the rule). A functional constraint can be used in a term position, as opposed to a constraint position for predicative rules. Otherwise, their syntax is the same. $ ConstraintName ({ $ Term \",\" } * ) Semantically, the output term of applying the constraint is substituted at the position of the application of the functional predicate. Note When we want to make the distinction between these two forms of constraints explicit, we usually refer to either groups with 'predicative constraint declarations' and 'predicative constraints', versus 'functional constraint declarations' and 'functional constraints', respectively. Info Every specification with functional predicates is normalized to a form with only regular predicates. To show the normal form of a specification in Eclipse, use the Spoofax > Syntax > Format normalized AST menu action.","title":"Functional Rules"},{"location":"references/statix/rules/#mapping-rules","text":"Another common pattern in Statix is defining a predicate that instantiates a predicate for all elements in a list. Statix allows derive such mapping rules using the maps keyword as follows: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) A lift specifier ( $Lift ) can be one of the following: * : The identity lift . This lift specifier indicates that this argument is passed to the mapped constraint unchanged. list(*) : The list lift : This lift specifier indicates that the mapped constraint will be instantiated for each element in the list at that argument position. Each constraint defined with maps , must contain at least one list lift. Otherwise, the mapping would be a no-op. ({$Lift \",\"}+) : The tuple lift : This lift specifier indicates that arguments are extracted from a tuple. For each tuple argument, a corresponding lifting is applied afterwards. The type of $MappingConstraintName is inferred by inverse application of the lift specifiers to the type of $MappedConstraintName . Therefore, no explicit declaration of the type of the mapping constraint is required. Similar to predicative constraints, functional mapping constraints can be derived: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) = $ Lift In addition to lift specifiers of the input arguments, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred terms from the mapped constraints are aggregated and returned by the mapping constraint. Example. A common example where mapping rules are used is when type-checking a list of declarations. A specification snippet for that could look as follows: rules declOk : scope * Decl declsOk maps declOk ( * , list ( * )) // rules for declOk In this snippet, the declsOk constraint instantiates declOk for each declaration in a list of declaration. Its inferred type is scope * list(Decl) . When mapping functional constraints, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred values of the mapped constraint are returned by the mapping constraint. When using multiple list lifts in the input, the resulting constraint will zip the arguments. This implicitly requires the input lists to be of equal length. The creation of a cartesian product can be achieved by repeated application of the maps construct for each argument. Info Similar to functional constraints, constraints derived using the maps construct are normalized to regular predicative constraints. This normalization can be inspected using the Spoofax > Syntax > Format normalized AST menu action.","title":"Mapping rules"},{"location":"references/statix/rules/#injections-of-namespaces-and-relations","text":"For convenience, it is possible to declare namespaces, namespace queries (both deprecated) and relations in a rules section as well. rules namespace Var : string resolve Var filter P * I * relation var : string -> TYPE","title":"Injections of Namespaces and Relations"},{"location":"references/statix/scope-graphs/","text":"Scope Graph Constraints \u00b6 Scopes \u00b6 new Edges \u00b6 assert edge constraint label declarations Declarations \u00b6 assert declaration constraint relation declarations permission to extend Query \u00b6 Permission to Extend \u00b6","title":"Scope Graph Constraints"},{"location":"references/statix/scope-graphs/#scope-graph-constraints","text":"","title":"Scope Graph Constraints"},{"location":"references/statix/scope-graphs/#scopes","text":"new","title":"Scopes"},{"location":"references/statix/scope-graphs/#edges","text":"assert edge constraint label declarations","title":"Edges"},{"location":"references/statix/scope-graphs/#declarations","text":"assert declaration constraint relation declarations permission to extend","title":"Declarations"},{"location":"references/statix/scope-graphs/#query","text":"","title":"Query"},{"location":"references/statix/scope-graphs/#permission-to-extend","text":"","title":"Permission to Extend"},{"location":"references/statix/stratego-api/","text":"Stratego API \u00b6 Executing the Solver \u00b6 Querying the Analysis Result \u00b6","title":"Stratego API"},{"location":"references/statix/stratego-api/#stratego-api","text":"","title":"Stratego API"},{"location":"references/statix/stratego-api/#executing-the-solver","text":"","title":"Executing the Solver"},{"location":"references/statix/stratego-api/#querying-the-analysis-result","text":"","title":"Querying the Analysis Result"},{"location":"references/statix/terms/","text":"Terms \u00b6 In Statix, data is represented using terms. This data can be a program, a typing annotation, or anything else that the specification defines. Terms are built from atoms and composites, such as constructors, tuples and lists. Additionally, Statix allows to inline several constraint results in terms. In this section, we explain the various types of terms that Statix supports, and, when appropriate, how their types should be declared. For a more in-depth explanation of the Statix Type System, see the section Types . Note Throughout this reference manual, we use the term 'sort' for syntactic categories, and 'type' for all other types (such as lists, tuples, scopes, etc.). However, in practise, these terms are both used in both meanings. Numerals \u00b6 Numeric literals are literals of the form [0-9]+ . Negative literals are not supported directly. All integer literals have the built-in type int . Strings \u00b6 String literals are arbitrary, single-line sequences of characters enclosed in double quotes. String literals may not contain unescaped backslashes, double quotes, or tabs. Double quotes and backslashes can be used in a string literal by prefixing them with another backslashes ( \\\" and \\\\ , respectively), while tabs, newlines and carriage returns can be encoded using respectively \\t , \\n and \\r . Otherwise, no escaping is required. String literals have the built-in type string . Identifiers \u00b6 Variables are identifiers of values of the following form: [a-zA-Z] [a-zA-Z0-9\\_]* [\\']* . With respect to type-checking, variables can be handled in two ways. When a variable occurs in the head of a rule , it is implicitly brought into scope with the type inferred from the rule type. Otherwise, it is required that the variable is introduced earlier, with the correct type. Apart from introduction in rule heads, variables can be introduced by existential constraints . In that case, the type of the variable is derived from its usage. Wildcards \u00b6 Wildcards are represented as _ , and denote variables without identity. Every occurrence of a wildcard is interpreted as a new variable. Because wildcards cannot reference each other, it is not required that the types of multiple wildcard occurrences coincide. Composite terms \u00b6 Composite terms can be build using constructor applications : $ ConsId ({ $ Term \",\" } * ) Here a term with constructor $ConsId and some term arguments is built. Composite terms must adhere to a signature. A signature describes which term compositions are valid, and must be declared in a signature section: signature sorts $ SortID * constructors $ ConsId : { $ Type \"*\" } + -> $ SortID $ ConsId : $ SortID First, the syntactic categories (which closely correspond to type identifiers in other languages) must be declared in a sorts subsection. Then, the constructor symbols can be declared in a constructors section. For each constructor, the types of the arguments and its sort should be provided. For nullary constructors (constructors without arguments), the arrow preceding the sort should be omitted. When a composite term is built, it is validated that all arguments match the type declaration from the signature. The type of the whole composite term is equal to the sort of the constructor. Tuples \u00b6 A built-in composite data construction is tuples : ({ $ Term \",\" } * ) Tuples have a statically fixed length, but the types of the arguments may differ. The type of the tuple expression is just the product of its arguments. The arity of a tuple may be anything except one, because unary tuples cause syntactic ambiguities with bracketed expressions. Lists \u00b6 Another built-in composite data construction is lists : [ { $ Term \",\" } *] Lists are created by comma-separating terms, enclosing them in square brackets. All terms should have the same type. Given that the type of the terms is T , the type of the list expression will be list(T) . Alternatively, lists can have a variable tail: [ { $ Term \",\" } * | $ Term ] In this syntax, the tail of the list is another term. This term should have type list(T) , where T is again the type of the first terms. Name Ascription \u00b6 It is possible to assign names to terms by prefixing the term with a variable name: $ Var @$ Term Note that this does not introduce a new variable with name $Var (except in a rule head , where all variables are introduced implicitly), but rather requires that a variable with corresponding name and type is already introduced. Tip In terms of equality constraints , the ascribe is equal to $Var == $Term . It is used to prevent the duplication of $Term . Type Ascription \u00b6 Statix allows to add inline type annotations to terms as follows: $ Term : $ Type The type-checker will validate that the term actually has the specified type, but the runtime behavior in not influenced by these ascriptions. Tip In general, the Statix type-checker should be able to infer all types. However, in case of a type error being reported at an incorrect position, these type ascriptions can help tracing the cause of the error. Arithmetic operations \u00b6 Arithmetic expressions can be inserted in terms as follows: # ( $ ArithExp ) Here, the type of the expression is int . For more information on arithmetic expressions, see Arithmetic Constraints Tip In terms of existential constraints , inline arithmetic expressions have behavior equal to {v} v #= $ArithExp , where v is used at the position of the arithmetic expression. So, for example, {T} T == CONS(#(21 * 2)) is equal to {T v} v #= 21 * 2, T == CONS(v) . AST Identifier \u00b6 In Spoofax, all terms in an AST are assigned an unique identifier (the term index) before analysis. This term identifier can be isolated as follows: astId ( $ Term ) Here, the type of $Term can be anything, and the type of the whole term will be astId . AST Identifiers are used to assign properties . New \u00b6 Statix allows inline creation of scopes : new Statically, the new term has type scope . At runtime, this creates a fresh scope, and inserts that at the position of the new term. Tip In terms of existential constraints , the inline new operator has behavior equal to {s} new s , where s is used at the position of the new term. So, for example, {T} T == CLASS(new) is equal to {T s} new s, T == CLASS(s) . Paths \u00b6 Part of a query result is the path from the resolved datum back to the scope where the query started. In order to represent paths, Statix has two built-in constructors: _PathEmpty : Unary constructor that carries a single scope. This constructor has type scope -> path . _PathStep : Ternary constructor that represents a traversed edge in a path. This constructor has type path * label * scope -> path . Note Although the labels in a _PathStep can be bound to a variable, and hence be compared with and included in other terms, no inspection, matching or comparison with label definitions is supported. Occurrences \u00b6 Warning Since Spoofax 2.5.15, namespaces and occurrences are deprecated. Statix has built-in support for namespaces. A term embedded in a particular namespace is called an occurrence . Occurrences can be written as follows: $ NamespaceId { $ SpaceTerms } $ NamespaceId { $ SpaceTerms @$ OccurrenceId } In this structure template, $SpaceTerms means a list of terms, separated by spaces. The occurrence identifier can be any term. In case the term has an AST identifier, that value will be used as the identity of the occurrence. Alternatively, the occurrence identifier can be left out: $ NamespaceId { $ SpaceTerms } The default occurrence identifier is - , which means that the occurrence has no identifier. The type of an occurrence literal is occurrence . For more information about namespaces, see the Queries section. Declaration Match \u00b6 Statix allows to query the current scope for declarations of a particular form: ?$ RelationId [ { $ Term \",\" } ] in $ Scope When using this expression, a functional relation $RelationId must be declared. The terms arguments must correspond to the argument of the relation, and the type of the term is the output type of the relation. For more information on querying the scope graph, see the Queries section. Tip In terms of regular queries , the declaration match is equal to a query with filter e , expecting a single output. E.g. T == ?var[\"x\"] in s is equal to query var filter e and { x' :- x' == \"x\"} in s |-> [(_, (_, T))] .","title":"Terms"},{"location":"references/statix/terms/#terms","text":"In Statix, data is represented using terms. This data can be a program, a typing annotation, or anything else that the specification defines. Terms are built from atoms and composites, such as constructors, tuples and lists. Additionally, Statix allows to inline several constraint results in terms. In this section, we explain the various types of terms that Statix supports, and, when appropriate, how their types should be declared. For a more in-depth explanation of the Statix Type System, see the section Types . Note Throughout this reference manual, we use the term 'sort' for syntactic categories, and 'type' for all other types (such as lists, tuples, scopes, etc.). However, in practise, these terms are both used in both meanings.","title":"Terms"},{"location":"references/statix/terms/#numerals","text":"Numeric literals are literals of the form [0-9]+ . Negative literals are not supported directly. All integer literals have the built-in type int .","title":"Numerals"},{"location":"references/statix/terms/#strings","text":"String literals are arbitrary, single-line sequences of characters enclosed in double quotes. String literals may not contain unescaped backslashes, double quotes, or tabs. Double quotes and backslashes can be used in a string literal by prefixing them with another backslashes ( \\\" and \\\\ , respectively), while tabs, newlines and carriage returns can be encoded using respectively \\t , \\n and \\r . Otherwise, no escaping is required. String literals have the built-in type string .","title":"Strings"},{"location":"references/statix/terms/#identifiers","text":"Variables are identifiers of values of the following form: [a-zA-Z] [a-zA-Z0-9\\_]* [\\']* . With respect to type-checking, variables can be handled in two ways. When a variable occurs in the head of a rule , it is implicitly brought into scope with the type inferred from the rule type. Otherwise, it is required that the variable is introduced earlier, with the correct type. Apart from introduction in rule heads, variables can be introduced by existential constraints . In that case, the type of the variable is derived from its usage.","title":"Identifiers"},{"location":"references/statix/terms/#wildcards","text":"Wildcards are represented as _ , and denote variables without identity. Every occurrence of a wildcard is interpreted as a new variable. Because wildcards cannot reference each other, it is not required that the types of multiple wildcard occurrences coincide.","title":"Wildcards"},{"location":"references/statix/terms/#composite-terms","text":"Composite terms can be build using constructor applications : $ ConsId ({ $ Term \",\" } * ) Here a term with constructor $ConsId and some term arguments is built. Composite terms must adhere to a signature. A signature describes which term compositions are valid, and must be declared in a signature section: signature sorts $ SortID * constructors $ ConsId : { $ Type \"*\" } + -> $ SortID $ ConsId : $ SortID First, the syntactic categories (which closely correspond to type identifiers in other languages) must be declared in a sorts subsection. Then, the constructor symbols can be declared in a constructors section. For each constructor, the types of the arguments and its sort should be provided. For nullary constructors (constructors without arguments), the arrow preceding the sort should be omitted. When a composite term is built, it is validated that all arguments match the type declaration from the signature. The type of the whole composite term is equal to the sort of the constructor.","title":"Composite terms"},{"location":"references/statix/terms/#tuples","text":"A built-in composite data construction is tuples : ({ $ Term \",\" } * ) Tuples have a statically fixed length, but the types of the arguments may differ. The type of the tuple expression is just the product of its arguments. The arity of a tuple may be anything except one, because unary tuples cause syntactic ambiguities with bracketed expressions.","title":"Tuples"},{"location":"references/statix/terms/#lists","text":"Another built-in composite data construction is lists : [ { $ Term \",\" } *] Lists are created by comma-separating terms, enclosing them in square brackets. All terms should have the same type. Given that the type of the terms is T , the type of the list expression will be list(T) . Alternatively, lists can have a variable tail: [ { $ Term \",\" } * | $ Term ] In this syntax, the tail of the list is another term. This term should have type list(T) , where T is again the type of the first terms.","title":"Lists"},{"location":"references/statix/terms/#name-ascription","text":"It is possible to assign names to terms by prefixing the term with a variable name: $ Var @$ Term Note that this does not introduce a new variable with name $Var (except in a rule head , where all variables are introduced implicitly), but rather requires that a variable with corresponding name and type is already introduced. Tip In terms of equality constraints , the ascribe is equal to $Var == $Term . It is used to prevent the duplication of $Term .","title":"Name Ascription"},{"location":"references/statix/terms/#type-ascription","text":"Statix allows to add inline type annotations to terms as follows: $ Term : $ Type The type-checker will validate that the term actually has the specified type, but the runtime behavior in not influenced by these ascriptions. Tip In general, the Statix type-checker should be able to infer all types. However, in case of a type error being reported at an incorrect position, these type ascriptions can help tracing the cause of the error.","title":"Type Ascription"},{"location":"references/statix/terms/#arithmetic-operations","text":"Arithmetic expressions can be inserted in terms as follows: # ( $ ArithExp ) Here, the type of the expression is int . For more information on arithmetic expressions, see Arithmetic Constraints Tip In terms of existential constraints , inline arithmetic expressions have behavior equal to {v} v #= $ArithExp , where v is used at the position of the arithmetic expression. So, for example, {T} T == CONS(#(21 * 2)) is equal to {T v} v #= 21 * 2, T == CONS(v) .","title":"Arithmetic operations"},{"location":"references/statix/terms/#ast-identifier","text":"In Spoofax, all terms in an AST are assigned an unique identifier (the term index) before analysis. This term identifier can be isolated as follows: astId ( $ Term ) Here, the type of $Term can be anything, and the type of the whole term will be astId . AST Identifiers are used to assign properties .","title":"AST Identifier"},{"location":"references/statix/terms/#new","text":"Statix allows inline creation of scopes : new Statically, the new term has type scope . At runtime, this creates a fresh scope, and inserts that at the position of the new term. Tip In terms of existential constraints , the inline new operator has behavior equal to {s} new s , where s is used at the position of the new term. So, for example, {T} T == CLASS(new) is equal to {T s} new s, T == CLASS(s) .","title":"New"},{"location":"references/statix/terms/#paths","text":"Part of a query result is the path from the resolved datum back to the scope where the query started. In order to represent paths, Statix has two built-in constructors: _PathEmpty : Unary constructor that carries a single scope. This constructor has type scope -> path . _PathStep : Ternary constructor that represents a traversed edge in a path. This constructor has type path * label * scope -> path . Note Although the labels in a _PathStep can be bound to a variable, and hence be compared with and included in other terms, no inspection, matching or comparison with label definitions is supported.","title":"Paths"},{"location":"references/statix/terms/#occurrences","text":"Warning Since Spoofax 2.5.15, namespaces and occurrences are deprecated. Statix has built-in support for namespaces. A term embedded in a particular namespace is called an occurrence . Occurrences can be written as follows: $ NamespaceId { $ SpaceTerms } $ NamespaceId { $ SpaceTerms @$ OccurrenceId } In this structure template, $SpaceTerms means a list of terms, separated by spaces. The occurrence identifier can be any term. In case the term has an AST identifier, that value will be used as the identity of the occurrence. Alternatively, the occurrence identifier can be left out: $ NamespaceId { $ SpaceTerms } The default occurrence identifier is - , which means that the occurrence has no identifier. The type of an occurrence literal is occurrence . For more information about namespaces, see the Queries section.","title":"Occurrences"},{"location":"references/statix/terms/#declaration-match","text":"Statix allows to query the current scope for declarations of a particular form: ?$ RelationId [ { $ Term \",\" } ] in $ Scope When using this expression, a functional relation $RelationId must be declared. The terms arguments must correspond to the argument of the relation, and the type of the term is the output type of the relation. For more information on querying the scope graph, see the Queries section. Tip In terms of regular queries , the declaration match is equal to a query with filter e , expecting a single output. E.g. T == ?var[\"x\"] in s is equal to query var filter e and { x' :- x' == \"x\"} in s |-> [(_, (_, T))] .","title":"Declaration Match"},{"location":"references/statix/tests/","text":"Tests \u00b6 Test Format \u00b6 Test Output \u00b6 Substitution Scope Graph Messages","title":"Tests"},{"location":"references/statix/tests/#tests","text":"","title":"Tests"},{"location":"references/statix/tests/#test-format","text":"","title":"Test Format"},{"location":"references/statix/tests/#test-output","text":"Substitution Scope Graph Messages","title":"Test Output"},{"location":"references/stratego/","text":"Stratego \u00b6 The Stratego language caters for the definition of program transformations. Transformations operate on the abstract syntax trees of programs. Abstract syntax trees are represented by means of first-order terms . By using the concrete syntax of a language, transformations can be expressed in the native syntax of the language under transformation, rather than using abstract syntax. A program is structured as a collection of modules , which may import each other. Transformations are defined by means of named rewrite rules . Rules may explicitly invoke rules. Alternatively, rules may be invoked by strategies that define how to combine rules into a more complex transformation using strategy combinators . Context-sensitive transformations can be expressed using dynamic rewrite rules . Starting with Stratego 2, terms and transformation strategies are (gradually) typed . Placeholder Convention \u00b6 In this reference manual we use placeholders to indicate the syntactic structure of language constructs. For example, a rewrite rule has the form $ Label : $ Term - > $ Term in which the $Label is the name of the rule, the first $Term the left-hand side, and the second the right-hand side of the rule. This convention should give an indication of the formal structure of a construct, without going down to the precise details of the syntax definition. As a side effect, the schema also shows the preferred indentation of language constructs where that is applicable. Source \u00b6 The sources of the Stratego implementation can be found at https://github.com/metaborg/stratego : The Stratego language implementation https://github.com/metaborg/strategoxt : The Stratego/XT ecosystem Todo Give more specific links to syntax definition etc.","title":"Stratego"},{"location":"references/stratego/#stratego","text":"The Stratego language caters for the definition of program transformations. Transformations operate on the abstract syntax trees of programs. Abstract syntax trees are represented by means of first-order terms . By using the concrete syntax of a language, transformations can be expressed in the native syntax of the language under transformation, rather than using abstract syntax. A program is structured as a collection of modules , which may import each other. Transformations are defined by means of named rewrite rules . Rules may explicitly invoke rules. Alternatively, rules may be invoked by strategies that define how to combine rules into a more complex transformation using strategy combinators . Context-sensitive transformations can be expressed using dynamic rewrite rules . Starting with Stratego 2, terms and transformation strategies are (gradually) typed .","title":"Stratego"},{"location":"references/stratego/#placeholder-convention","text":"In this reference manual we use placeholders to indicate the syntactic structure of language constructs. For example, a rewrite rule has the form $ Label : $ Term - > $ Term in which the $Label is the name of the rule, the first $Term the left-hand side, and the second the right-hand side of the rule. This convention should give an indication of the formal structure of a construct, without going down to the precise details of the syntax definition. As a side effect, the schema also shows the preferred indentation of language constructs where that is applicable.","title":"Placeholder Convention"},{"location":"references/stratego/#source","text":"The sources of the Stratego implementation can be found at https://github.com/metaborg/stratego : The Stratego language implementation https://github.com/metaborg/strategoxt : The Stratego/XT ecosystem Todo Give more specific links to syntax definition etc.","title":"Source"},{"location":"references/stratego/concrete-syntax/","text":"Concrete Syntax \u00b6 Mixing Grammars \u00b6 ToTerm and FromTerm Imploding Terms \u00b6","title":"Concrete Syntax"},{"location":"references/stratego/concrete-syntax/#concrete-syntax","text":"","title":"Concrete Syntax"},{"location":"references/stratego/concrete-syntax/#mixing-grammars","text":"ToTerm and FromTerm","title":"Mixing Grammars"},{"location":"references/stratego/concrete-syntax/#imploding-terms","text":"","title":"Imploding Terms"},{"location":"references/stratego/dynamic-rules/","text":"Dynamic Rewrite Rules \u00b6","title":"Dynamic Rewrite Rules"},{"location":"references/stratego/dynamic-rules/#dynamic-rewrite-rules","text":"","title":"Dynamic Rewrite Rules"},{"location":"references/stratego/lexical/","text":"Lexical Conventions \u00b6 Module Names \u00b6 Module names can be ??? Identifiers \u00b6 Identifiers used as names of constructors and transformations have the form ID = [ a-zA-Z ][ a-zA-Z0-9 \\ - \\ _ ] * In particular, hyphens can be part of identifiers. Identifiers cannot be followed by identifiers or keywords without intervening whitespace. Integers \u00b6 INT = [ 0 -9 ] + Check syntax of integers Whitespace \u00b6 Spaces, tabs, and newlines are whitespace and can occur between any two tokens. Comments \u00b6 Comments follow the C/Java tradition. That is, the language supports single line comments after \\\\ // a single line comment and multi-line comments between /* and */ /* a multi-line comment can be spread over multiple lines */ Comments can occur anywhere. Multi-line comments cannot be nested currently. Todo but this should be changed so that multi-line comments can be nested Reserved Words \u00b6 Todo provide list of reserved words","title":"Lexical Conventions"},{"location":"references/stratego/lexical/#lexical-conventions","text":"","title":"Lexical Conventions"},{"location":"references/stratego/lexical/#module-names","text":"Module names can be ???","title":"Module Names"},{"location":"references/stratego/lexical/#identifiers","text":"Identifiers used as names of constructors and transformations have the form ID = [ a-zA-Z ][ a-zA-Z0-9 \\ - \\ _ ] * In particular, hyphens can be part of identifiers. Identifiers cannot be followed by identifiers or keywords without intervening whitespace.","title":"Identifiers"},{"location":"references/stratego/lexical/#integers","text":"INT = [ 0 -9 ] + Check syntax of integers","title":"Integers"},{"location":"references/stratego/lexical/#whitespace","text":"Spaces, tabs, and newlines are whitespace and can occur between any two tokens.","title":"Whitespace"},{"location":"references/stratego/lexical/#comments","text":"Comments follow the C/Java tradition. That is, the language supports single line comments after \\\\ // a single line comment and multi-line comments between /* and */ /* a multi-line comment can be spread over multiple lines */ Comments can occur anywhere. Multi-line comments cannot be nested currently. Todo but this should be changed so that multi-line comments can be nested","title":"Comments"},{"location":"references/stratego/lexical/#reserved-words","text":"Todo provide list of reserved words","title":"Reserved Words"},{"location":"references/stratego/modules/","text":"Modules \u00b6 A Stratego program is organised as a collection of modules, which are imported from a main module. File Name and File Extension \u00b6 A module coincides with the file it resides in. It is not possible to define more than one module in a file, which precludes nested modules. The name of a module coincides with the file name, which should be fully qualified relative to a root directory. A Stratego is a file with the extension .str2 for Stratego 2. Modules for the Stratego 1 version of the language have extension .str . The file extension does not feature in the module names used in the language. Consider the following example of a module header: module compilation / translation imports desugaring / desugar Module Names \u00b6 Module names can be hierarchical. For example, consider the following directory structure - trans - compilation - optimization.str2 - translation.str2 - desugaring - desugar.str2 A declaration of or reference to a module uses its fully qualified name, with / to indicate the directory structure, relative to a 'root' directory. For example, if trans is declared as a root , then the module names for the modules above are - compilation/optimization - compilation/translation - desugaring/desugar Module Structure \u00b6 A Stratego module has the following structure, where a single occurrence of a construct can be multiplied: module $ ModuleName imports $ ModuleName signature sort $ Sort constructors $ ConstructorDef rules $ RuleDef strategies $ StrategyDef Thus, a module starts with a module header followed by a list of imports . The name of a module in the header and imports should correspond to the file name, relative to a 'root' directory. The rest of a module consists of signature , rules , and strategies sections, in any order and possibly repeated. A signature section introduces sorts and constructors. Rule definitions and strategy definitions introduce named transformations. The rules and strategies section headers are indicative only; rule and strategy definitions can actually be mixed. Imports \u00b6 A module should import all other modules from which it uses definitions. Imports are non-transitive and may be mutually recursive. Modules can extend rule and strategy definitions from other modules. This allows the modular extension of a language. Libraries \u00b6 A Stratego library is a closed collection of modules. A library can be pre-compiled since client programs may not extend its definitions. A library should provide external definitions to publicize the signatures of constructors and transformations it defines. Source Inclusion \u00b6 Todo Concrete Syntax \u00b6 When using concrete syntax in a module, a .meta file accompanying the module indicates the parse table to use.","title":"Modules"},{"location":"references/stratego/modules/#modules","text":"A Stratego program is organised as a collection of modules, which are imported from a main module.","title":"Modules"},{"location":"references/stratego/modules/#file-name-and-file-extension","text":"A module coincides with the file it resides in. It is not possible to define more than one module in a file, which precludes nested modules. The name of a module coincides with the file name, which should be fully qualified relative to a root directory. A Stratego is a file with the extension .str2 for Stratego 2. Modules for the Stratego 1 version of the language have extension .str . The file extension does not feature in the module names used in the language. Consider the following example of a module header: module compilation / translation imports desugaring / desugar","title":"File Name and File Extension"},{"location":"references/stratego/modules/#module-names","text":"Module names can be hierarchical. For example, consider the following directory structure - trans - compilation - optimization.str2 - translation.str2 - desugaring - desugar.str2 A declaration of or reference to a module uses its fully qualified name, with / to indicate the directory structure, relative to a 'root' directory. For example, if trans is declared as a root , then the module names for the modules above are - compilation/optimization - compilation/translation - desugaring/desugar","title":"Module Names"},{"location":"references/stratego/modules/#module-structure","text":"A Stratego module has the following structure, where a single occurrence of a construct can be multiplied: module $ ModuleName imports $ ModuleName signature sort $ Sort constructors $ ConstructorDef rules $ RuleDef strategies $ StrategyDef Thus, a module starts with a module header followed by a list of imports . The name of a module in the header and imports should correspond to the file name, relative to a 'root' directory. The rest of a module consists of signature , rules , and strategies sections, in any order and possibly repeated. A signature section introduces sorts and constructors. Rule definitions and strategy definitions introduce named transformations. The rules and strategies section headers are indicative only; rule and strategy definitions can actually be mixed.","title":"Module Structure"},{"location":"references/stratego/modules/#imports","text":"A module should import all other modules from which it uses definitions. Imports are non-transitive and may be mutually recursive. Modules can extend rule and strategy definitions from other modules. This allows the modular extension of a language.","title":"Imports"},{"location":"references/stratego/modules/#libraries","text":"A Stratego library is a closed collection of modules. A library can be pre-compiled since client programs may not extend its definitions. A library should provide external definitions to publicize the signatures of constructors and transformations it defines.","title":"Libraries"},{"location":"references/stratego/modules/#source-inclusion","text":"Todo","title":"Source Inclusion"},{"location":"references/stratego/modules/#concrete-syntax","text":"When using concrete syntax in a module, a .meta file accompanying the module indicates the parse table to use.","title":"Concrete Syntax"},{"location":"references/stratego/overlays/","text":"Overlays \u00b6 Todo","title":"Overlays"},{"location":"references/stratego/overlays/#overlays","text":"Todo","title":"Overlays"},{"location":"references/stratego/patterns/","text":"Patterns \u00b6 Term Patterns \u00b6 A term pattern , is a term extended with variables. In the term pattern Plus ( e , Int ( \"0\" )) the identifier e is a variable that stands for any term. Linear vs Non-Linear \u00b6 A pattern is linear if each variable occurs at most once, non-linear otherwise. The non-linear pattern Plus ( e , e ) stands for a Plus term with identical arguments. A term pattern without variables (aka term ) is ground . Substitution \u00b6 Substitution is the process of applying a map from variables to terms to a term pattern, replacing occurrence of variables in the domain of the map with the corresponding terms in the codomain of the map. Substitution is also the name for the mapping of variables to terms. Pattern Matching \u00b6 Pattern matching is the process of matching a ground term against a term pattern. A term t matches a term pattern p iff there is a substition S such that applying the substitution to the pattern S(p) yields the term t .","title":"Patterns"},{"location":"references/stratego/patterns/#patterns","text":"","title":"Patterns"},{"location":"references/stratego/patterns/#term-patterns","text":"A term pattern , is a term extended with variables. In the term pattern Plus ( e , Int ( \"0\" )) the identifier e is a variable that stands for any term.","title":"Term Patterns"},{"location":"references/stratego/patterns/#linear-vs-non-linear","text":"A pattern is linear if each variable occurs at most once, non-linear otherwise. The non-linear pattern Plus ( e , e ) stands for a Plus term with identical arguments. A term pattern without variables (aka term ) is ground .","title":"Linear vs Non-Linear"},{"location":"references/stratego/patterns/#substitution","text":"Substitution is the process of applying a map from variables to terms to a term pattern, replacing occurrence of variables in the domain of the map with the corresponding terms in the codomain of the map. Substitution is also the name for the mapping of variables to terms.","title":"Substitution"},{"location":"references/stratego/patterns/#pattern-matching","text":"Pattern matching is the process of matching a ground term against a term pattern. A term t matches a term pattern p iff there is a substition S such that applying the substitution to the pattern S(p) yields the term t .","title":"Pattern Matching"},{"location":"references/stratego/rewrite-rules/","text":"Rewrite Rules \u00b6 Rewrite rules are used to define basic transformations in Stratego. Simple Rewrite Rules \u00b6 A simple rewrite rule has the form $ Id : $ Term - > $ Term It consists of a name that identifies the rule, a left-hand side term pattern, and a right-hand side term pattern. Applying a rule to a term t entails matching t against the left-hand side, binding any variables and replacing it with an instantiation of the right-hand side. For example, the rewrite rule DeMorgan DeMorgan : Not ( And ( e1 , e2 )) - > Or ( Not ( e1 ), Not ( e2 )) transforms a negation of a conjunction to a disjunction of negations. Applying this rule to the term Not(And(Var(p), Var(q))) results in a substitution binding Var(p) to e1 and Var(q) to e2 , and the instantiation Or(Not(Var(p)), Not(Var(q))) of the right-hand side of the rule. Note that a rewrite rule defines a partial computation . Only if the pattern match succeeds is the transformation applied. Such (pattern match) failure is a first-class citizen in Stratego and its effects are discussed with strategy combinators . Rules with the Same Name \u00b6 Multiple rewrite rules may have the same name. When a (simple) rewrite rule fails to apply to a term, the next rule with the same name is tried. For examples, the following rules define desugarings of expressions. rules desugar-exp :: Exp - > Exp desugar-exp : Seq ([], e ) - > e desugar-exp : Seq ([ e ], Unit ()) - > e desugar-exp : Seq ([ e1 , e2 | e * ], e3 ) - > Seq ([ e1 ], Seq ([ e2 | e * ], e3 )) desugar-exp : Seq ([ Seq ( e1 * , e1 ) | e2 * ], e2 ) - > Seq ([ e1 * , e1 | e2 * ], e2 ) desugar-exp : Let ( dec * , [ e1 , e2 | e * ]) - > Let ( dec * , [ Seq ([ e1 , e2 | e * ], Unit ())]) When one rule fails to apply, the next rule is tried. When the left-hand sides are non-overlapping, the order of the rules does not matter. In case of overlap, the rules are tried in textual order. When overlapping rules are defined in separate modules, the order is undefined. Note We should consider specificity ordering. Conditional Rewrite Rules \u00b6 A conditional rewrite rule checks a condition or performs a side computation before instantiating the right-hand side of the rule. The basic form of a conditional rewrite rule in Stratego is $ Id : $ Term - > $ Term where $ Strategy where the strategy expression represents a computation that may fail. When the condition fails, the expectation is that some other rule will pick up the computation. For example, the following conditional rewrite rules combine pattern matching with the predicate is-atom to select the rule to apply: rules rco-atom :: Exp - > ( List ( Dec ) * Exp ) rco-atom : Let ( dec * , [ e ]) - > ( dec * , e ) where < is-atom > e rco-atom : e - > ([], e ) where < is-atom > e rco-atom : e - > ([ VarDec ( x , Tid ( \"int\" ), e )], Var ( x )) where < not ( is-atom )> e where < newname > \"tmp\" => x When the condition fails, the application of the rule fails (and the next rule is tried if there is one). Side Computations with With \u00b6 Failure is not always expected. When a condition is used to express a side computation, the expection may be that it should always succeed. However, due to a programming error (e.g. a missed case), the condition may fail in some cases. To guard against such programming errors, the with condition expresses that the programmer expects a side computation to always succeed. When a with clause fails, the program should fail with an exception (and a stack trace). $ Id : $ Term - > $ Term with $ Strategy For example, a translate transformation from expressions to list of (stack machine) instructions may use side computations to recursively apply the transformation. translate :: Exp - > List ( Instr ) translate : Plus ( e1 , e2 ) - > < concat >[ instrs1 , instrs2 , [ ADD ()]] with < translate > e1 => instrs1 with < translate > e2 => instrs2 The recursive applications are not expected to fail. Therefore a with is used. Thus, use a where clause when the condition is to determine whether to apply the rule, and use a with clause to perform a side computation, which has to succeed and will trow an exception when it fails. Combining With and Where \u00b6 Rewrite rules can combine multiple with/where clauses in any order. $ Id : $ Term - > $ Term where $ Strategy with $ Strategy with $ Strategy where $ Strategy The only rule is that with clauses should always succeed. For example, the following explicate-exp rule defines the translation of an operator expression in a source language to an operator expression in a target language. explicate-exp :: Exp - > CExp explicate-exp : op # ( es ) - > cop # ( atms ) where < is-operator > op with < map ( explicate-atom )> es => atms with < operator-to-coperator > op => cop The where condition tests whether the rule should be applied using the is-operator strategy. (By using generic term deconstruction to obtain the term constructor.) The with premises define side computations. Parameterized Rewrite Rules \u00b6 Rewrite rules can be parameterized with transformation strategies and with terms. In general, a parameterized rule has the form: $ Id ( $ StrategyArg , ... | $ TermArg , ... ) : $ Term - > $ Term where $ Strategy ... The strategy parameters represent computations that can be applied in the body of the rule. The term parameters are terms that are evaluated eagerly at the call site. The (arity of the) parameters of a rule are part of its identity. That is, rules with the same name, but different numbers of parameters are different. For example, the following rules define reversal of a list with an accumulator: rules reverse :: List ( a ) - > List ( a ) reverse (| List ( a )) :: List ( a ) - > List ( a ) s reverse : xs - > < reverse (|[])> xs reverse (| xs ) : [] - > xs reverse (| xs ) : [ y | ys ] - > < reverse-acc (|[ y | xs ])> ys When leaving out the term parameters, the bar can be left out as well $ Id ( $ StrategyArg ) : $ Term - > $ Term where $ Strategy For example, the map(s) strategy applies transformation s to each element of a list: map ( a - > b ) :: List ( a ) - > List ( b ) map ( s ) : [] - > [] map ( s ) : [ hd | tl ] - > [< s > hd | < map ( s )> tl ] The simple rewrite rules are above are the special case in which there are no strategy and term parameters. In that case, the parentheses can be left out as well. Note In the absence of a type system, the distinction between strategy arguments and term arguments was made based on the syntactic distinction. In a future version of the language, this syntactic distiction may no longer be necessary based on types. Typing Rewrite Rules \u00b6 As noted in the type section, rewrite rules can be typed using a signature of the form $Id($StrategyTypes | $TermTypes) :: $Type -> $Type Not providing a type signature amounts to declaring the rule as having signature $Id(?|?) :: ? -> ? That is, nothing is known about the term that is transformed or the strategies and terms that are passed. But do note that the arity of the strategy and term arguments is relevant for identifying the transformation rule that is defined.","title":"Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#rewrite-rules","text":"Rewrite rules are used to define basic transformations in Stratego.","title":"Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#simple-rewrite-rules","text":"A simple rewrite rule has the form $ Id : $ Term - > $ Term It consists of a name that identifies the rule, a left-hand side term pattern, and a right-hand side term pattern. Applying a rule to a term t entails matching t against the left-hand side, binding any variables and replacing it with an instantiation of the right-hand side. For example, the rewrite rule DeMorgan DeMorgan : Not ( And ( e1 , e2 )) - > Or ( Not ( e1 ), Not ( e2 )) transforms a negation of a conjunction to a disjunction of negations. Applying this rule to the term Not(And(Var(p), Var(q))) results in a substitution binding Var(p) to e1 and Var(q) to e2 , and the instantiation Or(Not(Var(p)), Not(Var(q))) of the right-hand side of the rule. Note that a rewrite rule defines a partial computation . Only if the pattern match succeeds is the transformation applied. Such (pattern match) failure is a first-class citizen in Stratego and its effects are discussed with strategy combinators .","title":"Simple Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#rules-with-the-same-name","text":"Multiple rewrite rules may have the same name. When a (simple) rewrite rule fails to apply to a term, the next rule with the same name is tried. For examples, the following rules define desugarings of expressions. rules desugar-exp :: Exp - > Exp desugar-exp : Seq ([], e ) - > e desugar-exp : Seq ([ e ], Unit ()) - > e desugar-exp : Seq ([ e1 , e2 | e * ], e3 ) - > Seq ([ e1 ], Seq ([ e2 | e * ], e3 )) desugar-exp : Seq ([ Seq ( e1 * , e1 ) | e2 * ], e2 ) - > Seq ([ e1 * , e1 | e2 * ], e2 ) desugar-exp : Let ( dec * , [ e1 , e2 | e * ]) - > Let ( dec * , [ Seq ([ e1 , e2 | e * ], Unit ())]) When one rule fails to apply, the next rule is tried. When the left-hand sides are non-overlapping, the order of the rules does not matter. In case of overlap, the rules are tried in textual order. When overlapping rules are defined in separate modules, the order is undefined. Note We should consider specificity ordering.","title":"Rules with the Same Name"},{"location":"references/stratego/rewrite-rules/#conditional-rewrite-rules","text":"A conditional rewrite rule checks a condition or performs a side computation before instantiating the right-hand side of the rule. The basic form of a conditional rewrite rule in Stratego is $ Id : $ Term - > $ Term where $ Strategy where the strategy expression represents a computation that may fail. When the condition fails, the expectation is that some other rule will pick up the computation. For example, the following conditional rewrite rules combine pattern matching with the predicate is-atom to select the rule to apply: rules rco-atom :: Exp - > ( List ( Dec ) * Exp ) rco-atom : Let ( dec * , [ e ]) - > ( dec * , e ) where < is-atom > e rco-atom : e - > ([], e ) where < is-atom > e rco-atom : e - > ([ VarDec ( x , Tid ( \"int\" ), e )], Var ( x )) where < not ( is-atom )> e where < newname > \"tmp\" => x When the condition fails, the application of the rule fails (and the next rule is tried if there is one).","title":"Conditional Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#side-computations-with-with","text":"Failure is not always expected. When a condition is used to express a side computation, the expection may be that it should always succeed. However, due to a programming error (e.g. a missed case), the condition may fail in some cases. To guard against such programming errors, the with condition expresses that the programmer expects a side computation to always succeed. When a with clause fails, the program should fail with an exception (and a stack trace). $ Id : $ Term - > $ Term with $ Strategy For example, a translate transformation from expressions to list of (stack machine) instructions may use side computations to recursively apply the transformation. translate :: Exp - > List ( Instr ) translate : Plus ( e1 , e2 ) - > < concat >[ instrs1 , instrs2 , [ ADD ()]] with < translate > e1 => instrs1 with < translate > e2 => instrs2 The recursive applications are not expected to fail. Therefore a with is used. Thus, use a where clause when the condition is to determine whether to apply the rule, and use a with clause to perform a side computation, which has to succeed and will trow an exception when it fails.","title":"Side Computations with With"},{"location":"references/stratego/rewrite-rules/#combining-with-and-where","text":"Rewrite rules can combine multiple with/where clauses in any order. $ Id : $ Term - > $ Term where $ Strategy with $ Strategy with $ Strategy where $ Strategy The only rule is that with clauses should always succeed. For example, the following explicate-exp rule defines the translation of an operator expression in a source language to an operator expression in a target language. explicate-exp :: Exp - > CExp explicate-exp : op # ( es ) - > cop # ( atms ) where < is-operator > op with < map ( explicate-atom )> es => atms with < operator-to-coperator > op => cop The where condition tests whether the rule should be applied using the is-operator strategy. (By using generic term deconstruction to obtain the term constructor.) The with premises define side computations.","title":"Combining With and Where"},{"location":"references/stratego/rewrite-rules/#parameterized-rewrite-rules","text":"Rewrite rules can be parameterized with transformation strategies and with terms. In general, a parameterized rule has the form: $ Id ( $ StrategyArg , ... | $ TermArg , ... ) : $ Term - > $ Term where $ Strategy ... The strategy parameters represent computations that can be applied in the body of the rule. The term parameters are terms that are evaluated eagerly at the call site. The (arity of the) parameters of a rule are part of its identity. That is, rules with the same name, but different numbers of parameters are different. For example, the following rules define reversal of a list with an accumulator: rules reverse :: List ( a ) - > List ( a ) reverse (| List ( a )) :: List ( a ) - > List ( a ) s reverse : xs - > < reverse (|[])> xs reverse (| xs ) : [] - > xs reverse (| xs ) : [ y | ys ] - > < reverse-acc (|[ y | xs ])> ys When leaving out the term parameters, the bar can be left out as well $ Id ( $ StrategyArg ) : $ Term - > $ Term where $ Strategy For example, the map(s) strategy applies transformation s to each element of a list: map ( a - > b ) :: List ( a ) - > List ( b ) map ( s ) : [] - > [] map ( s ) : [ hd | tl ] - > [< s > hd | < map ( s )> tl ] The simple rewrite rules are above are the special case in which there are no strategy and term parameters. In that case, the parentheses can be left out as well. Note In the absence of a type system, the distinction between strategy arguments and term arguments was made based on the syntactic distinction. In a future version of the language, this syntactic distiction may no longer be necessary based on types.","title":"Parameterized Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#typing-rewrite-rules","text":"As noted in the type section, rewrite rules can be typed using a signature of the form $Id($StrategyTypes | $TermTypes) :: $Type -> $Type Not providing a type signature amounts to declaring the rule as having signature $Id(?|?) :: ? -> ? That is, nothing is known about the term that is transformed or the strategies and terms that are passed. But do note that the arity of the strategy and term arguments is relevant for identifying the transformation rule that is defined.","title":"Typing Rewrite Rules"},{"location":"references/stratego/strategies/","text":"Strategy Definitions \u00b6 Strategy definitions give a name to a strategy expression. Simple Definitions \u00b6 A simple strategy definition gaves a name to a strategy expression . $ Id = s For example, the following definition defines desugar as an application of the innermost strategy to the rewrite rule(s) desugar-exp . strategies desugar :: Module - > Module desugar = innermost ( desugar-exp ) Parameterized Definitions \u00b6 Just like rewrite rules , strategy definitions can be parameterized with strategies and terms. $ Id ( $ StrategyArg , ... | $ TermArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... | t1 , ... ) = s When a strategy has no term arguments, the bar can be left out: $ Id ( $ StrategyArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... ) = s Simple strategy definitions are the special case in which a strategy does not have strategy and term arguments. For example, the following definition defines topdown(s) in terms of sequential composition and generic traversal: topdown ( TP ) :: TP topdown ( s ) = s ; all ( topdown ( s )) Extending Definitions \u00b6 Just like rewrite rules, strategy definitions can have multiple definitions. In case a strategy expression fails to apply, the next definition is applied. When definitions are in the same module, definitions are applied in the textual order they are defined in. When definitions are defined in separate modules, the order is undefined. External Definitions \u00b6 external definitions libraries Todo finish this section on external definitions Local Definitions \u00b6 Todo finish this section on local definitions","title":"Strategy Definitions"},{"location":"references/stratego/strategies/#strategy-definitions","text":"Strategy definitions give a name to a strategy expression.","title":"Strategy Definitions"},{"location":"references/stratego/strategies/#simple-definitions","text":"A simple strategy definition gaves a name to a strategy expression . $ Id = s For example, the following definition defines desugar as an application of the innermost strategy to the rewrite rule(s) desugar-exp . strategies desugar :: Module - > Module desugar = innermost ( desugar-exp )","title":"Simple Definitions"},{"location":"references/stratego/strategies/#parameterized-definitions","text":"Just like rewrite rules , strategy definitions can be parameterized with strategies and terms. $ Id ( $ StrategyArg , ... | $ TermArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... | t1 , ... ) = s When a strategy has no term arguments, the bar can be left out: $ Id ( $ StrategyArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... ) = s Simple strategy definitions are the special case in which a strategy does not have strategy and term arguments. For example, the following definition defines topdown(s) in terms of sequential composition and generic traversal: topdown ( TP ) :: TP topdown ( s ) = s ; all ( topdown ( s ))","title":"Parameterized Definitions"},{"location":"references/stratego/strategies/#extending-definitions","text":"Just like rewrite rules, strategy definitions can have multiple definitions. In case a strategy expression fails to apply, the next definition is applied. When definitions are in the same module, definitions are applied in the textual order they are defined in. When definitions are defined in separate modules, the order is undefined.","title":"Extending Definitions"},{"location":"references/stratego/strategies/#external-definitions","text":"external definitions libraries Todo finish this section on external definitions","title":"External Definitions"},{"location":"references/stratego/strategies/#local-definitions","text":"Todo finish this section on local definitions","title":"Local Definitions"},{"location":"references/stratego/strategy-combinators-match-build/","text":"Matching and Building Terms \u00b6 In previous chapters we have presented rewrite rules as basic transformation steps. However, rules are not really atomic transformation actions. To see this, consider what happens when the rewrite rule is applied. First it matches the subject term against the pattern And(Or(x, y), z) in the left-hand side. This means that a substitution for the variables x, y, and z is sought, that makes the pattern equal to the subject term. If the match fails, the rule fails. If the match succeeds, the pattern Or(And(x, z), And(y, z)) on the right-hand side is instantiated with the bindings found during the match of the left-hand side. The instantiated term then replaces the original subject term. Furthermore, the rule limits the scope of the variables occurring in the rule. That is, the variables x, y, z are local to this rule. After the rule is applied the bindings to these variables are invisible again. Thus, rather than considering rules as the atomic actions of transformation programs, Stratego provides their constituents, that is building terms from patterns and matching terms against patterns, as atomic actions, and makes these available to the programmer. In this chapter, you will learn these basic actions and their use in the composition of more complex operations such as various flavors of rewrite rules. 8.1. Building Terms The build operation !p replaces the subject term with the instantiation of the pattern p using the bindings from the environment to the variables occurring in p. For example, the strategy !Or(And(x, z), And(y, z)) replaces the subject term with the instantiation of Or(And(x, z), And(y, z)) using bindings to variables x, y and z. stratego> !Int(\"10\") Int(\"10\") stratego> !Plus(Var(\"a\"), Int(\"10\")) Plus(Var(\"a\"), Int(\"10\")) It is possible to build terms with variables. We call this building a term pattern. A pattern is a term with meta-variables. The current term is replaced by an instantiation of pattern p. stratego> :binding e e is bound to Var(\"b\") stratego> !Plus(Var(\"a\"),e) Plus(Var(\"a\"),Var(\"b\")) stratego> !e Var(\"b\") 8.2. Matching Terms Pattern matching allows the analysis of terms. The simplest case is matching against a literal term. The match operation ?t matches the subject term against the term t. Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(Int(\"3\"),Var(\"b\")) command failed Matching against a term pattern with variables binds those variables to (parts of) the current term. The match strategy ?x compares the current term (t) to variable x. It binds variable x to term t in the environment. A variable can only be bound once, or to the same term. Plus(Var(\"a\"),Int(\"3\")) stratego> ?e stratego> :binding e e is bound to Plus(Var(\"a\"),Int(\"3\")) stratego> !Int(\"17\") stratego> ?e command failed The general case is matching against an arbitrary term pattern. The match strategy ?p compares the current term to a pattern p. It will add bindings for the variables in pattern p to the environment. The wildcard _ in a match will match any term. Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(e,_) stratego> :binding e e is bound to Var(\"a\") Plus(Var(\"a\"),Int(\"3\")) Patterns may be non-linear. Multiple occurrences of the same variable can occur and each occurrence matches the same term. Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(e,e) command failed stratego> !Plus(Var(\"a\"),Var(\"a\")) stratego> ?Plus(e,e) stratego> :binding e e is bound to Var(\"a\") Non-linear pattern matching is a way to test equality of terms. Indeed the equality predicates from the Stratego Library are defined using non-linear pattern matching: equal = ?(x, x) equal(|x) = ?x The equal strategy tests whether the current term is a a pair of the same terms. The equal(|x) strategy tests whether the current term is equal to the argument term. stratego> equal = ?(x, x) stratego> !(\"a\", \"a\") (\"a\", \"a\") stratego> equal (\"a\", \"a\") stratego> !(\"a\", \"b\") (\"a\", \"b\") stratego> equal command failed stratego> equal(|x) = ?x stratego> !Foo(Bar()) Foo(Bar) stratego> equal(|Foo(Baz())) command failed stratego> equal(|Foo(Bar())) Foo(Bar) 8.3. Implementing Rewrite Rules Match and build are first-class citizens in Stratego, which means that they can be used and combined just like any other strategy expressions. In particular, we can implement rewrite rules using these operations, since a rewrite rule is basically a match followed by a build. For example, consider the following combination of match and build: Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(e1, e2); !Plus(e2, e1) Plus(Int(\"3\"),Var(\"a\")) This combination first recognizes a term, binds variables to the pattern in the match, and then replaces the current term with the instantiation of the build pattern. Note that the variable bindings are propagated from the match to the build. Stratego provides syntactic sugar for various combinations of match and build. We\u2019ll explore these in the rest of this chapter. 8.3.1. Anonymous Rewrite Rule An anonymous rewrite rule (p1 -> p2) transforms a term matching p1 into an instantiation of p2. Such a rule is equivalent to the sequence ?p1; !p2. Plus(Var(\"a\"),Int(\"3\")) stratego> (Plus(e1, e2) -> Plus(e2, e1)) Plus(Int(\"3\"),Var(\"a\")) 8.3.2. Term variable scope Once a variable is bound it cannot be rebound to a different term. Thus, when we have applied an anonymous rule once, its variables are bound and the next time it is applied it only succeeds for the same term. For example, in the next session the second application of the rule fails, because e2 is bound to Int(\"3\") and does not match with Var(\"b\"). stratego> !Plus(Var(\"a\"),Int(\"3\")) Plus(Var(\"a\"),Int(\"3\")) stratego> (Plus(e1,e2) -> Plus(e2,e1)) Plus(Int(\"3\"),Var(\"a\")) stratego> :binding e1 e1 is bound to Var(\"a\") stratego> :binding e2 e2 is bound to Int(\"3\") stratego> !Plus(Var(\"a\"),Var(\"b\")) Plus(Var(\"a\"),Var(\"b\")) stratego> (Plus(e1,e2) -> Plus(e2,e1)) command failed To use a variable name more than once Stratego provides term variable scope. A scope {x1,...,xn : s} locally undefines the variables xi. That is, the binding to a variable xi outside the scope is not visible inside it, nor is the binding to xi inside the scope visible outside it. For example, to continue the session above, if we wrap the anonymous swap rule in a scope for its variables, it can be applied multiple times. stratego> !Plus(Var(\"a\"),Int(\"3\")) Plus(Var(\"a\"),Int(\"3\")) stratego> {e3,e4 : (Plus(e3,e4) -> Plus(e4,e3))} Plus(Var(\"a\"),Int(\"3\")) stratego> :binding e3 e3 is not bound to a term stratego> !Plus(Var(\"a\"),Var(\"b\")) Plus(Var(\"a\"),Var(\"b\")) stratego> {e3,e4 : (Plus(e3,e4) -> Plus(e4,e3))} Plus(Var(\"b\"),Var(\"a\")) Of course we can name such a scoped rule using a strategy definition, and then invoke it by its name: stratego> SwapArgs = {e1,e2 : (Plus(e1,e2) -> Plus(e2,e1))} stratego> !Plus(Var(\"a\"),Int(\"3\")) Plus(Var(\"a\"),Int(\"3\")) stratego> SwapArgs Plus(Int(\"3\"),Var(\"a\")) 8.3.3. Implicit Variable Scope When using match and build directly in a strategy definition, rather than in the form of a rule, the definition contains free variables. Strictly speaking such variables should be declared using a scope, that is one should write SwapArgs = {e1,e2 : (Plus(e1,e2) -> Plus(e2,e1))} However, since declaring all variables at the top of a definition is distracting and does not add much to the definition, such a scope declaration can be left out. Thus, one can write SwapArgs = (Plus(e1,e2) -> Plus(e2,e1)) instead. The scope is automatically inserted by the compiler. This implies that there is no global scope for term variables. Of course, variables in inner scopes should be declared where necessary. In particular, note that variable scope is not inserted for strategy definitions in a let binding, such as let SwapArgs = (Plus(e1,e2) -> Plus(e2,e1)) in ... end While the variables are bound in the enclosing definition, they are not restricted to SwapArgs in this case, since in a let you typically want to use bindings to variables in the enclosing code. 8.3.4. Where Often it is useful to apply a strategy only to test whether some property holds or to compute some auxiliary result. For this purpose, Stratego provides the where(s) combinator, which applies s to the current term, but restores that term afterwards. Any bindings to variables are kept, however. Plus(Int(\"14\"),Int(\"3\")) stratego> where(?Plus(Int(i),Int(j)); (i,j) => k) Plus(Int(\"14\"),Int(\"3\")) stratego> :binding i i is bound to \"14\" stratego> :binding k k is bound to \"17\" With the match and build constructs where(s) is in fact just syntactic sugar for {x: ?x; s; !x} with x a fresh variable not occurring in s. Thus, the current subject term is saved by binding it to a new variable x, then the strategy s is applied, and finally, the original term is restored by building x. We saw the use of where in the definition of if-then-else in Chapter 7. 8.3.5. Conditional rewrite rule A simple rewrite rule succeeds if the match of the left-hand side succeeds. Sometimes it is useful to place additional requirements on the application of a rule, or to compute some value for use in the right-hand side of the rule. This can be achieved with conditional rewrite rules. A conditional rule L: p1 -> p2 where s is a simple rule extended with an additional computation s which should succeed in order for the rule to apply. The condition can be used to test properties of terms in the left-hand side, or to compute terms to be used in the right-hand side. The latter is done by binding such new terms to variables used in the right-hand side. For example, the EvalPlus rule in the following session uses a condition to compute the sum of i and j: stratego> EvalPlus: Plus(Int(i),Int(j)) -> Int(k) where !(i,j); addS; ?k stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> EvalPlus Int(\"17\") A conditional rule can be desugared similarly to an unconditional rule. That is, a conditional rule of the form L : p1 -> p2 where s is syntactic sugar for L = ?p1; where(s); !p2 Thus, after the match with p1 succeeds the strategy s is applied to the subject term. Only if the application of s succeeds, is the right-hand side p2 built. Note that since s is applied within a where, the build !p2 is applied to the original subject term; only variable bindings computed within s can be used in p2. As an example, consider the following constant folding rule, which reduces an addition of two integer constants to the constant obtained by computing the addition. EvalPlus : Add(Int(i),Int(j)) -> Int(k) where !(i,j); addS; ?k The addition is computed by applying the primitive strategy addS to the pair of integers (i,j) and matching the result against the variable k, which is then used in the right-hand side. This rule is desugared to EvalPlus = ?Add(Int(i),Int(j)); where(!(i,j); addS; ?k); !Int(k) 8.3.6. Lambda Rules Sometimes it is useful to define a rule anonymously within a strategy expression. The syntax for anonymous rules with scopes is a bit much since it requires enumerating all variables. A lambda rule of the form p1 -> p2 where s \\ is an anonymous rewrite rule for which the variables in the left-hand side p1 are local to the rule, that is, it is equivalent to an expression of the form {x1,...,xn : (p1 -> p2 where s)} with x1,\u2026,xn the variables of p1. This means that any variables used in s and p2 that do not occur in p1 are bound in the context of the rule. A typical example of the use of an anonymous rule is stratego> ![(1,2),(3,4),(5,6)] [(1,2),(3,4),(5,6)] stratego> map( (x, y) -> x ) [1,3,5] 8.4. Apply and Match One frequently occuring scenario is that of applying a strategy to a term and then matching the result against a pattern. This typically occurs in the condition of a rule. In the constant folding example above we saw this scenario: EvalPlus : Add(Int(i),Int(j)) -> Int(k) where !(i,j); addS; ?k In the condition, first the term (i,j) is built, then the strategy addS is applied to it, and finally the result is matched against the pattern k. To improve the readability of such expressions, the following two constructs are provided. The operation p captures the notion of applying a strategy to a term, i.e., the scenario !p; s. The operation s => p capture the notion of applying a strategy to the current subject term and then matching the result against the pattern p, i.e., s; ?p. The combined operation p1 => p2 thus captures the notion of applying a strategy to a term p1 and matching the result against p2, i.e, !p1; s; ?p2. Using this notation we can improve the constant folding rule above as EvalPlus : Add(Int(i),Int(j)) -> Int(k) where (i,j) => k Applying Strategies in Build. Sometimes it useful to apply a strategy directly to a subterm of a pattern, for example in the right-hand side of a rule, instead of computing a value in a condition, binding the result to a variable, and then using the variable in the build pattern. The constant folding rule above, for example, could be further simplified by directly applying the addition in the right-hand side: EvalPlus : Add(Int(i),Int(j)) -> Int( (i,j)) This abbreviates the conditional rule above. In general, a strategy application in a build pattern can always be expressed by computing the application before the build and binding the result to a new variable, which then replaces the application in the build pattern. Another example is the following definition of the map(s) strategy, which applies a strategy to each term in a list: map(s) : [] -> [] map(s) : [x | xs] -> [ x | xs] 8.5 Auxiliary values and assignment As mentioned above, it can be convenient to apply a strategy only to compute some auxiliary result. Although the where construct created to constrain when a rule or strategy may apply (as covered in Sections 8.3.4 and 8.3.5 above) can be used for this purpose, often it is better to use the with strategy specifically designed with computing auxiliaries in mind. Specifically, if s is any strategy, the strategy with(s) executes s on the current subject term and then restores the current subject term. In other words, s is executed solely for its side effects, such as binding variables. In this respect, with is like where. However, with(s) differs in a key way: if the strategy s fails, Stratego immediately stops with an error, reporting the strategy that failed. Thus, if with(s) is used for auxiliary computations that really should not fail if the transformation is proceeding properly, there is no opportunity for Stratego to backtrack and/or continue applying other strategies, potentially creating an error at a point far removed from the place that things actually went awry. In short, using with(s) instead of where(s) any time the intention is not to constrain the applicability of a rule or strategy generally makes debugging your Stratego program significantly easier. Also as with where, we can add a with clause to a rewrite rule in exactly the same way. In other words, L : p1 -> p2 with s is syntactic sugar for L = ?p1; with(s); !p2 So as an example, the where version of EvalPlus from Section 8.4 would be better cast as EvalPlus : Add(Int(i),Int(j)) -> Int(k) with (i,j) => k because after all, there is no chance that Stratego will be unable to add two integers, and so if the contents of the with clause fails it means something has gone wrong \u2013 perhaps an Int term somehow ended up with a parameter that does not actually represent an integer \u2013 and Stratego should quit now. Furthermore, in setting auxiliary variables often the full power of Stratego strategies is not used, but rather new terms are simply built as needed. Stratego provides an := operator for this purpose; the above rule can be written probably more clearly as EvalPlus : Add(Int(i),Int(j)) -> Int(k) with k := (i,j) Technically, p1 := p2 (which can be used anywhere a strategy is called for, although it is primarily useful in with and where clauses) is just syntactic sugar for !p2; ?p1. In other words, it builds the value p2, and then matches it with p1. In the typical case that p1 is just a variable, this ends up assigning the result of building the expression p2 to that variable. To sum up, we have actually already seen an example of both with and := in the \u201cglue\u201d strategy used to run a Stratego transformation via Editor Services: do-eval: (selected, _, _, path, project-path) -> (filename, result) with filename := path ; result := selected To make the operation of this rule clearer, the two components of the outcome are separated into auxiliary computations in the with clause, and these two auxiliaries are implemented as assignments with the := operator. Moreover, if either the eval strategy fails or if Stratego is unable to compute the proper output filename, there is no point in continuing. So Stratego will simply terminate immediately and report the error. 8.6 Wrap and Project Term wrapping and projection are concise idioms for constructing terms that wrap the current term and for extracting subterms from the current term. 8.6.1. Term Wrap One often write rules of the form x -> Foo(Bar(x)), i.e. wrapping a term pattern around the current term. Using rule syntax this is quite verbose. The syntactic abstraction of term wraps, allows the concise specification of such little transformations as !Foo(Bar( )). In general, a term wrap is a build strategy !p[ ] containing one or more strategy applications that are not applied to a term. When executing the the build operation, each occurrence of such a strategy application is replaced with the term resulting from applying s to the current subject term, i.e., the one that is being replaced by the build. The following sessions illustrate some uses of term wraps: 3 stratego> !( , ) (3,3) stratego> !( , ) (4,3) stratego> !\"foobar\" \"foobar\" stratego> !Call( , []) Call(\"foobar\", []) stratego> mod2 = ( ,2) stratego> !6 6 stratego> mod2 0 As should now be a common pattern, term projects are implemented by translation to a combination of match and build expressions. Thus, a term wrap !p[ ] is translated to a strategy expression {x: where(s => x); !p[x]} where x is a fresh variable not occurring in s. In other words, the strategy s is applied to the current subject term, i.e., the term to which the build is applied. As an example, the term wrap !Foo(Bar( )) is desugared to the strategy {x: where(id => x); !Foo(Bar(x))} which after simplification is equivalent to {x: ?x; !Foo(Bar(x))}, i.e., exactly the original lambda rule x -> Foo(Bar(x)). 8.6.2. Term Project Term projections are the match dual of term wraps. Term projections can be used to project a subterm from a term pattern. For example, the expression ?And( ,x) matches terms of the form And(t1,t2) and reduces them to the first subterm t1. Another example is the strategy map(?FunDec( , , )) which reduces a list of function declarations to a list of the names of the functions, i.e., the first arguments of the FunDec constructor. Here are some more examples: [1,2,3] stratego> ?[_| ] [2,3] stratego> !Call(\"foobar\", []) Call(\"foobar\", []) stratego> ?Call( , []) \"foobar\" Term projections can also be used to apply additional constraints to subterms in a match pattern. For example, ?Call(x, <?args; length => 3>) matches only with function calls with three arguments. A match expression ?p[ ] is desugared as {x: ?p[x]; x} That is, after the pattern p[x] matches, it is reduced to the subterm bound to x to which s is applied. The result is also the result of the projection. When multiple projects are used within a match the outcome is undefined, i.e., the order in which the projects will be performed can not be counted on.","title":"Matching and Building Terms"},{"location":"references/stratego/strategy-combinators-match-build/#matching-and-building-terms","text":"In previous chapters we have presented rewrite rules as basic transformation steps. However, rules are not really atomic transformation actions. To see this, consider what happens when the rewrite rule is applied. First it matches the subject term against the pattern And(Or(x, y), z) in the left-hand side. This means that a substitution for the variables x, y, and z is sought, that makes the pattern equal to the subject term. If the match fails, the rule fails. If the match succeeds, the pattern Or(And(x, z), And(y, z)) on the right-hand side is instantiated with the bindings found during the match of the left-hand side. The instantiated term then replaces the original subject term. Furthermore, the rule limits the scope of the variables occurring in the rule. That is, the variables x, y, z are local to this rule. After the rule is applied the bindings to these variables are invisible again. Thus, rather than considering rules as the atomic actions of transformation programs, Stratego provides their constituents, that is building terms from patterns and matching terms against patterns, as atomic actions, and makes these available to the programmer. In this chapter, you will learn these basic actions and their use in the composition of more complex operations such as various flavors of rewrite rules. 8.1. Building Terms The build operation !p replaces the subject term with the instantiation of the pattern p using the bindings from the environment to the variables occurring in p. For example, the strategy !Or(And(x, z), And(y, z)) replaces the subject term with the instantiation of Or(And(x, z), And(y, z)) using bindings to variables x, y and z. stratego> !Int(\"10\") Int(\"10\") stratego> !Plus(Var(\"a\"), Int(\"10\")) Plus(Var(\"a\"), Int(\"10\")) It is possible to build terms with variables. We call this building a term pattern. A pattern is a term with meta-variables. The current term is replaced by an instantiation of pattern p. stratego> :binding e e is bound to Var(\"b\") stratego> !Plus(Var(\"a\"),e) Plus(Var(\"a\"),Var(\"b\")) stratego> !e Var(\"b\") 8.2. Matching Terms Pattern matching allows the analysis of terms. The simplest case is matching against a literal term. The match operation ?t matches the subject term against the term t. Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(Int(\"3\"),Var(\"b\")) command failed Matching against a term pattern with variables binds those variables to (parts of) the current term. The match strategy ?x compares the current term (t) to variable x. It binds variable x to term t in the environment. A variable can only be bound once, or to the same term. Plus(Var(\"a\"),Int(\"3\")) stratego> ?e stratego> :binding e e is bound to Plus(Var(\"a\"),Int(\"3\")) stratego> !Int(\"17\") stratego> ?e command failed The general case is matching against an arbitrary term pattern. The match strategy ?p compares the current term to a pattern p. It will add bindings for the variables in pattern p to the environment. The wildcard _ in a match will match any term. Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(e,_) stratego> :binding e e is bound to Var(\"a\") Plus(Var(\"a\"),Int(\"3\")) Patterns may be non-linear. Multiple occurrences of the same variable can occur and each occurrence matches the same term. Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(e,e) command failed stratego> !Plus(Var(\"a\"),Var(\"a\")) stratego> ?Plus(e,e) stratego> :binding e e is bound to Var(\"a\") Non-linear pattern matching is a way to test equality of terms. Indeed the equality predicates from the Stratego Library are defined using non-linear pattern matching: equal = ?(x, x) equal(|x) = ?x The equal strategy tests whether the current term is a a pair of the same terms. The equal(|x) strategy tests whether the current term is equal to the argument term. stratego> equal = ?(x, x) stratego> !(\"a\", \"a\") (\"a\", \"a\") stratego> equal (\"a\", \"a\") stratego> !(\"a\", \"b\") (\"a\", \"b\") stratego> equal command failed stratego> equal(|x) = ?x stratego> !Foo(Bar()) Foo(Bar) stratego> equal(|Foo(Baz())) command failed stratego> equal(|Foo(Bar())) Foo(Bar) 8.3. Implementing Rewrite Rules Match and build are first-class citizens in Stratego, which means that they can be used and combined just like any other strategy expressions. In particular, we can implement rewrite rules using these operations, since a rewrite rule is basically a match followed by a build. For example, consider the following combination of match and build: Plus(Var(\"a\"),Int(\"3\")) stratego> ?Plus(e1, e2); !Plus(e2, e1) Plus(Int(\"3\"),Var(\"a\")) This combination first recognizes a term, binds variables to the pattern in the match, and then replaces the current term with the instantiation of the build pattern. Note that the variable bindings are propagated from the match to the build. Stratego provides syntactic sugar for various combinations of match and build. We\u2019ll explore these in the rest of this chapter. 8.3.1. Anonymous Rewrite Rule An anonymous rewrite rule (p1 -> p2) transforms a term matching p1 into an instantiation of p2. Such a rule is equivalent to the sequence ?p1; !p2. Plus(Var(\"a\"),Int(\"3\")) stratego> (Plus(e1, e2) -> Plus(e2, e1)) Plus(Int(\"3\"),Var(\"a\")) 8.3.2. Term variable scope Once a variable is bound it cannot be rebound to a different term. Thus, when we have applied an anonymous rule once, its variables are bound and the next time it is applied it only succeeds for the same term. For example, in the next session the second application of the rule fails, because e2 is bound to Int(\"3\") and does not match with Var(\"b\"). stratego> !Plus(Var(\"a\"),Int(\"3\")) Plus(Var(\"a\"),Int(\"3\")) stratego> (Plus(e1,e2) -> Plus(e2,e1)) Plus(Int(\"3\"),Var(\"a\")) stratego> :binding e1 e1 is bound to Var(\"a\") stratego> :binding e2 e2 is bound to Int(\"3\") stratego> !Plus(Var(\"a\"),Var(\"b\")) Plus(Var(\"a\"),Var(\"b\")) stratego> (Plus(e1,e2) -> Plus(e2,e1)) command failed To use a variable name more than once Stratego provides term variable scope. A scope {x1,...,xn : s} locally undefines the variables xi. That is, the binding to a variable xi outside the scope is not visible inside it, nor is the binding to xi inside the scope visible outside it. For example, to continue the session above, if we wrap the anonymous swap rule in a scope for its variables, it can be applied multiple times. stratego> !Plus(Var(\"a\"),Int(\"3\")) Plus(Var(\"a\"),Int(\"3\")) stratego> {e3,e4 : (Plus(e3,e4) -> Plus(e4,e3))} Plus(Var(\"a\"),Int(\"3\")) stratego> :binding e3 e3 is not bound to a term stratego> !Plus(Var(\"a\"),Var(\"b\")) Plus(Var(\"a\"),Var(\"b\")) stratego> {e3,e4 : (Plus(e3,e4) -> Plus(e4,e3))} Plus(Var(\"b\"),Var(\"a\")) Of course we can name such a scoped rule using a strategy definition, and then invoke it by its name: stratego> SwapArgs = {e1,e2 : (Plus(e1,e2) -> Plus(e2,e1))} stratego> !Plus(Var(\"a\"),Int(\"3\")) Plus(Var(\"a\"),Int(\"3\")) stratego> SwapArgs Plus(Int(\"3\"),Var(\"a\")) 8.3.3. Implicit Variable Scope When using match and build directly in a strategy definition, rather than in the form of a rule, the definition contains free variables. Strictly speaking such variables should be declared using a scope, that is one should write SwapArgs = {e1,e2 : (Plus(e1,e2) -> Plus(e2,e1))} However, since declaring all variables at the top of a definition is distracting and does not add much to the definition, such a scope declaration can be left out. Thus, one can write SwapArgs = (Plus(e1,e2) -> Plus(e2,e1)) instead. The scope is automatically inserted by the compiler. This implies that there is no global scope for term variables. Of course, variables in inner scopes should be declared where necessary. In particular, note that variable scope is not inserted for strategy definitions in a let binding, such as let SwapArgs = (Plus(e1,e2) -> Plus(e2,e1)) in ... end While the variables are bound in the enclosing definition, they are not restricted to SwapArgs in this case, since in a let you typically want to use bindings to variables in the enclosing code. 8.3.4. Where Often it is useful to apply a strategy only to test whether some property holds or to compute some auxiliary result. For this purpose, Stratego provides the where(s) combinator, which applies s to the current term, but restores that term afterwards. Any bindings to variables are kept, however. Plus(Int(\"14\"),Int(\"3\")) stratego> where(?Plus(Int(i),Int(j)); (i,j) => k) Plus(Int(\"14\"),Int(\"3\")) stratego> :binding i i is bound to \"14\" stratego> :binding k k is bound to \"17\" With the match and build constructs where(s) is in fact just syntactic sugar for {x: ?x; s; !x} with x a fresh variable not occurring in s. Thus, the current subject term is saved by binding it to a new variable x, then the strategy s is applied, and finally, the original term is restored by building x. We saw the use of where in the definition of if-then-else in Chapter 7. 8.3.5. Conditional rewrite rule A simple rewrite rule succeeds if the match of the left-hand side succeeds. Sometimes it is useful to place additional requirements on the application of a rule, or to compute some value for use in the right-hand side of the rule. This can be achieved with conditional rewrite rules. A conditional rule L: p1 -> p2 where s is a simple rule extended with an additional computation s which should succeed in order for the rule to apply. The condition can be used to test properties of terms in the left-hand side, or to compute terms to be used in the right-hand side. The latter is done by binding such new terms to variables used in the right-hand side. For example, the EvalPlus rule in the following session uses a condition to compute the sum of i and j: stratego> EvalPlus: Plus(Int(i),Int(j)) -> Int(k) where !(i,j); addS; ?k stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> EvalPlus Int(\"17\") A conditional rule can be desugared similarly to an unconditional rule. That is, a conditional rule of the form L : p1 -> p2 where s is syntactic sugar for L = ?p1; where(s); !p2 Thus, after the match with p1 succeeds the strategy s is applied to the subject term. Only if the application of s succeeds, is the right-hand side p2 built. Note that since s is applied within a where, the build !p2 is applied to the original subject term; only variable bindings computed within s can be used in p2. As an example, consider the following constant folding rule, which reduces an addition of two integer constants to the constant obtained by computing the addition. EvalPlus : Add(Int(i),Int(j)) -> Int(k) where !(i,j); addS; ?k The addition is computed by applying the primitive strategy addS to the pair of integers (i,j) and matching the result against the variable k, which is then used in the right-hand side. This rule is desugared to EvalPlus = ?Add(Int(i),Int(j)); where(!(i,j); addS; ?k); !Int(k) 8.3.6. Lambda Rules Sometimes it is useful to define a rule anonymously within a strategy expression. The syntax for anonymous rules with scopes is a bit much since it requires enumerating all variables. A lambda rule of the form p1 -> p2 where s \\ is an anonymous rewrite rule for which the variables in the left-hand side p1 are local to the rule, that is, it is equivalent to an expression of the form {x1,...,xn : (p1 -> p2 where s)} with x1,\u2026,xn the variables of p1. This means that any variables used in s and p2 that do not occur in p1 are bound in the context of the rule. A typical example of the use of an anonymous rule is stratego> ![(1,2),(3,4),(5,6)] [(1,2),(3,4),(5,6)] stratego> map( (x, y) -> x ) [1,3,5] 8.4. Apply and Match One frequently occuring scenario is that of applying a strategy to a term and then matching the result against a pattern. This typically occurs in the condition of a rule. In the constant folding example above we saw this scenario: EvalPlus : Add(Int(i),Int(j)) -> Int(k) where !(i,j); addS; ?k In the condition, first the term (i,j) is built, then the strategy addS is applied to it, and finally the result is matched against the pattern k. To improve the readability of such expressions, the following two constructs are provided. The operation p captures the notion of applying a strategy to a term, i.e., the scenario !p; s. The operation s => p capture the notion of applying a strategy to the current subject term and then matching the result against the pattern p, i.e., s; ?p. The combined operation p1 => p2 thus captures the notion of applying a strategy to a term p1 and matching the result against p2, i.e, !p1; s; ?p2. Using this notation we can improve the constant folding rule above as EvalPlus : Add(Int(i),Int(j)) -> Int(k) where (i,j) => k Applying Strategies in Build. Sometimes it useful to apply a strategy directly to a subterm of a pattern, for example in the right-hand side of a rule, instead of computing a value in a condition, binding the result to a variable, and then using the variable in the build pattern. The constant folding rule above, for example, could be further simplified by directly applying the addition in the right-hand side: EvalPlus : Add(Int(i),Int(j)) -> Int( (i,j)) This abbreviates the conditional rule above. In general, a strategy application in a build pattern can always be expressed by computing the application before the build and binding the result to a new variable, which then replaces the application in the build pattern. Another example is the following definition of the map(s) strategy, which applies a strategy to each term in a list: map(s) : [] -> [] map(s) : [x | xs] -> [ x | xs] 8.5 Auxiliary values and assignment As mentioned above, it can be convenient to apply a strategy only to compute some auxiliary result. Although the where construct created to constrain when a rule or strategy may apply (as covered in Sections 8.3.4 and 8.3.5 above) can be used for this purpose, often it is better to use the with strategy specifically designed with computing auxiliaries in mind. Specifically, if s is any strategy, the strategy with(s) executes s on the current subject term and then restores the current subject term. In other words, s is executed solely for its side effects, such as binding variables. In this respect, with is like where. However, with(s) differs in a key way: if the strategy s fails, Stratego immediately stops with an error, reporting the strategy that failed. Thus, if with(s) is used for auxiliary computations that really should not fail if the transformation is proceeding properly, there is no opportunity for Stratego to backtrack and/or continue applying other strategies, potentially creating an error at a point far removed from the place that things actually went awry. In short, using with(s) instead of where(s) any time the intention is not to constrain the applicability of a rule or strategy generally makes debugging your Stratego program significantly easier. Also as with where, we can add a with clause to a rewrite rule in exactly the same way. In other words, L : p1 -> p2 with s is syntactic sugar for L = ?p1; with(s); !p2 So as an example, the where version of EvalPlus from Section 8.4 would be better cast as EvalPlus : Add(Int(i),Int(j)) -> Int(k) with (i,j) => k because after all, there is no chance that Stratego will be unable to add two integers, and so if the contents of the with clause fails it means something has gone wrong \u2013 perhaps an Int term somehow ended up with a parameter that does not actually represent an integer \u2013 and Stratego should quit now. Furthermore, in setting auxiliary variables often the full power of Stratego strategies is not used, but rather new terms are simply built as needed. Stratego provides an := operator for this purpose; the above rule can be written probably more clearly as EvalPlus : Add(Int(i),Int(j)) -> Int(k) with k := (i,j) Technically, p1 := p2 (which can be used anywhere a strategy is called for, although it is primarily useful in with and where clauses) is just syntactic sugar for !p2; ?p1. In other words, it builds the value p2, and then matches it with p1. In the typical case that p1 is just a variable, this ends up assigning the result of building the expression p2 to that variable. To sum up, we have actually already seen an example of both with and := in the \u201cglue\u201d strategy used to run a Stratego transformation via Editor Services: do-eval: (selected, _, _, path, project-path) -> (filename, result) with filename := path ; result := selected To make the operation of this rule clearer, the two components of the outcome are separated into auxiliary computations in the with clause, and these two auxiliaries are implemented as assignments with the := operator. Moreover, if either the eval strategy fails or if Stratego is unable to compute the proper output filename, there is no point in continuing. So Stratego will simply terminate immediately and report the error. 8.6 Wrap and Project Term wrapping and projection are concise idioms for constructing terms that wrap the current term and for extracting subterms from the current term. 8.6.1. Term Wrap One often write rules of the form x -> Foo(Bar(x)), i.e. wrapping a term pattern around the current term. Using rule syntax this is quite verbose. The syntactic abstraction of term wraps, allows the concise specification of such little transformations as !Foo(Bar( )). In general, a term wrap is a build strategy !p[ ] containing one or more strategy applications that are not applied to a term. When executing the the build operation, each occurrence of such a strategy application is replaced with the term resulting from applying s to the current subject term, i.e., the one that is being replaced by the build. The following sessions illustrate some uses of term wraps: 3 stratego> !( , ) (3,3) stratego> !( , ) (4,3) stratego> !\"foobar\" \"foobar\" stratego> !Call( , []) Call(\"foobar\", []) stratego> mod2 = ( ,2) stratego> !6 6 stratego> mod2 0 As should now be a common pattern, term projects are implemented by translation to a combination of match and build expressions. Thus, a term wrap !p[ ] is translated to a strategy expression {x: where(s => x); !p[x]} where x is a fresh variable not occurring in s. In other words, the strategy s is applied to the current subject term, i.e., the term to which the build is applied. As an example, the term wrap !Foo(Bar( )) is desugared to the strategy {x: where(id => x); !Foo(Bar(x))} which after simplification is equivalent to {x: ?x; !Foo(Bar(x))}, i.e., exactly the original lambda rule x -> Foo(Bar(x)). 8.6.2. Term Project Term projections are the match dual of term wraps. Term projections can be used to project a subterm from a term pattern. For example, the expression ?And( ,x) matches terms of the form And(t1,t2) and reduces them to the first subterm t1. Another example is the strategy map(?FunDec( , , )) which reduces a list of function declarations to a list of the names of the functions, i.e., the first arguments of the FunDec constructor. Here are some more examples: [1,2,3] stratego> ?[_| ] [2,3] stratego> !Call(\"foobar\", []) Call(\"foobar\", []) stratego> ?Call( , []) \"foobar\" Term projections can also be used to apply additional constraints to subterms in a match pattern. For example, ?Call(x, <?args; length => 3>) matches only with function calls with three arguments. A match expression ?p[ ] is desugared as {x: ?p[x]; x} That is, after the pattern p[x] matches, it is reduced to the subterm bound to x to which s is applied. The result is also the result of the projection. When multiple projects are used within a match the outcome is undefined, i.e., the order in which the projects will be performed can not be counted on.","title":"Matching and Building Terms"},{"location":"references/stratego/strategy-combinators-traversal/","text":"Traversal Combinators \u00b6 1 2 Congruence Operators \u00b6 The definition of the traversal rules above frequently occurs in the definition of transformation strategies. Congruence operators provide a convenient abbreviation of precisely this operation. A congruence operator applies a strategy to each direct subterm of a specific constructor. For each n-ary constructor c declared in a signature, there is a corresponding congruence operator c(s1 , ..., sn), which applies to terms of the form c(t1 , ..., tn) by applying the argument strategies to the corresponding argument terms. A congruence fails if the application of one the argument strategies fails or if constructor of the operator and that of the term do not match. Example. For example, consider the following signature of expressions: module expressions signature sorts Exp constructors Int : String -> Exp Var : String -> Exp Plus : Exp * Exp -> Exp Times : Exp * Exp -> Exp The following Stratego Shell session applies the congruence operators Plus and Times to a term: stratego> import expressions stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> Plus(!Var(\"a\"), id) Plus(Var(\"a\"),Int(\"3\")) stratego> Times(id, !Int(\"42\")) command failed The first application shows how a congruence transforms a specific subterm, that is the strategy applied can be different for each subterm. The second application shows that a congruence only succeeds for terms constructed with the same constructor. The import at the start of the session is necessary to declare the constructors used; the definitions of congruences are derived from constructor declarations. Forgetting this import would lead to a complaint about an undeclared operator: stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> Plus(!Var(\"a\"), id) operator Plus/(2,0) not defined command failed Defining Traversals with Congruences. Now we return to our dnf/cnf example, to see how congruence operators can help in their implementation. Since congruence operators basically define a one-step traversal for a specific constructor, they capture the traversal rules defined above. That is, a traversal rule such as proptr(s) : And(x, y) -> And( x, y) can be written by the congruence And(s,s). Applying this to the prop-dnf program we can replace the traversal rules by congruences as follows: module prop-dnf10 imports libstrategolib prop-rules strategies main = io-wrap(dnf) strategies proptr(s) = Not(s) <+ And(s, s) <+ Or(s, s) <+ Impl(s, s) <+ Eq(s, s) propbu(s) = try(proptr(propbu(s))); s strategies dnf = propbu(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DAOL <+ DAOR); dnf)) cnf = propbu(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DOAL <+ DOAR); cnf)) Observe how the five traversal rules have been reduced to five congruences which fit on a single line. Traversing Tuples and Lists. Congruences can also be applied to tuples, (s1,s2,...,sn), and lists, [s1,s2,...,sn]. A special list congruence is [] which \u2018visits\u2019 the empty list. As an example, consider again the definition of map(s) using recursive traversal rules: map(s) : [] -> [] map(s) : [x | xs] -> [ x | xs] Using list congruences we can define this strategy as: map(s) = [] <+ [s | map(s)] The [] congruence matches an empty list. The [s | map(s)] congruence matches a non-empty list, and applies s to the head of the list and map(s) to the tail. Thus, map(s) applies s to each element of a list: stratego> import libstratego-lib stratego> ![1,2,3] [1,2,3] stratego> map(inc) [2,3,4] Note that map(s) only succeeds if s succeeds for each element of the list. The fetch and filter strategies are variations on map that use the failure of s to list elements. fetch(s) = [s | id] <+ [id | fetch(s)] The fetch strategy traverses a list until it finds a element for which s succeeds and then stops. That element is the only one that is transformed. filter(s) = [] + ([s | filter(s)] <+ ?[ | ]; filter(s)) The filter strategy applies s to each element of a list, but only keeps the elements for which it succeeds. stratego> import libstratego-lib stratego> even = where( ( ( ,2),0)) stratego> ![1,2,3,4,5,6,7,8] [1,2,3,4,5,6,7,8] stratego> filter(even) [2,4,6,8] Format Checking. Another application of congruences is in the definition of format checkers. A format checker describes a subset of a term language using a recursive pattern. This can be used to verify input or output of a transformation, and for documentation purposes. Format checkers defined with congruences can check subsets of signatures or regular tree grammars. For example, the subset of terms of a signature in a some normal form. As an example, consider checking the output of the dnf and cnf transformations. conj(s) = And(conj(s), conj(s)) <+ s disj(s) = Or (disj(s), disj(s)) <+ s // Conjunctive normal form conj-nf = conj(disj(Not(Atom(id)) <+ Atom(id))) // Disjunctive normal form disj-nf = disj(conj(Not(Atom(id)) <+ Atom(id))) The strategies conj(s) and disj(s) check that the subject term is a conjunct or a disjunct, respectively, with terms satisfying s at the leaves. The strategies conj-nf and disj-nf check that the subject term is in conjunctive or disjunctive normal form, respectively. Using congruence operators we constructed a generic, i.e. transformation independent, bottom-up traversal for proposition terms. The same can be done for other data types. However, since the sets of constructors of abstract syntax trees of typical programming languages can be quite large, this may still amount to quite a bit of work that is not reusable across data types; even though a strategy such as bottom-up traversal, is basically data-type independent. Thus, Stratego provides generic traversal by means of several generic one-step descent operators. The operator all, applies a strategy to all direct subterms. The operator one, applies a strategy to one direct subterm, and the operator some, applies a strategy to as many direct subterms as possible, and at least one. Visiting All Subterms \u00b6 The all(s) strategy transforms a constructor application by applying the parameter strategy s to each direct subterm. An application of all(s) fails if the application to one of the subterms fails. The following example shows how all (1) applies to any term, and (2) applies its argument strategy uniformly to all direct subterms. That is, it is not possible to do something special for a particular subterm (that\u2019s what congruences are for). stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> all(!Var(\"a\")) Plus(Var(\"a\"),Var(\"a\")) stratego> !Times(Var(\"b\"),Int(\"3\")) Times(Var(\"b\"),Int(\"3\")) stratego> all(!Var(\"z\")) Times(Var(\"z\"),Var(\"z\")) The all(s) operator is really the ultimate replacement for the traversal rules that we saw above. Instead of specifying a rule or congruence for each constructor, the single application of the all operator takes care of traversing all constructors. Thus, we can replace the propbu strategy by a completely generic definition of bottom-up traversal. Consider again the last definition of propbu: proptr(s) = Not(s) <+ And(s, s) <+ Or(s, s) <+ Impl(s, s) <+ Eq(s, s) propbu(s) = try(proptr(propbu(s))); s The role of proptr(s) in this definition can be replaced by all(s), since that achieves exactly the same, namely applying s to the direct subterms of constructors: propbu(s) = all(propbu(s)); s Moreover, all succeeds on any constructor in any signature, so we can also as you see above drop the try as well, which was there only because proptr fails on the Atom(...), True(), and False() nodes at the leaves. However, the strategy now is completely generic, i.e. independent of the particular structure it is applied to. In the Stratego Library this strategy is called bottomup(s), and defined as follows: bottomup(s) = all(bottomup(s)); s It first recursively transforms the subterms of the subject term and then applies s to the result. Using this definition, the normalization of propositions now reduces to the following module, which is only concerned with the selection and composition of rewrite rules: module prop-dnf11 imports libstrategolib prop-rules strategies main = io-wrap(dnf) strategies dnf = bottomup(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DAOL <+ DAOR); dnf)) cnf = bottomup(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DOAL <+ DOAR); cnf)) In fact, these definitions still contain a reusable pattern. With a little squinting we see that the definitions match the following pattern: dnf = bottomup(try(dnf-rules; dnf)) cnf = bottomup(try(cnf-rules; cnf)) In which we can recognize the definition of innermost reduction, which the Stratego Library defines as: innermost(s) = bottomup(try(s; innermost(s))) The innermost strategy performs a bottom-up traversal of a term. After transforming the subterms of a term it tries to apply the transformation s. If successful the result is recursively transformed with an application of innermost. This brings us to the final form for the proposition normalizations: module prop-dnf12 imports libstrategolib prop-rules strategies main = io-wrap(dnf) strategies dnf = innermost(DN <+ DefI <+ DefE <+ DMA <+ DMO <+ DAOL <+ DAOR) cnf = innermost(DN <+ DefI <+ DefE <+ DMA <+ DMO <+ DOAL <+ DOAR) Different transformations can be achieved by using a selection of rules and a strategy, which is generic, yet defined in Stratego itself using strategy combinators. Visiting One Subterm \u00b6 The one(s) strategy transforms a constructor application by applying the parameter strategy s to exactly one direct subterm. An application of one(s) fails if the application to all of the subterms fails. The following Stratego Shell session illustrates the behavior of the combinator: stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> one(!Var(\"a\")) Plus(Var(\"a\"),Int(\"3\")) stratego> one( Int(x) -> Int( (x,\"1\")) ) Plus(Var(\"a\"),Int(\"4\")) stratego> one(?Plus( , )) command failed A frequently used application of one is the oncetd(s) traversal, which performs a left to right depth first search/transformation that stops as soon as s has been successfully applied. oncetd(s) = s <+ one(oncetd(s)) Thus, s is first applied to the root of the subject term. If that fails, its direct subterms are searched one by one (from left to right), with a recursive call to oncetd(s). An application of oncetd is the contains(|t) strategy, which checks whether the subject term contains a subterm that is equal to t. contains(|t) = oncetd(?t) Through the depth first search of oncetd, either an occurrence of t is found, or all subterms are verified to be unequal to t. Here are some other one-pass traversals using the one combinator: oncebu(s) = one(oncebu(s)) <+ s spinetd(s) = s; try(one(spinetd(s))) spinebu(s) = try(one(spinebu(s))); s Exercise: figure out what these strategies do. Here are some fixe-point traversals, i.e., traversals that apply their argument transformation exhaustively to the subject term. reduce(s) = repeat(rec x(one(x) + s)) outermost(s) = repeat(oncetd(s)) innermostI(s) = repeat(oncebu(s)) The difference is the subterm selection strategy. Exercise: create rewrite rules and terms that demonstrate the differences between these strategies. Visiting Some Subterms \u00b6 The some(s) strategy transforms a constructor application by applying the parameter strategy s to as many direct subterms as possible and at least one. An application of some(s) fails if the application to all of the subterms fails. Some one-pass traversals based on some: sometd(s) = s <+ some(sometd(s)) somebu(s) = some(somebu(s)) <+ s A fixed-point traversal with some: reduce-par(s) = repeat(rec x(some(x) + s)) References \u00b6 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9","title":"Traversal Combinators"},{"location":"references/stratego/strategy-combinators-traversal/#traversal-combinators","text":"1 2","title":"Traversal Combinators"},{"location":"references/stratego/strategy-combinators-traversal/#congruence-operators","text":"The definition of the traversal rules above frequently occurs in the definition of transformation strategies. Congruence operators provide a convenient abbreviation of precisely this operation. A congruence operator applies a strategy to each direct subterm of a specific constructor. For each n-ary constructor c declared in a signature, there is a corresponding congruence operator c(s1 , ..., sn), which applies to terms of the form c(t1 , ..., tn) by applying the argument strategies to the corresponding argument terms. A congruence fails if the application of one the argument strategies fails or if constructor of the operator and that of the term do not match. Example. For example, consider the following signature of expressions: module expressions signature sorts Exp constructors Int : String -> Exp Var : String -> Exp Plus : Exp * Exp -> Exp Times : Exp * Exp -> Exp The following Stratego Shell session applies the congruence operators Plus and Times to a term: stratego> import expressions stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> Plus(!Var(\"a\"), id) Plus(Var(\"a\"),Int(\"3\")) stratego> Times(id, !Int(\"42\")) command failed The first application shows how a congruence transforms a specific subterm, that is the strategy applied can be different for each subterm. The second application shows that a congruence only succeeds for terms constructed with the same constructor. The import at the start of the session is necessary to declare the constructors used; the definitions of congruences are derived from constructor declarations. Forgetting this import would lead to a complaint about an undeclared operator: stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> Plus(!Var(\"a\"), id) operator Plus/(2,0) not defined command failed Defining Traversals with Congruences. Now we return to our dnf/cnf example, to see how congruence operators can help in their implementation. Since congruence operators basically define a one-step traversal for a specific constructor, they capture the traversal rules defined above. That is, a traversal rule such as proptr(s) : And(x, y) -> And( x, y) can be written by the congruence And(s,s). Applying this to the prop-dnf program we can replace the traversal rules by congruences as follows: module prop-dnf10 imports libstrategolib prop-rules strategies main = io-wrap(dnf) strategies proptr(s) = Not(s) <+ And(s, s) <+ Or(s, s) <+ Impl(s, s) <+ Eq(s, s) propbu(s) = try(proptr(propbu(s))); s strategies dnf = propbu(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DAOL <+ DAOR); dnf)) cnf = propbu(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DOAL <+ DOAR); cnf)) Observe how the five traversal rules have been reduced to five congruences which fit on a single line. Traversing Tuples and Lists. Congruences can also be applied to tuples, (s1,s2,...,sn), and lists, [s1,s2,...,sn]. A special list congruence is [] which \u2018visits\u2019 the empty list. As an example, consider again the definition of map(s) using recursive traversal rules: map(s) : [] -> [] map(s) : [x | xs] -> [ x | xs] Using list congruences we can define this strategy as: map(s) = [] <+ [s | map(s)] The [] congruence matches an empty list. The [s | map(s)] congruence matches a non-empty list, and applies s to the head of the list and map(s) to the tail. Thus, map(s) applies s to each element of a list: stratego> import libstratego-lib stratego> ![1,2,3] [1,2,3] stratego> map(inc) [2,3,4] Note that map(s) only succeeds if s succeeds for each element of the list. The fetch and filter strategies are variations on map that use the failure of s to list elements. fetch(s) = [s | id] <+ [id | fetch(s)] The fetch strategy traverses a list until it finds a element for which s succeeds and then stops. That element is the only one that is transformed. filter(s) = [] + ([s | filter(s)] <+ ?[ | ]; filter(s)) The filter strategy applies s to each element of a list, but only keeps the elements for which it succeeds. stratego> import libstratego-lib stratego> even = where( ( ( ,2),0)) stratego> ![1,2,3,4,5,6,7,8] [1,2,3,4,5,6,7,8] stratego> filter(even) [2,4,6,8] Format Checking. Another application of congruences is in the definition of format checkers. A format checker describes a subset of a term language using a recursive pattern. This can be used to verify input or output of a transformation, and for documentation purposes. Format checkers defined with congruences can check subsets of signatures or regular tree grammars. For example, the subset of terms of a signature in a some normal form. As an example, consider checking the output of the dnf and cnf transformations. conj(s) = And(conj(s), conj(s)) <+ s disj(s) = Or (disj(s), disj(s)) <+ s // Conjunctive normal form conj-nf = conj(disj(Not(Atom(id)) <+ Atom(id))) // Disjunctive normal form disj-nf = disj(conj(Not(Atom(id)) <+ Atom(id))) The strategies conj(s) and disj(s) check that the subject term is a conjunct or a disjunct, respectively, with terms satisfying s at the leaves. The strategies conj-nf and disj-nf check that the subject term is in conjunctive or disjunctive normal form, respectively. Using congruence operators we constructed a generic, i.e. transformation independent, bottom-up traversal for proposition terms. The same can be done for other data types. However, since the sets of constructors of abstract syntax trees of typical programming languages can be quite large, this may still amount to quite a bit of work that is not reusable across data types; even though a strategy such as bottom-up traversal, is basically data-type independent. Thus, Stratego provides generic traversal by means of several generic one-step descent operators. The operator all, applies a strategy to all direct subterms. The operator one, applies a strategy to one direct subterm, and the operator some, applies a strategy to as many direct subterms as possible, and at least one.","title":"Congruence Operators"},{"location":"references/stratego/strategy-combinators-traversal/#visiting-all-subterms","text":"The all(s) strategy transforms a constructor application by applying the parameter strategy s to each direct subterm. An application of all(s) fails if the application to one of the subterms fails. The following example shows how all (1) applies to any term, and (2) applies its argument strategy uniformly to all direct subterms. That is, it is not possible to do something special for a particular subterm (that\u2019s what congruences are for). stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> all(!Var(\"a\")) Plus(Var(\"a\"),Var(\"a\")) stratego> !Times(Var(\"b\"),Int(\"3\")) Times(Var(\"b\"),Int(\"3\")) stratego> all(!Var(\"z\")) Times(Var(\"z\"),Var(\"z\")) The all(s) operator is really the ultimate replacement for the traversal rules that we saw above. Instead of specifying a rule or congruence for each constructor, the single application of the all operator takes care of traversing all constructors. Thus, we can replace the propbu strategy by a completely generic definition of bottom-up traversal. Consider again the last definition of propbu: proptr(s) = Not(s) <+ And(s, s) <+ Or(s, s) <+ Impl(s, s) <+ Eq(s, s) propbu(s) = try(proptr(propbu(s))); s The role of proptr(s) in this definition can be replaced by all(s), since that achieves exactly the same, namely applying s to the direct subterms of constructors: propbu(s) = all(propbu(s)); s Moreover, all succeeds on any constructor in any signature, so we can also as you see above drop the try as well, which was there only because proptr fails on the Atom(...), True(), and False() nodes at the leaves. However, the strategy now is completely generic, i.e. independent of the particular structure it is applied to. In the Stratego Library this strategy is called bottomup(s), and defined as follows: bottomup(s) = all(bottomup(s)); s It first recursively transforms the subterms of the subject term and then applies s to the result. Using this definition, the normalization of propositions now reduces to the following module, which is only concerned with the selection and composition of rewrite rules: module prop-dnf11 imports libstrategolib prop-rules strategies main = io-wrap(dnf) strategies dnf = bottomup(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DAOL <+ DAOR); dnf)) cnf = bottomup(try(DN <+ (DefI <+ DefE <+ DMA <+ DMO <+ DOAL <+ DOAR); cnf)) In fact, these definitions still contain a reusable pattern. With a little squinting we see that the definitions match the following pattern: dnf = bottomup(try(dnf-rules; dnf)) cnf = bottomup(try(cnf-rules; cnf)) In which we can recognize the definition of innermost reduction, which the Stratego Library defines as: innermost(s) = bottomup(try(s; innermost(s))) The innermost strategy performs a bottom-up traversal of a term. After transforming the subterms of a term it tries to apply the transformation s. If successful the result is recursively transformed with an application of innermost. This brings us to the final form for the proposition normalizations: module prop-dnf12 imports libstrategolib prop-rules strategies main = io-wrap(dnf) strategies dnf = innermost(DN <+ DefI <+ DefE <+ DMA <+ DMO <+ DAOL <+ DAOR) cnf = innermost(DN <+ DefI <+ DefE <+ DMA <+ DMO <+ DOAL <+ DOAR) Different transformations can be achieved by using a selection of rules and a strategy, which is generic, yet defined in Stratego itself using strategy combinators.","title":"Visiting All Subterms"},{"location":"references/stratego/strategy-combinators-traversal/#visiting-one-subterm","text":"The one(s) strategy transforms a constructor application by applying the parameter strategy s to exactly one direct subterm. An application of one(s) fails if the application to all of the subterms fails. The following Stratego Shell session illustrates the behavior of the combinator: stratego> !Plus(Int(\"14\"),Int(\"3\")) Plus(Int(\"14\"),Int(\"3\")) stratego> one(!Var(\"a\")) Plus(Var(\"a\"),Int(\"3\")) stratego> one( Int(x) -> Int( (x,\"1\")) ) Plus(Var(\"a\"),Int(\"4\")) stratego> one(?Plus( , )) command failed A frequently used application of one is the oncetd(s) traversal, which performs a left to right depth first search/transformation that stops as soon as s has been successfully applied. oncetd(s) = s <+ one(oncetd(s)) Thus, s is first applied to the root of the subject term. If that fails, its direct subterms are searched one by one (from left to right), with a recursive call to oncetd(s). An application of oncetd is the contains(|t) strategy, which checks whether the subject term contains a subterm that is equal to t. contains(|t) = oncetd(?t) Through the depth first search of oncetd, either an occurrence of t is found, or all subterms are verified to be unequal to t. Here are some other one-pass traversals using the one combinator: oncebu(s) = one(oncebu(s)) <+ s spinetd(s) = s; try(one(spinetd(s))) spinebu(s) = try(one(spinebu(s))); s Exercise: figure out what these strategies do. Here are some fixe-point traversals, i.e., traversals that apply their argument transformation exhaustively to the subject term. reduce(s) = repeat(rec x(one(x) + s)) outermost(s) = repeat(oncetd(s)) innermostI(s) = repeat(oncebu(s)) The difference is the subterm selection strategy. Exercise: create rewrite rules and terms that demonstrate the differences between these strategies.","title":"Visiting One Subterm"},{"location":"references/stratego/strategy-combinators-traversal/#visiting-some-subterms","text":"The some(s) strategy transforms a constructor application by applying the parameter strategy s to as many direct subterms as possible and at least one. An application of some(s) fails if the application to all of the subterms fails. Some one-pass traversals based on some: sometd(s) = s <+ some(sometd(s)) somebu(s) = some(somebu(s)) <+ s A fixed-point traversal with some: reduce-par(s) = repeat(rec x(some(x) + s))","title":"Visiting Some Subterms"},{"location":"references/stratego/strategy-combinators-traversal/#references","text":"Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9","title":"References"},{"location":"references/stratego/strategy-combinators-type-unifying/","text":"Type Unifying Transformations \u00b6 1 2 In Chapter 5 we have seen combinators for composing type preserving strategies. That is, structural transformations in which basic transformation rules don\u2019t change the type of a term. Such strategies are typically applied in transformations, which change the structure of a term, but not its type. Examples are simplification and optimization. In this chapter we consider the class of type unifying strategies, in which terms of different types are mapped onto one type. The application area for this type of strategy is analysis of expressions with examples such as free variables collection and call-graph extraction. We consider the following example problems: term-size: Count the number of nodes in a term occurrences: Count number of occurrences of a subterm in a term collect-vars: Collect all variables in expression free-vars: Collect all free variables in expression These problems have in common that they reduce a structure to a single value or to a collection of derived values. The structure of the original term is usually lost. We start with examining these problems in the context of lists, and then generalize the solutions we find there to arbitrary terms using generic term deconstruction, which allows concise implementation of generic type unifying strategies, similarly to the generic traversal strategies of Chapter 5. 10.1. Type Unifying List Transformations We start with considering type-unifying operations on lists. Sum. Reducing a list to a value can be conveniently expressed by means of a fold, which has as parameters operations for reducing the list constructors. The foldr/2 strategy reduces a list by replacing each Cons by an application of s2, and the empty list by s1. foldr(s1, s2) = []; s1 <+ [y|ys] -> (y, ys) \\ Thus, when applied to a list with three terms the result is [t1,t2,t3] => (t1, (t2, (t3, []))) A typical application of foldr/2 is sum, which reduces a list to the sum of its elements. It sums the elements of a list of integers, using 0 for the empty list and add to combine the head of a list and the result of folding the tail. sum = foldr(!0, add) The effect of sum is illustrated by the following application: [1,2,3] => (1, (2, (3, <!0> []))) => 6 Note the build operator for replacing the empty list with 0; writing foldr(0, add) would be wrong, since 0 by itself is a congruence operator, which basically matches the subject term with the term 0 (rather than replacing it). Size. The foldr/2 strategy does not touch the elements of a list. The foldr/3 strategy is a combination of fold and map that extends foldr/2 with a parameter that is applied to the elements of the list. foldr(s1, s2, f) = []; s1 <+ [y|ys] -> ( y, ys) \\ Thus, when applying it to a list with three elements, we get: [t1,t2,t3] => ( t1, ( t2, ( t3, []))) Now we can solve our first example problem term-size. The size of a list is its length, which corresponds to the sum of the list with the elements replaced by 1. length = foldr(!0, add, !1) Number of occurrences. The number of occurrences in a list of terms that satisfy some predicate, entails only counting those elements in the list for which the predicate succeeds. (Where a predicate is implemented with a strategy that succeeds only for the elements in the domain of the predicate.) This follows the same pattern as counting the length of a list, but now only counting the elements for which s succeeds. list-occurrences(s) = foldr(!0, add, s < !1 + !0) Using list-occurrences and a match strategy we can count the number of variables in a list: list-occurrences(?Var(_)) Collect. The next problem is to collect all terms for which a strategy succeeds. We have already seen how to do this for lists. The filter strategy reduces a list to the elements for which its argument strategy succeeds. filter(s) = [] <+ [s | filter(s)] <+ ?[ | ] Collecting the variables in a list is a matter of filtering with the ?Var(_) match. filter(?Var(_)) The final problem, collecting the free variables in a term, does not really have a counter part in lists, but we can mimick this if we consider having two lists; where the second list is the one with the bound variables that should be excluded. (filter(?Var(_)),id); diff This collects the variables in the first list and subtracts the variables in the second list. 10.2. Extending Fold to Expressions We have seen how to do typical analysis transformations on lists. How can we generalize this to arbitrary terms? The general idea of a folding operator is that it replaces the constructors of a data-type by applying a function to combine the reduced arguments of constructor applications. For example, the following definition is a sketch for a fold over abstract syntax trees: fold-exp(binop, assign, if, ...) = rec f( fold-binop(f, binop) <+ fold-assign(f, assign) <+ fold-if(f, if) <+ ... ) fold-binop(f, s) : BinOp(op, e1, e2) -> (op, e1, e2) fold-assign(f, s) : Assign(e1, e2) -> ( e1, e2) fold-if(f, s) : If(e1, e2, e3) -> ( e1, e2, e3) For each constructor of the data-type the fold has an argument strategy and a rule that matches applications of the constructor, which it replaces with an application of the strategy to the tuple of subterms reduced by a recursive invocation of the fold. Instantiation of this strategy requires a rule for each constructor of the data-type. For instance, the following instantiation defines term-size using fold-exp by providing rules that sum up the sizes of the subterms and add one (inc) to account for the node itself. term-size = fold-exp(BinOpSize, AssignSize, IfSize, ...) BinOpSize : (Plus(), e1, e2) -> (e1, e2) AssignSize : (e1, e2) -> (e1, e2) IfSize : (e1, e2, e3) -> (e1, (e2, e3)) This looks suspiciously like the traversal rules in Chapter 5. Defining folds in this manner has several limitations. In the definition of fold, one parameter for each constructor is provided and traversal is defined explicitly for each constructor. Furthermore, in the instantiation of fold, one rule for each constructor is needed, and the default behaviour is not generically specified. One solution would be to use the generic traversal strategy bottomup to deal with fold: fold-exp(s) = bottomup(s) term-size = fold-exp(BinOpSize <+ AssignSize <+ IfSize <+ ...) BinOpSize : BinOp(Plus(), e1, e2) -> (1, (e1, e2)) AssignSize : Assign(e1, e2) -> (e1, e2) IfSize : If(e1, e2, e3) -> (e1, (e2, e3)) Although the recursive application to subterms is now defined generically , one still has to specify rules for the default behavior. 10.3. Generic Term Deconstruction Instead of having folding rules that are specific to a data type, such as BinOpSize : BinOp(op, e1, e2) -> (1, (e1, e2)) AssignSize : Assign(e1, e2) -> (1, (e1, e2)) we would like to have a generic definition of the form CSize : c(e1, e2, ...) -> (e1, (e2, ...)) This requires generic decomposition of a constructor application into its constructor and the list with children. This can be done using the # operator. The match strategy ?p1#(p2) decomposes a constructor application into its constructor name and the list of direct subterms. Matching such a pattern against a term of the form C(t1,...,tn) results in a match of \"C\" against p1 and a match of [t1,...,tn] against p2. Plus(Int(\"1\"), Var(\"2\")) stratego> ?c#(xs) stratego> :binding c variable c bound to \"Plus\" stratego> :binding xs variable xs bound to [Int(\"1\"), Var(\"2\")] Crush. Using generic term deconstruction we can now generalize the type unifying operations on lists to arbitrary terms. In analogy with the generic traversal operators we need a generic one-level reduction operator. The crush/3 strategy reduces a constructor application by folding the list of its subterms using foldr/3. crush(nul, sum, s) : c#(xs) -> xs Thus, crush performs a fold-map over the direct subterms of a term. The following application illustrates what C(t1, t2) => ( t1, ( t2, [])) The following Shell session instantiates this application in two ways: stratego> import libstrategolib stratego> !Plus(Int(\"1\"), Var(\"2\")) Plus(Int(\"1\"),Var(\"2\")) stratego> crush(id, id, id) (Int(\"1\"),(Var(\"2\"),[])) stratego> !Plus(Int(\"1\"), Var(\"2\")) Plus(Int(\"1\"),Var(\"2\")) stratego> crush(!Tail( ), !Sum( , ), !Arg( )) Sum(Arg(Int(\"1\")),Sum(Arg(Var(\"2\")),Tail([]))) The crush strategy is the tool we need to implement solutions for the example problems above. Size. Counting the number of direct subterms of a term is similar to counting the number of elements of a list. The definition of node-size is the same as the definition of length, except that it uses crush instead of foldr: node-size = crush(!0, add, !1) Counting the number of subterms (nodes) in a term is a similar problem. But, instead of counting each direct subterm as 1, we need to count its subterms. term-size = crush(!1, add, term-size) The term-size strategy achieves this simply with a recursive call to itself. stratego> Plus(Int(\"1\"), Var(\"2\")) 2 stratego> Plus(Int(\"1\"), Var(\"2\")) 5 Occurrences. Counting the number of occurrences of a certain term in another term, or more generally, counting the number of subterms that satisfy some predicate is similar to counting the term size. However, only those terms satisfying the predicate should be counted. The solution is again similar to the solution for lists, but now using crush. om-occurrences(s) = s < !1 + crush(!0, add, om-occurrences(s)) The om-occurrences strategy counts the outermost subterms satisfying s. That is, the strategy stops counting as soon as it finds a subterm for which s succeeds. The following strategy counts all occurrences: occurrences(s) = (<s < !1 + !0>, ) It counts the current term if it satisfies s and adds that to the occurrences in the subterms. stratego> Plus(Int(\"1\"), Plus(Int(\"34\"), Var(\"2\"))) 2 stratego> Plus(Int(\"1\"), Plus(Int(\"34\"), Var(\"2\"))) 1 stratego> Plus(Int(\"1\"), Plus(Int(\"34\"), Var(\"2\"))) 2 Collect. Collecting the subterms that satisfy a predicate is similar to counting, but now a list of subterms is produced. The collect(s) strategy collects all outermost occurrences satisfying s. collect(s) = ![ ] <+ crush(![], union, collect(s)) When encountering a subterm for which s succeeds, a singleton list is produced. For other terms, the matching subterms are collected for each direct subterm, and the resulting lists are combined with union to remove duplicates. A typical application of collect is the collection of all variables in an expression, which can be defined as follows: get-vars = collect(?Var( )) Applying get-vars to an expression AST produces the list of all subterms matching Var( ). The collect-all(s) strategy collects all occurrences satisfying s. collect-all(s) = ![ | ] <+ crush(![], union, collect(s)) If s succeeds for the subject term combines the subject term with the collected terms from the subterms. Free Variables. Collecting the variables in an expression is easy, as we saw above. However, when dealing with languages with variable bindings, a common operation is to extract only the free variables in an expression or block of statements. That is, the occurrences of variables that are not bound by a variable declaration. For example, in the expression x + let var y := x + 1 in f(y, a + x + b) end the free variables are {x, a, b}, but not y, since it is bound by the declaration in the let. Similarly, in the function definition function f(x : int) = let var y := h(x) in x + g(z) * y end the only free variable is z since x and y are declared. Here is a free variable extraction strategy for Tiger expressions. It follows a similar pattern of mixing generic and data-type specific operations as we saw in Chapter 5. The crush alternative takes care of the non-special constructors, while ExpVars and FreeVars deal with the special cases, i.e. variables and variable binding constructs: free-vars = ExpVars <+ FreeVars(free-vars) <+ crush(![], union, free-vars) ExpVars : Var(x) -> [x] FreeVars(fv) : Let([VarDec(x, t, e1)], e2) -> ( e1, ( e2, [x])) FreeVars(fv) : Let([FunctionDec(fdecs)], e2) -> ( ( fdecs, e2), fs) where ,_,_,_))> fdecs => fs FreeVars(fv) : FunDec(f, xs, t, e) -> ( e, xs) where xs => xs The FreeVars rules for binding constructs use their fv parameter to recursively get the free variables from subterms, and they subtract the bound variables from any free variables found using diff. We can even capture the pattern exhibited here in a generic collection algorithm with support for special cases: collect-exc(base, special : (a -> b) * a -> b) = base <+ special(collect-exc(base, special)) <+ crush(![], union, collect-exc(base, special)) The special parameter is a strategy parameterized with a recursive call to the collection strategy. The original definition of free-vars above, can now be replaced with free-vars = collect-exc(ExpVars, FreeVars) 10.4. Generic Term Construction It can also be useful to construct terms generically. For example, in parse tree implosion, application nodes should be reduced to constructor applications. Hence build operators can also use the # operator. In a strategy !p1#(p2), the current subject term is replaced by a constructor application, where the constructor name is provided by p1 and the list of subterms by p2. So, if p1 evaluates to \"C\" and p2 evaluates to [t1,...,tn], the expression !p1#(p2) build the term C(t1,...,tn). Imploding Parse Trees. A typical application of generic term construction is the implosion of parse trees to abstract syntax trees performed by implode-asfix. Parse trees produced by sglr have the form: appl(prod(sorts, sort, attrs([cons(\"C\")])),[t1,...,tn]) That is, a node in a parse tree consists of an encoding of the original production from the syntax definition, and a list with subtrees. The production includes a constructor annotation cons(\"C\") with the name of the abstract syntax tree constructor. Such a tree node should be imploded to an abstract syntax tree node of the form C(t1,...,tn). Thus, this requires the construction of a term with constructor C given the string with its name. The following implosion strategy achieves this using generic term construction: implode = appl(id, map(implode)); Implode Implode : appl(prod(sorts, sort, attrs([cons\u00a9])), ts) -> c#(ts) The Implode rule rewrites an appl term to a constructor application, by extracting the constructor name from the production and then using generic term construction to apply the constructor. Note that this is a gross over simplification of the actual implementation of implode-asfix. See the source code for the full strategy. Generic term construction and deconstruction support the definition of generic analysis and generic translation problems. The generic solutions for the example problems term size, number of occurrences, and subterm collection demonstrate the general approach to solving these types of problems. References \u00b6 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9","title":"Type Unifying Transformations"},{"location":"references/stratego/strategy-combinators-type-unifying/#type-unifying-transformations","text":"1 2 In Chapter 5 we have seen combinators for composing type preserving strategies. That is, structural transformations in which basic transformation rules don\u2019t change the type of a term. Such strategies are typically applied in transformations, which change the structure of a term, but not its type. Examples are simplification and optimization. In this chapter we consider the class of type unifying strategies, in which terms of different types are mapped onto one type. The application area for this type of strategy is analysis of expressions with examples such as free variables collection and call-graph extraction. We consider the following example problems: term-size: Count the number of nodes in a term occurrences: Count number of occurrences of a subterm in a term collect-vars: Collect all variables in expression free-vars: Collect all free variables in expression These problems have in common that they reduce a structure to a single value or to a collection of derived values. The structure of the original term is usually lost. We start with examining these problems in the context of lists, and then generalize the solutions we find there to arbitrary terms using generic term deconstruction, which allows concise implementation of generic type unifying strategies, similarly to the generic traversal strategies of Chapter 5. 10.1. Type Unifying List Transformations We start with considering type-unifying operations on lists. Sum. Reducing a list to a value can be conveniently expressed by means of a fold, which has as parameters operations for reducing the list constructors. The foldr/2 strategy reduces a list by replacing each Cons by an application of s2, and the empty list by s1. foldr(s1, s2) = []; s1 <+ [y|ys] -> (y, ys) \\ Thus, when applied to a list with three terms the result is [t1,t2,t3] => (t1, (t2, (t3, []))) A typical application of foldr/2 is sum, which reduces a list to the sum of its elements. It sums the elements of a list of integers, using 0 for the empty list and add to combine the head of a list and the result of folding the tail. sum = foldr(!0, add) The effect of sum is illustrated by the following application: [1,2,3] => (1, (2, (3, <!0> []))) => 6 Note the build operator for replacing the empty list with 0; writing foldr(0, add) would be wrong, since 0 by itself is a congruence operator, which basically matches the subject term with the term 0 (rather than replacing it). Size. The foldr/2 strategy does not touch the elements of a list. The foldr/3 strategy is a combination of fold and map that extends foldr/2 with a parameter that is applied to the elements of the list. foldr(s1, s2, f) = []; s1 <+ [y|ys] -> ( y, ys) \\ Thus, when applying it to a list with three elements, we get: [t1,t2,t3] => ( t1, ( t2, ( t3, []))) Now we can solve our first example problem term-size. The size of a list is its length, which corresponds to the sum of the list with the elements replaced by 1. length = foldr(!0, add, !1) Number of occurrences. The number of occurrences in a list of terms that satisfy some predicate, entails only counting those elements in the list for which the predicate succeeds. (Where a predicate is implemented with a strategy that succeeds only for the elements in the domain of the predicate.) This follows the same pattern as counting the length of a list, but now only counting the elements for which s succeeds. list-occurrences(s) = foldr(!0, add, s < !1 + !0) Using list-occurrences and a match strategy we can count the number of variables in a list: list-occurrences(?Var(_)) Collect. The next problem is to collect all terms for which a strategy succeeds. We have already seen how to do this for lists. The filter strategy reduces a list to the elements for which its argument strategy succeeds. filter(s) = [] <+ [s | filter(s)] <+ ?[ | ] Collecting the variables in a list is a matter of filtering with the ?Var(_) match. filter(?Var(_)) The final problem, collecting the free variables in a term, does not really have a counter part in lists, but we can mimick this if we consider having two lists; where the second list is the one with the bound variables that should be excluded. (filter(?Var(_)),id); diff This collects the variables in the first list and subtracts the variables in the second list. 10.2. Extending Fold to Expressions We have seen how to do typical analysis transformations on lists. How can we generalize this to arbitrary terms? The general idea of a folding operator is that it replaces the constructors of a data-type by applying a function to combine the reduced arguments of constructor applications. For example, the following definition is a sketch for a fold over abstract syntax trees: fold-exp(binop, assign, if, ...) = rec f( fold-binop(f, binop) <+ fold-assign(f, assign) <+ fold-if(f, if) <+ ... ) fold-binop(f, s) : BinOp(op, e1, e2) -> (op, e1, e2) fold-assign(f, s) : Assign(e1, e2) -> ( e1, e2) fold-if(f, s) : If(e1, e2, e3) -> ( e1, e2, e3) For each constructor of the data-type the fold has an argument strategy and a rule that matches applications of the constructor, which it replaces with an application of the strategy to the tuple of subterms reduced by a recursive invocation of the fold. Instantiation of this strategy requires a rule for each constructor of the data-type. For instance, the following instantiation defines term-size using fold-exp by providing rules that sum up the sizes of the subterms and add one (inc) to account for the node itself. term-size = fold-exp(BinOpSize, AssignSize, IfSize, ...) BinOpSize : (Plus(), e1, e2) -> (e1, e2) AssignSize : (e1, e2) -> (e1, e2) IfSize : (e1, e2, e3) -> (e1, (e2, e3)) This looks suspiciously like the traversal rules in Chapter 5. Defining folds in this manner has several limitations. In the definition of fold, one parameter for each constructor is provided and traversal is defined explicitly for each constructor. Furthermore, in the instantiation of fold, one rule for each constructor is needed, and the default behaviour is not generically specified. One solution would be to use the generic traversal strategy bottomup to deal with fold: fold-exp(s) = bottomup(s) term-size = fold-exp(BinOpSize <+ AssignSize <+ IfSize <+ ...) BinOpSize : BinOp(Plus(), e1, e2) -> (1, (e1, e2)) AssignSize : Assign(e1, e2) -> (e1, e2) IfSize : If(e1, e2, e3) -> (e1, (e2, e3)) Although the recursive application to subterms is now defined generically , one still has to specify rules for the default behavior. 10.3. Generic Term Deconstruction Instead of having folding rules that are specific to a data type, such as BinOpSize : BinOp(op, e1, e2) -> (1, (e1, e2)) AssignSize : Assign(e1, e2) -> (1, (e1, e2)) we would like to have a generic definition of the form CSize : c(e1, e2, ...) -> (e1, (e2, ...)) This requires generic decomposition of a constructor application into its constructor and the list with children. This can be done using the # operator. The match strategy ?p1#(p2) decomposes a constructor application into its constructor name and the list of direct subterms. Matching such a pattern against a term of the form C(t1,...,tn) results in a match of \"C\" against p1 and a match of [t1,...,tn] against p2. Plus(Int(\"1\"), Var(\"2\")) stratego> ?c#(xs) stratego> :binding c variable c bound to \"Plus\" stratego> :binding xs variable xs bound to [Int(\"1\"), Var(\"2\")] Crush. Using generic term deconstruction we can now generalize the type unifying operations on lists to arbitrary terms. In analogy with the generic traversal operators we need a generic one-level reduction operator. The crush/3 strategy reduces a constructor application by folding the list of its subterms using foldr/3. crush(nul, sum, s) : c#(xs) -> xs Thus, crush performs a fold-map over the direct subterms of a term. The following application illustrates what C(t1, t2) => ( t1, ( t2, [])) The following Shell session instantiates this application in two ways: stratego> import libstrategolib stratego> !Plus(Int(\"1\"), Var(\"2\")) Plus(Int(\"1\"),Var(\"2\")) stratego> crush(id, id, id) (Int(\"1\"),(Var(\"2\"),[])) stratego> !Plus(Int(\"1\"), Var(\"2\")) Plus(Int(\"1\"),Var(\"2\")) stratego> crush(!Tail( ), !Sum( , ), !Arg( )) Sum(Arg(Int(\"1\")),Sum(Arg(Var(\"2\")),Tail([]))) The crush strategy is the tool we need to implement solutions for the example problems above. Size. Counting the number of direct subterms of a term is similar to counting the number of elements of a list. The definition of node-size is the same as the definition of length, except that it uses crush instead of foldr: node-size = crush(!0, add, !1) Counting the number of subterms (nodes) in a term is a similar problem. But, instead of counting each direct subterm as 1, we need to count its subterms. term-size = crush(!1, add, term-size) The term-size strategy achieves this simply with a recursive call to itself. stratego> Plus(Int(\"1\"), Var(\"2\")) 2 stratego> Plus(Int(\"1\"), Var(\"2\")) 5 Occurrences. Counting the number of occurrences of a certain term in another term, or more generally, counting the number of subterms that satisfy some predicate is similar to counting the term size. However, only those terms satisfying the predicate should be counted. The solution is again similar to the solution for lists, but now using crush. om-occurrences(s) = s < !1 + crush(!0, add, om-occurrences(s)) The om-occurrences strategy counts the outermost subterms satisfying s. That is, the strategy stops counting as soon as it finds a subterm for which s succeeds. The following strategy counts all occurrences: occurrences(s) = (<s < !1 + !0>, ) It counts the current term if it satisfies s and adds that to the occurrences in the subterms. stratego> Plus(Int(\"1\"), Plus(Int(\"34\"), Var(\"2\"))) 2 stratego> Plus(Int(\"1\"), Plus(Int(\"34\"), Var(\"2\"))) 1 stratego> Plus(Int(\"1\"), Plus(Int(\"34\"), Var(\"2\"))) 2 Collect. Collecting the subterms that satisfy a predicate is similar to counting, but now a list of subterms is produced. The collect(s) strategy collects all outermost occurrences satisfying s. collect(s) = ![ ] <+ crush(![], union, collect(s)) When encountering a subterm for which s succeeds, a singleton list is produced. For other terms, the matching subterms are collected for each direct subterm, and the resulting lists are combined with union to remove duplicates. A typical application of collect is the collection of all variables in an expression, which can be defined as follows: get-vars = collect(?Var( )) Applying get-vars to an expression AST produces the list of all subterms matching Var( ). The collect-all(s) strategy collects all occurrences satisfying s. collect-all(s) = ![ | ] <+ crush(![], union, collect(s)) If s succeeds for the subject term combines the subject term with the collected terms from the subterms. Free Variables. Collecting the variables in an expression is easy, as we saw above. However, when dealing with languages with variable bindings, a common operation is to extract only the free variables in an expression or block of statements. That is, the occurrences of variables that are not bound by a variable declaration. For example, in the expression x + let var y := x + 1 in f(y, a + x + b) end the free variables are {x, a, b}, but not y, since it is bound by the declaration in the let. Similarly, in the function definition function f(x : int) = let var y := h(x) in x + g(z) * y end the only free variable is z since x and y are declared. Here is a free variable extraction strategy for Tiger expressions. It follows a similar pattern of mixing generic and data-type specific operations as we saw in Chapter 5. The crush alternative takes care of the non-special constructors, while ExpVars and FreeVars deal with the special cases, i.e. variables and variable binding constructs: free-vars = ExpVars <+ FreeVars(free-vars) <+ crush(![], union, free-vars) ExpVars : Var(x) -> [x] FreeVars(fv) : Let([VarDec(x, t, e1)], e2) -> ( e1, ( e2, [x])) FreeVars(fv) : Let([FunctionDec(fdecs)], e2) -> ( ( fdecs, e2), fs) where ,_,_,_))> fdecs => fs FreeVars(fv) : FunDec(f, xs, t, e) -> ( e, xs) where xs => xs The FreeVars rules for binding constructs use their fv parameter to recursively get the free variables from subterms, and they subtract the bound variables from any free variables found using diff. We can even capture the pattern exhibited here in a generic collection algorithm with support for special cases: collect-exc(base, special : (a -> b) * a -> b) = base <+ special(collect-exc(base, special)) <+ crush(![], union, collect-exc(base, special)) The special parameter is a strategy parameterized with a recursive call to the collection strategy. The original definition of free-vars above, can now be replaced with free-vars = collect-exc(ExpVars, FreeVars) 10.4. Generic Term Construction It can also be useful to construct terms generically. For example, in parse tree implosion, application nodes should be reduced to constructor applications. Hence build operators can also use the # operator. In a strategy !p1#(p2), the current subject term is replaced by a constructor application, where the constructor name is provided by p1 and the list of subterms by p2. So, if p1 evaluates to \"C\" and p2 evaluates to [t1,...,tn], the expression !p1#(p2) build the term C(t1,...,tn). Imploding Parse Trees. A typical application of generic term construction is the implosion of parse trees to abstract syntax trees performed by implode-asfix. Parse trees produced by sglr have the form: appl(prod(sorts, sort, attrs([cons(\"C\")])),[t1,...,tn]) That is, a node in a parse tree consists of an encoding of the original production from the syntax definition, and a list with subtrees. The production includes a constructor annotation cons(\"C\") with the name of the abstract syntax tree constructor. Such a tree node should be imploded to an abstract syntax tree node of the form C(t1,...,tn). Thus, this requires the construction of a term with constructor C given the string with its name. The following implosion strategy achieves this using generic term construction: implode = appl(id, map(implode)); Implode Implode : appl(prod(sorts, sort, attrs([cons\u00a9])), ts) -> c#(ts) The Implode rule rewrites an appl term to a constructor application, by extracting the constructor name from the production and then using generic term construction to apply the constructor. Note that this is a gross over simplification of the actual implementation of implode-asfix. See the source code for the full strategy. Generic term construction and deconstruction support the definition of generic analysis and generic translation problems. The generic solutions for the example problems term size, number of occurrences, and subterm collection demonstrate the general approach to solving these types of problems.","title":"Type Unifying Transformations"},{"location":"references/stratego/strategy-combinators-type-unifying/#references","text":"Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9","title":"References"},{"location":"references/stratego/strategy-combinators/","text":"Strategy Combinators \u00b6 Rather than defining rewrite rules and high-level strategies as primitives of the language, Stratego provides strategy combinators as basic building blocks from which these can defined 1 . Thus, Stratego consists of a core language 2 and a 'sugar' language defined by reduction to the core language. Warning While it useful to understand the constructs defined in this and the next sections, their use should be avoided in favour of the higher-level language constructs where possible. Identity and Failure \u00b6 The most basic operations in Stratego are id and fail . The identity strategy id always succeeds and behaves as the identity function on terms. The failure strategy fail always fails. The operations have no side effects. Sequential Composition \u00b6 The sequential composition s1 ; s2 of the strategies s1 and s2 first applies the strategy s1 to the subject term and then s2 to the result of that first application. The strategy fails if either s1 or s2 fails. Sequential composition is associative. Identity is a left and right unit for sequential composition; since id always succeeds and leaves the term alone, it has no additional effect to the strategy that it is composed with. Failure is a left zero for sequential composition; since fail always fails the next strategy will never be reached. This leads to the following equations: ( s1 ; s2 ) ; s3 = s1 ; ( s2 ; s3 ) id ; s = s s ; id = s fail ; s = fail However, not for all strategies s we have that failure is a right zero for sequential composition: s ; fail = fail // is not a law Although the composition s; fail will always fail, the execution of s may have side effects that are not performed by fail . For example, consider printing a term in s . As an example of the use of sequential composition consider the following rewrite rules. A : P ( Z (), x ) - > x B : P ( S ( x ), y ) - > P ( x , S ( y )) The following applications shows the effect of first applying B and then A : < B > ! P ( S ( Z ()), Z ()) => P ( S ( Z ), Z ) < A > P ( Z , S ( Z )) => S ( Z ) Using the sequential composition of the two rules, this effect can be achieved \u2018in one step\u2019: < B ; A > ! P ( S ( Z ()), Z ()) => S ( Z ) The following application shows that the application of a composition fails if the second strategy in the composition fails to apply to the result of the first: < B ; B > ! P ( S ( Z ()), Z ()) // fails Left Choice \u00b6 Choosing between rules to apply is achieved using one of several choice combinators, all of which are based on the guarded choice combinator. The common approach is that failure to apply one strategy leads to backtracking to an alternative strategy. The left choice or deterministic choice s1 <+ s2 tries to apply s1 and s2 in that order. That is, it first tries to apply s1 , and if that succeeds the choice succeeds. However, if the application of s1 fails, s2 is applied to the original term. Properties. Left choice is associative. Identity is a left zero for left choice; since id always succeeds, the alternative strategy will never be tried. Failure is a left and right unit for left choice; since fail always fails, the choice will always backtrack to the alternative strategy, and use of fail as alternative strategy is pointless. ( s1 < + s2 ) < + s3 = s1 < + ( s2 < + s3 ) id < + s = id fail < + s = s s < + fail = s However, identity is not a right zero for left choice. That is, not for all strategies s we have that s < + id = s // is not a law The expression s <+ id always succeeds, even (especially) in the case that s fails, in which case the right-hand side of the equation fails of course. Local Backtracking. The left choice combinator is a local backtracking combinator. That is, the choice is committed once the left-hand side strategy has succeeded, even if the continuation strategy fails. This is expressed by the fact that the property ( s1 < + s2 ); s3 = ( s1 ; s3 ) < + ( s2 ; s3 ) // is not a law does not hold for all s1 , s2 , and s3 . The difference is illustrated by the following applications: <( B < + id ); B > P ( S ( Z ), Z ) // fails <( B ; B ) < + ( id ; B )> P ( S ( Z ()), Z ()) => P ( Z , S ( Z )) In the application of (B <+ id); B , the first application of B succeeds after which the choice is committed. The subsequent application of B then fails. This is equivalent to first applying (B <+ id) and then applying B to the result. The application of (B; B) <+ (id; B) , however, is successful; the application of B; B fails, after which the choice backtracks to id; B , which succeeds. Choosing between Transformations. \u00b6 The typical use of left choice is to create a composite strategy trying one from several possible transformations. If the strategies that are composed are mutually exclusive, that is, don\u2019t succeed for the same terms, their sum is a transformation that (deterministically) covers a larger set of terms. For example, consider the following two rewrite rules: PlusAssoc : Plus ( Plus ( e1 , e2 ), e3 ) - > Plus ( e1 , Plus ( e2 , e3 )) PlusZero : Plus ( Int ( \"0\" ), e ) - > e These rules are mutually exclusive, since there is no term that matches the left-hand sides of both rules. Combining the rules with left choice into PlusAssoc <+ PlusZero creates a strategy that transforms terms matching both rules as illustrated by the following applications: < PlusAssoc > Plus ( Int ( \"0\" ), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Int ( \"0\" ), Int ( \"3\" )) => Int ( \"3\" ) < PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) => Plus ( Var ( \"x\" ), Plus ( Int ( \"42\" ), Int ( \"3\" ))) Ordering Overlapping Rules. \u00b6 When two rules or strategies are mutually exlusive the order of applying them does not matter. In cases where strategies are overlapping, that is, succeed for the same terms, the order becomes crucial to determining the semantics of the composition. For example, consider the following rewrite rules reducing applications of Mem: Mem1 : Mem ( x ,[]) - > False () Mem2 : Mem ( x ,[ x | xs ]) - > True () Mem3 : Mem ( x ,[ y | ys ]) - > Mem ( x , ys ) Rules Mem2 and Mem3 have overlapping left-hand sides. Rule Mem2 only applies if the first argument is equal to the head element of the list in the second argument. Rule Mem3 applies always if the list in the second argument is non-empty. < Mem2 > Mem ( 1 , [ 1 , 2 , 3 ]) => True () < Mem3 > Mem ( 1 , [ 1 , 2 , 3 ]) => Mem ( 1 ,[ 2 , 3 ]) In such situations, depending on the order of the rules, different results are produced. (The rules form a non-confluent rewriting system.) By ordering the rules as Mem2 <+ Mem3 , rule Mem2 is tried before Mem3 , and we have a deterministic transformation strategy. Try \u00b6 A useful application of <+ in combination with id is the reflexive closure of a strategy s: try ( s ) = s < + id The user-defined strategy combinator try tries to apply its argument strategy s , but if that fails, just succeeds using id . Guarded Left Choice \u00b6 Sometimes it is not desirable to backtrack to the alternative specified in a choice. Rather, after passing a guard, the choice should be committed. This can be expressed using the guarded left choice operator s1 < s2 + s3 . If s1 succeeds s2 is applied, else s3 is applied. If s2 fails, the complete expression fails; no backtracking to s3 takes place. Properties. This combinator is a generalization of the left choice combinator <+ . s1 < + s2 = s1 < id + s2 The following laws make clear that the \u2018branches\u2019 of the choice are selected by the success or failure of the guard: id < s2 + s3 = s2 fail < s2 + s3 = s3 If the right branch always fails, the construct reduces to the sequential composition of the guard and the left branch. s1 < s2 + fail = s1 ; s2 Guarded choice is not associative: ( s1 < s2 + s3 ) < s4 + s5 = s1 < s2 + ( s3 < s4 + s5 ) // not a law To see why consider the possible traces of these expressions. For example, when s1 and s2 succeed subsequently, the left-hand side expression calls s4 , while the right-hand side expression does not. However, sequential composition distributes over guarded choice from left and right: ( s1 < s2 + s3 ); s4 = s1 < ( s2 ; s4 ) + ( s3 ; s4 ) s0 ; ( s1 < s2 + s3 ) = ( s0 ; s1 ) < s2 + s3 Examples. The guarded left choice operator is most useful for the implementation of higher-level control-flow strategies. For example, the negation not(s) of a strategy s , succeeds if s fails, and fails when it succeeds: not ( s ) = s < fail + id Since failure discards the effect of a (successful) transformation, this has the effect of testing whether s succeeds. So we have the following laws for not: not ( id ) = fail not ( fail ) = id However, side effects performed by s are not undone, of course. Therefore, the following equation does not hold: not ( not ( s )) = s // not a law Another example of the use of guarded choice is the restore-always combinator: restore-always ( s , r ) = s < r + ( r ; fail ) It applies a \u2018restore\u2019 strategy r after applying a strategy s , even if s fails, and preserves the success/failure behavior of s . Since fail discards the transformation effect of r , this is mostly useful for ensuring that some side-effecting operation is done (or undone) after applying s . If-then-else \u00b6 The guarded choice combinator is similar to the traditional if-then-else construct of programming languages. The difference is that the \u2018then\u2019 branch applies to the result of the application of the condition. Stratego\u2019s if s1 then s2 else s3 end construct is more like the traditional construct since both branches apply to the original term. The condition strategy is only used to test if it succeeds or fails, but it\u2019s transformation effect is undone. However, the condition strategy s1 is still applied to the current term. The if s1 then s2 end strategy is similar; if the condition fails, the strategy succeeds. The if-then-else-end strategy is just syntactic sugar for a combination of guarded choice and the where combinator: if s1 then s2 else s3 end ==> // transforms to where ( s1 ) < s2 + s3 The strategy where(s) succeeds if s succeeds, but returns the original subject term. The implementation of the where combinator is discussed in the section on matching and building terms . The following laws show that the branches are selected by success or failure of the condition: if id then s2 else s3 end = s2 if fail then s2 else s3 end = s3 The if-then-end strategy is an abbreviation for the if-then-else-end with the identity strategy as right branch: if s1 then s2 end = where ( s1 ) < s2 + id Examples. The inclusive or or(s1, s2) succeeds if one of the strategies s1 or s2 succeeds, but guarantees that both are applied, in the order s1 first, then s2 : or ( s1 , s2 ) = if s1 then try ( where ( s2 )) else where ( s2 ) end This ensures that any side effects are always performed, in contrast to s1 <\\+ s2 , where s2 is only executed if s1 fails. (Thus, left choice implements a short circuit Boolean or.) Similarly, the following and(s1, s2) combinator is the non-short circuit version of Boolean conjunction: and ( s1 , s2 ) = if s1 then where ( s2 ) else where ( s2 ); fail end Switch \u00b6 The switch construct is an n-ary branching construct similar to its counter parts in other programming languages. It is defined in terms of guarded choice. The switch construct has the following form: switch s0 case s1 : s1 ' case s2 : s2 ' ... otherwise : sdef end The switch first applies the s0 strategy to the current term t resulting in a term t' . Then it tries the cases in turn applying each si to t' . As soon as this succeeds the corresponding case is selected and si' is applied to the t , the term to which the switch was applied. If none of the cases applies, the default strategy sdef from the otherwise is applied. The switch construct is syntactic sugar for a nested if-then-else: { x : where ( s0 => x ); if < s1 > x then s1 ' else if < s2 > x then s2 ' else if ... then ... else sdef end end end } Non-Deterministic Choice \u00b6 The deterministic left choice operator prescribes that the left alternative should be tried before the right alternative, and that the latter is only used if the first fails. There are applications where it is not necessary to define the order of the alternatives. In those cases non-deterministic choice can be used. The non-deterministic choice operator s1 + s2 chooses one of the two strategies s1 or s2 to apply, such that the one it chooses succeeds. If both strategies fail, then the choice fails as well. Operationally the choice operator first tries one strategy, and, if that fails, tries the other. The order in which this is done is undefined, i.e., arbitrarily chosen by the compiler. The + combinator is used to model modular composition of rewrite rules and strategies with the same name. Multiple definitions with the same name, are merged into a single definition with that name, where the bodies are composed with + . The following transformation illustrates this: f = s1 f = s2 => f = s1 + s2 This transformation is somewhat simplified; the complete transformation needs to take care of local variables and parameters. While the + combinator is used internally by the compiler for this purpose, programmers are advised not to use this combinator, but rather use the left choice combinator <+ to avoid surprises. Recursion \u00b6 Repeated application of a strategy can be achieved with recursion. There are two styles for doing this; with a recursive definition or using the fixpoint operator rec . A recursive definition is a normal strategy definition with a recursive call in its body. f ( s ) = ... f ( s ) ... Another way to define recursion is using the fixpoint operator rec x(s) , which recurses on applications of x within s. For example, the definition f ( s ) = rec x ( ... x ... ) is equivalent to the one above. The advantage of the rec operator is that it allows the definition of an unnamed strategy expression to be recursive. For example, in the definition g ( s ) = foo ; rec x ( ... x ... ); bar the strategy between foo and bar is a recursive strategy that does not recurse to g(s) . Originally, the rec operator was the only way to define recursive strategies. It is still in the language in the first place because it is widely used in many existing programs, and in the second place because it can be a concise expression of a recursive strategy, since call parameters are not included in the call. Furthermore, all free variables remain in scope. The repeat strategy applies a transformation s until it fails. It is defined as a recursive definition using try as follows: try ( s ) = s < + id repeat ( s ) = try ( s ; repeat ( s )) An equivalent definition using rec is: repeat ( s ) = rec x ( try ( s ; x )) A Library of Iteration Strategies. \u00b6 Using sequential composition, choice, and recursion a large variety of iteration strategies can be defined. The following definitions are part of the Stratego Library (in module strategy/iteration). repeat ( s ) = rec x ( try ( s ; x )) repeat ( s , c ) = ( s ; repeat ( s , c )) < + c repeat1 ( s , c ) = s ; ( repeat1 ( s , c ) < + c ) repeat1 ( s ) = repeat1 ( s , id ) repeat-until ( s , c ) = s ; if c then id else repeat-until ( s , c ) end while ( c , s ) = if c then s ; while ( c , s ) end do-while ( s , c ) = s ; if c then do-while ( s , c ) end References \u00b6 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9","title":"Strategy Combinators"},{"location":"references/stratego/strategy-combinators/#strategy-combinators","text":"Rather than defining rewrite rules and high-level strategies as primitives of the language, Stratego provides strategy combinators as basic building blocks from which these can defined 1 . Thus, Stratego consists of a core language 2 and a 'sugar' language defined by reduction to the core language. Warning While it useful to understand the constructs defined in this and the next sections, their use should be avoided in favour of the higher-level language constructs where possible.","title":"Strategy Combinators"},{"location":"references/stratego/strategy-combinators/#identity-and-failure","text":"The most basic operations in Stratego are id and fail . The identity strategy id always succeeds and behaves as the identity function on terms. The failure strategy fail always fails. The operations have no side effects.","title":"Identity and Failure"},{"location":"references/stratego/strategy-combinators/#sequential-composition","text":"The sequential composition s1 ; s2 of the strategies s1 and s2 first applies the strategy s1 to the subject term and then s2 to the result of that first application. The strategy fails if either s1 or s2 fails. Sequential composition is associative. Identity is a left and right unit for sequential composition; since id always succeeds and leaves the term alone, it has no additional effect to the strategy that it is composed with. Failure is a left zero for sequential composition; since fail always fails the next strategy will never be reached. This leads to the following equations: ( s1 ; s2 ) ; s3 = s1 ; ( s2 ; s3 ) id ; s = s s ; id = s fail ; s = fail However, not for all strategies s we have that failure is a right zero for sequential composition: s ; fail = fail // is not a law Although the composition s; fail will always fail, the execution of s may have side effects that are not performed by fail . For example, consider printing a term in s . As an example of the use of sequential composition consider the following rewrite rules. A : P ( Z (), x ) - > x B : P ( S ( x ), y ) - > P ( x , S ( y )) The following applications shows the effect of first applying B and then A : < B > ! P ( S ( Z ()), Z ()) => P ( S ( Z ), Z ) < A > P ( Z , S ( Z )) => S ( Z ) Using the sequential composition of the two rules, this effect can be achieved \u2018in one step\u2019: < B ; A > ! P ( S ( Z ()), Z ()) => S ( Z ) The following application shows that the application of a composition fails if the second strategy in the composition fails to apply to the result of the first: < B ; B > ! P ( S ( Z ()), Z ()) // fails","title":"Sequential Composition"},{"location":"references/stratego/strategy-combinators/#left-choice","text":"Choosing between rules to apply is achieved using one of several choice combinators, all of which are based on the guarded choice combinator. The common approach is that failure to apply one strategy leads to backtracking to an alternative strategy. The left choice or deterministic choice s1 <+ s2 tries to apply s1 and s2 in that order. That is, it first tries to apply s1 , and if that succeeds the choice succeeds. However, if the application of s1 fails, s2 is applied to the original term. Properties. Left choice is associative. Identity is a left zero for left choice; since id always succeeds, the alternative strategy will never be tried. Failure is a left and right unit for left choice; since fail always fails, the choice will always backtrack to the alternative strategy, and use of fail as alternative strategy is pointless. ( s1 < + s2 ) < + s3 = s1 < + ( s2 < + s3 ) id < + s = id fail < + s = s s < + fail = s However, identity is not a right zero for left choice. That is, not for all strategies s we have that s < + id = s // is not a law The expression s <+ id always succeeds, even (especially) in the case that s fails, in which case the right-hand side of the equation fails of course. Local Backtracking. The left choice combinator is a local backtracking combinator. That is, the choice is committed once the left-hand side strategy has succeeded, even if the continuation strategy fails. This is expressed by the fact that the property ( s1 < + s2 ); s3 = ( s1 ; s3 ) < + ( s2 ; s3 ) // is not a law does not hold for all s1 , s2 , and s3 . The difference is illustrated by the following applications: <( B < + id ); B > P ( S ( Z ), Z ) // fails <( B ; B ) < + ( id ; B )> P ( S ( Z ()), Z ()) => P ( Z , S ( Z )) In the application of (B <+ id); B , the first application of B succeeds after which the choice is committed. The subsequent application of B then fails. This is equivalent to first applying (B <+ id) and then applying B to the result. The application of (B; B) <+ (id; B) , however, is successful; the application of B; B fails, after which the choice backtracks to id; B , which succeeds.","title":"Left Choice"},{"location":"references/stratego/strategy-combinators/#choosing-between-transformations","text":"The typical use of left choice is to create a composite strategy trying one from several possible transformations. If the strategies that are composed are mutually exclusive, that is, don\u2019t succeed for the same terms, their sum is a transformation that (deterministically) covers a larger set of terms. For example, consider the following two rewrite rules: PlusAssoc : Plus ( Plus ( e1 , e2 ), e3 ) - > Plus ( e1 , Plus ( e2 , e3 )) PlusZero : Plus ( Int ( \"0\" ), e ) - > e These rules are mutually exclusive, since there is no term that matches the left-hand sides of both rules. Combining the rules with left choice into PlusAssoc <+ PlusZero creates a strategy that transforms terms matching both rules as illustrated by the following applications: < PlusAssoc > Plus ( Int ( \"0\" ), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Int ( \"0\" ), Int ( \"3\" )) => Int ( \"3\" ) < PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) => Plus ( Var ( \"x\" ), Plus ( Int ( \"42\" ), Int ( \"3\" )))","title":"Choosing between Transformations."},{"location":"references/stratego/strategy-combinators/#ordering-overlapping-rules","text":"When two rules or strategies are mutually exlusive the order of applying them does not matter. In cases where strategies are overlapping, that is, succeed for the same terms, the order becomes crucial to determining the semantics of the composition. For example, consider the following rewrite rules reducing applications of Mem: Mem1 : Mem ( x ,[]) - > False () Mem2 : Mem ( x ,[ x | xs ]) - > True () Mem3 : Mem ( x ,[ y | ys ]) - > Mem ( x , ys ) Rules Mem2 and Mem3 have overlapping left-hand sides. Rule Mem2 only applies if the first argument is equal to the head element of the list in the second argument. Rule Mem3 applies always if the list in the second argument is non-empty. < Mem2 > Mem ( 1 , [ 1 , 2 , 3 ]) => True () < Mem3 > Mem ( 1 , [ 1 , 2 , 3 ]) => Mem ( 1 ,[ 2 , 3 ]) In such situations, depending on the order of the rules, different results are produced. (The rules form a non-confluent rewriting system.) By ordering the rules as Mem2 <+ Mem3 , rule Mem2 is tried before Mem3 , and we have a deterministic transformation strategy.","title":"Ordering Overlapping Rules."},{"location":"references/stratego/strategy-combinators/#try","text":"A useful application of <+ in combination with id is the reflexive closure of a strategy s: try ( s ) = s < + id The user-defined strategy combinator try tries to apply its argument strategy s , but if that fails, just succeeds using id .","title":"Try"},{"location":"references/stratego/strategy-combinators/#guarded-left-choice","text":"Sometimes it is not desirable to backtrack to the alternative specified in a choice. Rather, after passing a guard, the choice should be committed. This can be expressed using the guarded left choice operator s1 < s2 + s3 . If s1 succeeds s2 is applied, else s3 is applied. If s2 fails, the complete expression fails; no backtracking to s3 takes place. Properties. This combinator is a generalization of the left choice combinator <+ . s1 < + s2 = s1 < id + s2 The following laws make clear that the \u2018branches\u2019 of the choice are selected by the success or failure of the guard: id < s2 + s3 = s2 fail < s2 + s3 = s3 If the right branch always fails, the construct reduces to the sequential composition of the guard and the left branch. s1 < s2 + fail = s1 ; s2 Guarded choice is not associative: ( s1 < s2 + s3 ) < s4 + s5 = s1 < s2 + ( s3 < s4 + s5 ) // not a law To see why consider the possible traces of these expressions. For example, when s1 and s2 succeed subsequently, the left-hand side expression calls s4 , while the right-hand side expression does not. However, sequential composition distributes over guarded choice from left and right: ( s1 < s2 + s3 ); s4 = s1 < ( s2 ; s4 ) + ( s3 ; s4 ) s0 ; ( s1 < s2 + s3 ) = ( s0 ; s1 ) < s2 + s3 Examples. The guarded left choice operator is most useful for the implementation of higher-level control-flow strategies. For example, the negation not(s) of a strategy s , succeeds if s fails, and fails when it succeeds: not ( s ) = s < fail + id Since failure discards the effect of a (successful) transformation, this has the effect of testing whether s succeeds. So we have the following laws for not: not ( id ) = fail not ( fail ) = id However, side effects performed by s are not undone, of course. Therefore, the following equation does not hold: not ( not ( s )) = s // not a law Another example of the use of guarded choice is the restore-always combinator: restore-always ( s , r ) = s < r + ( r ; fail ) It applies a \u2018restore\u2019 strategy r after applying a strategy s , even if s fails, and preserves the success/failure behavior of s . Since fail discards the transformation effect of r , this is mostly useful for ensuring that some side-effecting operation is done (or undone) after applying s .","title":"Guarded Left Choice"},{"location":"references/stratego/strategy-combinators/#if-then-else","text":"The guarded choice combinator is similar to the traditional if-then-else construct of programming languages. The difference is that the \u2018then\u2019 branch applies to the result of the application of the condition. Stratego\u2019s if s1 then s2 else s3 end construct is more like the traditional construct since both branches apply to the original term. The condition strategy is only used to test if it succeeds or fails, but it\u2019s transformation effect is undone. However, the condition strategy s1 is still applied to the current term. The if s1 then s2 end strategy is similar; if the condition fails, the strategy succeeds. The if-then-else-end strategy is just syntactic sugar for a combination of guarded choice and the where combinator: if s1 then s2 else s3 end ==> // transforms to where ( s1 ) < s2 + s3 The strategy where(s) succeeds if s succeeds, but returns the original subject term. The implementation of the where combinator is discussed in the section on matching and building terms . The following laws show that the branches are selected by success or failure of the condition: if id then s2 else s3 end = s2 if fail then s2 else s3 end = s3 The if-then-end strategy is an abbreviation for the if-then-else-end with the identity strategy as right branch: if s1 then s2 end = where ( s1 ) < s2 + id Examples. The inclusive or or(s1, s2) succeeds if one of the strategies s1 or s2 succeeds, but guarantees that both are applied, in the order s1 first, then s2 : or ( s1 , s2 ) = if s1 then try ( where ( s2 )) else where ( s2 ) end This ensures that any side effects are always performed, in contrast to s1 <\\+ s2 , where s2 is only executed if s1 fails. (Thus, left choice implements a short circuit Boolean or.) Similarly, the following and(s1, s2) combinator is the non-short circuit version of Boolean conjunction: and ( s1 , s2 ) = if s1 then where ( s2 ) else where ( s2 ); fail end","title":"If-then-else"},{"location":"references/stratego/strategy-combinators/#switch","text":"The switch construct is an n-ary branching construct similar to its counter parts in other programming languages. It is defined in terms of guarded choice. The switch construct has the following form: switch s0 case s1 : s1 ' case s2 : s2 ' ... otherwise : sdef end The switch first applies the s0 strategy to the current term t resulting in a term t' . Then it tries the cases in turn applying each si to t' . As soon as this succeeds the corresponding case is selected and si' is applied to the t , the term to which the switch was applied. If none of the cases applies, the default strategy sdef from the otherwise is applied. The switch construct is syntactic sugar for a nested if-then-else: { x : where ( s0 => x ); if < s1 > x then s1 ' else if < s2 > x then s2 ' else if ... then ... else sdef end end end }","title":"Switch"},{"location":"references/stratego/strategy-combinators/#non-deterministic-choice","text":"The deterministic left choice operator prescribes that the left alternative should be tried before the right alternative, and that the latter is only used if the first fails. There are applications where it is not necessary to define the order of the alternatives. In those cases non-deterministic choice can be used. The non-deterministic choice operator s1 + s2 chooses one of the two strategies s1 or s2 to apply, such that the one it chooses succeeds. If both strategies fail, then the choice fails as well. Operationally the choice operator first tries one strategy, and, if that fails, tries the other. The order in which this is done is undefined, i.e., arbitrarily chosen by the compiler. The + combinator is used to model modular composition of rewrite rules and strategies with the same name. Multiple definitions with the same name, are merged into a single definition with that name, where the bodies are composed with + . The following transformation illustrates this: f = s1 f = s2 => f = s1 + s2 This transformation is somewhat simplified; the complete transformation needs to take care of local variables and parameters. While the + combinator is used internally by the compiler for this purpose, programmers are advised not to use this combinator, but rather use the left choice combinator <+ to avoid surprises.","title":"Non-Deterministic Choice"},{"location":"references/stratego/strategy-combinators/#recursion","text":"Repeated application of a strategy can be achieved with recursion. There are two styles for doing this; with a recursive definition or using the fixpoint operator rec . A recursive definition is a normal strategy definition with a recursive call in its body. f ( s ) = ... f ( s ) ... Another way to define recursion is using the fixpoint operator rec x(s) , which recurses on applications of x within s. For example, the definition f ( s ) = rec x ( ... x ... ) is equivalent to the one above. The advantage of the rec operator is that it allows the definition of an unnamed strategy expression to be recursive. For example, in the definition g ( s ) = foo ; rec x ( ... x ... ); bar the strategy between foo and bar is a recursive strategy that does not recurse to g(s) . Originally, the rec operator was the only way to define recursive strategies. It is still in the language in the first place because it is widely used in many existing programs, and in the second place because it can be a concise expression of a recursive strategy, since call parameters are not included in the call. Furthermore, all free variables remain in scope. The repeat strategy applies a transformation s until it fails. It is defined as a recursive definition using try as follows: try ( s ) = s < + id repeat ( s ) = try ( s ; repeat ( s )) An equivalent definition using rec is: repeat ( s ) = rec x ( try ( s ; x ))","title":"Recursion"},{"location":"references/stratego/strategy-combinators/#a-library-of-iteration-strategies","text":"Using sequential composition, choice, and recursion a large variety of iteration strategies can be defined. The following definitions are part of the Stratego Library (in module strategy/iteration). repeat ( s ) = rec x ( try ( s ; x )) repeat ( s , c ) = ( s ; repeat ( s , c )) < + c repeat1 ( s , c ) = s ; ( repeat1 ( s , c ) < + c ) repeat1 ( s ) = repeat1 ( s , id ) repeat-until ( s , c ) = s ; if c then id else repeat-until ( s , c ) end while ( c , s ) = if c then s ; while ( c , s ) end do-while ( s , c ) = s ; if c then do-while ( s , c ) end","title":"A Library of Iteration Strategies."},{"location":"references/stratego/strategy-combinators/#references","text":"Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661(05)80027-1 , doi:10.1016/S1571-0661(05)80027-1 . \u21a9","title":"References"},{"location":"references/stratego/terms/","text":"Terms \u00b6 Stratego programs transform terms. When using Stratego for program transformation, terms typically represent the abstract syntax tree of a program. But Stratego does not much care what a term represents. Terms can just as well represent structured documents, software models, or anything else that can be rendered in a structured format. Generally program text is transformed into a term by means of parsing, and turned back into program text by means of pretty-printing. One way to achieve this is by using SDF3 . Annotated Term Format \u00b6 Terms in Stratego are terms in the Annotated Term Format , or ATerms for short 1 . The ATerm format provides a set of constructs for representing trees, comparable to XML or abstract data types in functional programming languages. For example, the code 4 + f(5 * x) might be represented in a term as: Plus ( Int ( \"4\" ), Call ( \"f\" , [ Mul ( Int ( \"5\" ), Var ( \"x\" ))])) ATerms are constructed from the following elements: Integer : An integer constant, that is a list of decimal digits, is an ATerm. Examples: 1 , 12343 {.docutils .literal .notranslate}. String : A string constant, that is a list of characters between double quotes is an ATerm. Special characters such as double quotes and newlines should be escaped using a backslash. The backslash character itself should be escaped as well. Examples: \"foobar\" , \"string with quotes\\\"\" , \"escaped escape character\\\\ and a newline\\n\" . Constructor application : A constructor is an identifier, that is an alphanumeric string starting with a letter, or a double quoted string. A constructor application c(t1,...,tn) creates a term by applying a constructor to a list of zero or more terms. For example, the term Plus(Int(\"4\"),Var(\"x\")) uses the constructors Plus , Int , and Var to create a nested term from the strings \"4\" and \"x\" . List : A list is a term of the form [t1,...,tn] , that is a list of zero or more terms between square brackets. While all applications of a specific constructor typically have the same number of subterms, lists can have a variable number of subterms. The elements of a list are typically of the same type, while the subterms of a constructor application can vary in type. Example: The second argument of the call to \"f\" in the term Call(\"f\",[Int(\"5\"),Var(\"x\")]) is a list of expressions. Tuple : A tuple (t1,...,tn) is a constructor application without a constructor. Example: (Var(\"x\"), Type(\"int\")) Annotation : The elements defined above are used to create the structural part of terms. Optionally, a term can be annotated with a list of terms. These annotations typically carry additional semantic information about the term. An annotated term has the form t{t1,...,tn} . Example: Lt(Var(\"n\"),Int(\"1\")){Type(\"bool\")} . The contents of annotations is up to the application. Persistent Representation \u00b6 The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer. Namespaces \u00b6 Currently, the constructors of terms live in a global namespace. In the future, we want to support qualified names. References \u00b6 Mark G. J. van den Brand, H. A. de Jong, Paul Klint, and Pieter A. Olivier. Efficient annotated terms. Software: Practice and Experience , 30(3):259\u2013291, 2000. \u21a9","title":"Terms"},{"location":"references/stratego/terms/#terms","text":"Stratego programs transform terms. When using Stratego for program transformation, terms typically represent the abstract syntax tree of a program. But Stratego does not much care what a term represents. Terms can just as well represent structured documents, software models, or anything else that can be rendered in a structured format. Generally program text is transformed into a term by means of parsing, and turned back into program text by means of pretty-printing. One way to achieve this is by using SDF3 .","title":"Terms"},{"location":"references/stratego/terms/#annotated-term-format","text":"Terms in Stratego are terms in the Annotated Term Format , or ATerms for short 1 . The ATerm format provides a set of constructs for representing trees, comparable to XML or abstract data types in functional programming languages. For example, the code 4 + f(5 * x) might be represented in a term as: Plus ( Int ( \"4\" ), Call ( \"f\" , [ Mul ( Int ( \"5\" ), Var ( \"x\" ))])) ATerms are constructed from the following elements: Integer : An integer constant, that is a list of decimal digits, is an ATerm. Examples: 1 , 12343 {.docutils .literal .notranslate}. String : A string constant, that is a list of characters between double quotes is an ATerm. Special characters such as double quotes and newlines should be escaped using a backslash. The backslash character itself should be escaped as well. Examples: \"foobar\" , \"string with quotes\\\"\" , \"escaped escape character\\\\ and a newline\\n\" . Constructor application : A constructor is an identifier, that is an alphanumeric string starting with a letter, or a double quoted string. A constructor application c(t1,...,tn) creates a term by applying a constructor to a list of zero or more terms. For example, the term Plus(Int(\"4\"),Var(\"x\")) uses the constructors Plus , Int , and Var to create a nested term from the strings \"4\" and \"x\" . List : A list is a term of the form [t1,...,tn] , that is a list of zero or more terms between square brackets. While all applications of a specific constructor typically have the same number of subterms, lists can have a variable number of subterms. The elements of a list are typically of the same type, while the subterms of a constructor application can vary in type. Example: The second argument of the call to \"f\" in the term Call(\"f\",[Int(\"5\"),Var(\"x\")]) is a list of expressions. Tuple : A tuple (t1,...,tn) is a constructor application without a constructor. Example: (Var(\"x\"), Type(\"int\")) Annotation : The elements defined above are used to create the structural part of terms. Optionally, a term can be annotated with a list of terms. These annotations typically carry additional semantic information about the term. An annotated term has the form t{t1,...,tn} . Example: Lt(Var(\"n\"),Int(\"1\")){Type(\"bool\")} . The contents of annotations is up to the application.","title":"Annotated Term Format"},{"location":"references/stratego/terms/#persistent-representation","text":"The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer.","title":"Persistent Representation"},{"location":"references/stratego/terms/#namespaces","text":"Currently, the constructors of terms live in a global namespace. In the future, we want to support qualified names.","title":"Namespaces"},{"location":"references/stratego/terms/#references","text":"Mark G. J. van den Brand, H. A. de Jong, Paul Klint, and Pieter A. Olivier. Efficient annotated terms. Software: Practice and Experience , 30(3):259\u2013291, 2000. \u21a9","title":"References"},{"location":"references/stratego/types/","text":"Types \u00b6 Annotated Terms provide a generic, untyped format to represent tree-structured data. Stratego transformations can transform such data, but require at least that the arities of term constructors that are used in rules are declared. Further, starting with Stratego 2, the types of terms and term transformations may be declared and checked in more detail 1 . Term Signatures \u00b6 A signature declares sorts and constructors for these sorts using a definition of the form: signature sorts $ Sort ... constructors $ Constructor : $ Type * ... - > $ Sort A sort is determined by an identifier and optionally has arguments. For each constructor, a signature declares the number and types of its arguments. The Stratego1 compiler only checks the arity of constructor applications against the signature. The Stratego2 compiler uses signature definitions to type check code if it has been given a type signature. Injections \u00b6 An injection is a constructor without name and with a single domain argument. signature constructors : $ Sort - > $ Sort Injections include an entire type as a subtype of another type without cluttering the tree structure. Example Signature \u00b6 For example, the following signature declares some typical constructors for constructing abstract syntax trees of expressions in a programming language: signature sorts Id Exp constructors : string - > Id Var : Id - > Exp Int : Int - > Exp Plus : Exp * Exp - > Exp Mul : Exp * Exp - > Exp Call : Id * List ( Exp ) - > Exp Note that the first injection allows strings to be used as identifiers ( Id ). The List Type \u00b6 The List(Exp) type used above is built-in and corresponds to homogenous lists of terms of the same type ( Exp in this case). Polymorphic Types \u00b6 Stratego2 also supports user-defined polymorphic types. That is, sorts can have parameters. For example, the following signature defines the type of priority queues, polymorphic in the carrier type, in which the priority is determined by the length of the list. signature sorts PrioQ ( * ) constructors NilQ : PrioQ ( a ) ConsQ : a * int * List ( a ) * PrioQ ( a ) - > PrioQ ( a ) Signature from Syntax Definition \u00b6 Signatures can be generated automatically from a syntax definition in SDF3 . That ensures that the signature describes exactly the abstract syntax terms generated by a parser generated from that syntax definition. Furthermore, an automatically generated pretty-printer maps terms conforming to such a signature to a well-formed textual representation of the program. Todo link to pretty-printer generation Transformation Types \u00b6 Starting with Stratego2, the language also supports the definition of types for transformation definitions. The general form of a transformation type definition is $ Id ( $ StrategyType , ... | $ TermType , ... ) :: $ TermType - > $ TermType defining the signature of a transformation with name $Id with the types of its strategy arguments and term arguments, the type of the 'current term' to which the transformation is applied, and the type of the term that is returned, if the transformation succeeds. When a transformation only has strategy parameters, the bar can be left out, resulting in a signature of the form: $ Id ( $ StrategyType , ... ) :: $ TermType - > $ TermType When a transformation also has no strategy parameters, the parentheses can be left out as well, resulting in a signature of the form: $ Id :: $ TermType - > $ TermType Type Dynamic \u00b6 Stratego2 is a gradually typed language in order to facilitate the migration from (mostly) untyped Stratego1 code to typed Stratego2 code. Furthermore, some patterns in Stratego cannot be typed statically. Type dynamic , written ? , represents the unknown type. Type Casts \u00b6 Gradual type systems allow a term with the dynamic type to be used in any place where a static type is required. Stratego2 will insert a type cast at such a point to check at run time that the term is type-correct. This way, a Stratego program halts execution in predictable places when a run time type error occurs. There can be no run time type errors in fully statically typed code either, only at the boundary between dynamically and statically typed code. Type Preserving Transformations \u00b6 A type preserving transformation transforms any type to itself (or fails). In signatures, a type preserving transformation is indicated with TP . For example, the topdown traversal is type preserving if its argument strategy is. Thus, its signature is defined as topdown ( s : TP ) :: TP The type-checking for a type preserving transformation is very strict. It should be in terms of other type preserving transformations, or match the input term to a specific type and return a term from that specific type. Is Type \u00b6 Given the definition of a term signature, the is(S) strategy checks whether a term is of sort S and fails if that is not the case. For example, the strategy <is(Exp)>t checks that term t conforms to the signature of sort Exp . The is(S) strategy uses the same mechanism as type casts for checking a term type at run time. References \u00b6 Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9","title":"Types"},{"location":"references/stratego/types/#types","text":"Annotated Terms provide a generic, untyped format to represent tree-structured data. Stratego transformations can transform such data, but require at least that the arities of term constructors that are used in rules are declared. Further, starting with Stratego 2, the types of terms and term transformations may be declared and checked in more detail 1 .","title":"Types"},{"location":"references/stratego/types/#term-signatures","text":"A signature declares sorts and constructors for these sorts using a definition of the form: signature sorts $ Sort ... constructors $ Constructor : $ Type * ... - > $ Sort A sort is determined by an identifier and optionally has arguments. For each constructor, a signature declares the number and types of its arguments. The Stratego1 compiler only checks the arity of constructor applications against the signature. The Stratego2 compiler uses signature definitions to type check code if it has been given a type signature.","title":"Term Signatures"},{"location":"references/stratego/types/#injections","text":"An injection is a constructor without name and with a single domain argument. signature constructors : $ Sort - > $ Sort Injections include an entire type as a subtype of another type without cluttering the tree structure.","title":"Injections"},{"location":"references/stratego/types/#example-signature","text":"For example, the following signature declares some typical constructors for constructing abstract syntax trees of expressions in a programming language: signature sorts Id Exp constructors : string - > Id Var : Id - > Exp Int : Int - > Exp Plus : Exp * Exp - > Exp Mul : Exp * Exp - > Exp Call : Id * List ( Exp ) - > Exp Note that the first injection allows strings to be used as identifiers ( Id ).","title":"Example Signature"},{"location":"references/stratego/types/#the-list-type","text":"The List(Exp) type used above is built-in and corresponds to homogenous lists of terms of the same type ( Exp in this case).","title":"The List Type"},{"location":"references/stratego/types/#polymorphic-types","text":"Stratego2 also supports user-defined polymorphic types. That is, sorts can have parameters. For example, the following signature defines the type of priority queues, polymorphic in the carrier type, in which the priority is determined by the length of the list. signature sorts PrioQ ( * ) constructors NilQ : PrioQ ( a ) ConsQ : a * int * List ( a ) * PrioQ ( a ) - > PrioQ ( a )","title":"Polymorphic Types"},{"location":"references/stratego/types/#signature-from-syntax-definition","text":"Signatures can be generated automatically from a syntax definition in SDF3 . That ensures that the signature describes exactly the abstract syntax terms generated by a parser generated from that syntax definition. Furthermore, an automatically generated pretty-printer maps terms conforming to such a signature to a well-formed textual representation of the program. Todo link to pretty-printer generation","title":"Signature from Syntax Definition"},{"location":"references/stratego/types/#transformation-types","text":"Starting with Stratego2, the language also supports the definition of types for transformation definitions. The general form of a transformation type definition is $ Id ( $ StrategyType , ... | $ TermType , ... ) :: $ TermType - > $ TermType defining the signature of a transformation with name $Id with the types of its strategy arguments and term arguments, the type of the 'current term' to which the transformation is applied, and the type of the term that is returned, if the transformation succeeds. When a transformation only has strategy parameters, the bar can be left out, resulting in a signature of the form: $ Id ( $ StrategyType , ... ) :: $ TermType - > $ TermType When a transformation also has no strategy parameters, the parentheses can be left out as well, resulting in a signature of the form: $ Id :: $ TermType - > $ TermType","title":"Transformation Types"},{"location":"references/stratego/types/#type-dynamic","text":"Stratego2 is a gradually typed language in order to facilitate the migration from (mostly) untyped Stratego1 code to typed Stratego2 code. Furthermore, some patterns in Stratego cannot be typed statically. Type dynamic , written ? , represents the unknown type.","title":"Type Dynamic"},{"location":"references/stratego/types/#type-casts","text":"Gradual type systems allow a term with the dynamic type to be used in any place where a static type is required. Stratego2 will insert a type cast at such a point to check at run time that the term is type-correct. This way, a Stratego program halts execution in predictable places when a run time type error occurs. There can be no run time type errors in fully statically typed code either, only at the boundary between dynamically and statically typed code.","title":"Type Casts"},{"location":"references/stratego/types/#type-preserving-transformations","text":"A type preserving transformation transforms any type to itself (or fails). In signatures, a type preserving transformation is indicated with TP . For example, the topdown traversal is type preserving if its argument strategy is. Thus, its signature is defined as topdown ( s : TP ) :: TP The type-checking for a type preserving transformation is very strict. It should be in terms of other type preserving transformations, or match the input term to a specific type and return a term from that specific type.","title":"Type Preserving Transformations"},{"location":"references/stratego/types/#is-type","text":"Given the definition of a term signature, the is(S) strategy checks whether a term is of sort S and fails if that is not the case. For example, the strategy <is(Exp)>t checks that term t conforms to the signature of sort Exp . The is(S) strategy uses the same mechanism as type casts for checking a term type at run time.","title":"Is Type"},{"location":"references/stratego/types/#references","text":"Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9","title":"References"},{"location":"references/syntax/","text":"SDF3 \u00b6 SDF3 is the meta-language in Spoofax for syntax definition. A syntax definition is structured as a collection of modules , which may import each other. Symbols are the building blocks of productions . Productions are defined for lexical , context-free , or kernel syntax. Start symbols indicate the entry point of a syntax definition. SDF3 automatically generates a pretty-printer for template -based productions. Grammars can be disambiguated by means of rejects, priorities, associativity, and restrictions. SDF3 provides additional constructs for the definition of layout-sensitivite languages. Permissive grammars are automatically generated for error-recovery parsing. Handwritten recovery rules can be added to tweak recovery behavior. Source \u00b6 The sources of the SDF3 implementation can be found at https://github.com/metaborg/sdf/tree/master/org.metaborg.meta.lang.template : The SDF3 language implementation (SDF3 was called TemplateLang before and it has not been renamed everywhere yet)","title":"SDF3"},{"location":"references/syntax/#sdf3","text":"SDF3 is the meta-language in Spoofax for syntax definition. A syntax definition is structured as a collection of modules , which may import each other. Symbols are the building blocks of productions . Productions are defined for lexical , context-free , or kernel syntax. Start symbols indicate the entry point of a syntax definition. SDF3 automatically generates a pretty-printer for template -based productions. Grammars can be disambiguated by means of rejects, priorities, associativity, and restrictions. SDF3 provides additional constructs for the definition of layout-sensitivite languages. Permissive grammars are automatically generated for error-recovery parsing. Handwritten recovery rules can be added to tweak recovery behavior.","title":"SDF3"},{"location":"references/syntax/#source","text":"The sources of the SDF3 implementation can be found at https://github.com/metaborg/sdf/tree/master/org.metaborg.meta.lang.template : The SDF3 language implementation (SDF3 was called TemplateLang before and it has not been renamed everywhere yet)","title":"Source"},{"location":"references/syntax/configuration/","text":"Configuration \u00b6 When using SDF3 inside Spoofax, it is possible to specify different configuration options that. They allow using the new parser generator, specifying the shape of completion placeholders, or disable SDF altogether. These options should be specified in the metaborg.yaml file. For example, to disable SDF for the current project, use: language : sdf : enabled : false This configuration should be present when defining language components for a language that has SDF enabled. SDF3 allows generating placeholders for code completion. The default \"shape\" of placeholders is [[Symbol]] . However, it is possible to tweak this shape using the configuration below (the configuration for suffix is optional): language : sdf : placeholder : prefix : \"$\" suffix : \"$\" Currently, the path to the parse table is specified in the :file: Syntax.esv file, commonly as table: target/metaborg/sdf.tbl . When the ESV file does not contain this entry, it is also possible to specify the path to the parse table in the :file: metaborg.yaml file. This is useful when testing an external parse table, or using a parse table different from the one being generated in the project. In the example below, the table is loaded from the path tables/sdf.tbl . The same can be applied to the parse table used for code completion. language : sdf : parse-table : \"tables/sdf.tbl\" completion-parse-table : \"tables/sdf-completions.tbl\" In a Spoofax project, it is also possible to use SDF2 instead of SDF3. This enables SDF2 tools such as the SDF2 parenthesizer, signature generator, etc. For example: language : sdf : version : sdf2 By default SDF3 compilation works by generating SDF2 files, and depending on the SDF2 toolchain. However, a new (and experimental) parse table generator can be selected by writing: language : sdf : sdf2table : java This configuration disables the SDF2 generation, and may cause problems when defining grammars to use concrete syntax, since this feature is not supported yet by SDF3. However, the java parse table generator supports Unicode, whereas SDF2 generation does not. Furthermore, dynamic can be used instead of java , to enable lazy parse table generation, where the parse table is generated while the program is parsed. A namespaced grammar can be generated automatically from an SDF3 grammar. This namespacing is done by adding the language name to all module names and sort names. The generated grammar is put in src-gen/syntax . The configuration to enable this is: language : sdf : generate-namespaced : true Note that namespacing doesn't not handle imports of grammar files from other projects very well. JSGLR version \u00b6 An experimental new version of the SGLR parser implementation is available: JSGLR2. It supports parsing, imploding and syntax highlighting. Error reporting, recovery and completions are currently not supported. It can be enabled with: language : sdf : jsglr-version : v2 There are some extensions of JSGLR2 available. To use them, change the jsglr-version by replacing v2 with one of the following: data-dependent : Data-dependent JSGLR2 solves deep priority conflicts using data-dependent parsing, which does not require duplicating the grammar productions. incremental : Incremental JSGLR2 reuses previous parse results to speed up parsing. layout-sensitive : Layout-sensitive JSGLR2, see Layout Sensitivity . recovery : JSGLR2 with recovery tries to recover from parse errors. This extension is experimental. recovery-incremental : Incremental JSGLR2 with recovery. This extension is experimental. JSGLR2 logging \u00b6 Logging is available for JSGLR2. It can be enabled with: language : sdf : jsglr2-logging : all Since logging all parsing events is quite verbose, several other scopes are available in addition to the all option: none : Log nothing (default). minimal : Only log the start and end of a parse, including a measurement of total parse time (including imploding and tokenization). parsing : Log all standard parsing events (such as stack and parse forest operations, action execution, etc.) but no variant-specific events (e.g. related to recovery). recovery : Log the recovery iterations and the recovery productions that are applied. Todo Whenever changing any of these configurations, clean the project before rebuilding. Todo Write documentation on how to use SDF3 outside of Spoofax","title":"Configuration"},{"location":"references/syntax/configuration/#configuration","text":"When using SDF3 inside Spoofax, it is possible to specify different configuration options that. They allow using the new parser generator, specifying the shape of completion placeholders, or disable SDF altogether. These options should be specified in the metaborg.yaml file. For example, to disable SDF for the current project, use: language : sdf : enabled : false This configuration should be present when defining language components for a language that has SDF enabled. SDF3 allows generating placeholders for code completion. The default \"shape\" of placeholders is [[Symbol]] . However, it is possible to tweak this shape using the configuration below (the configuration for suffix is optional): language : sdf : placeholder : prefix : \"$\" suffix : \"$\" Currently, the path to the parse table is specified in the :file: Syntax.esv file, commonly as table: target/metaborg/sdf.tbl . When the ESV file does not contain this entry, it is also possible to specify the path to the parse table in the :file: metaborg.yaml file. This is useful when testing an external parse table, or using a parse table different from the one being generated in the project. In the example below, the table is loaded from the path tables/sdf.tbl . The same can be applied to the parse table used for code completion. language : sdf : parse-table : \"tables/sdf.tbl\" completion-parse-table : \"tables/sdf-completions.tbl\" In a Spoofax project, it is also possible to use SDF2 instead of SDF3. This enables SDF2 tools such as the SDF2 parenthesizer, signature generator, etc. For example: language : sdf : version : sdf2 By default SDF3 compilation works by generating SDF2 files, and depending on the SDF2 toolchain. However, a new (and experimental) parse table generator can be selected by writing: language : sdf : sdf2table : java This configuration disables the SDF2 generation, and may cause problems when defining grammars to use concrete syntax, since this feature is not supported yet by SDF3. However, the java parse table generator supports Unicode, whereas SDF2 generation does not. Furthermore, dynamic can be used instead of java , to enable lazy parse table generation, where the parse table is generated while the program is parsed. A namespaced grammar can be generated automatically from an SDF3 grammar. This namespacing is done by adding the language name to all module names and sort names. The generated grammar is put in src-gen/syntax . The configuration to enable this is: language : sdf : generate-namespaced : true Note that namespacing doesn't not handle imports of grammar files from other projects very well.","title":"Configuration"},{"location":"references/syntax/configuration/#jsglr-version","text":"An experimental new version of the SGLR parser implementation is available: JSGLR2. It supports parsing, imploding and syntax highlighting. Error reporting, recovery and completions are currently not supported. It can be enabled with: language : sdf : jsglr-version : v2 There are some extensions of JSGLR2 available. To use them, change the jsglr-version by replacing v2 with one of the following: data-dependent : Data-dependent JSGLR2 solves deep priority conflicts using data-dependent parsing, which does not require duplicating the grammar productions. incremental : Incremental JSGLR2 reuses previous parse results to speed up parsing. layout-sensitive : Layout-sensitive JSGLR2, see Layout Sensitivity . recovery : JSGLR2 with recovery tries to recover from parse errors. This extension is experimental. recovery-incremental : Incremental JSGLR2 with recovery. This extension is experimental.","title":"JSGLR version"},{"location":"references/syntax/configuration/#jsglr2-logging","text":"Logging is available for JSGLR2. It can be enabled with: language : sdf : jsglr2-logging : all Since logging all parsing events is quite verbose, several other scopes are available in addition to the all option: none : Log nothing (default). minimal : Only log the start and end of a parse, including a measurement of total parse time (including imploding and tokenization). parsing : Log all standard parsing events (such as stack and parse forest operations, action execution, etc.) but no variant-specific events (e.g. related to recovery). recovery : Log the recovery iterations and the recovery productions that are applied. Todo Whenever changing any of these configurations, clean the project before rebuilding. Todo Write documentation on how to use SDF3 outside of Spoofax","title":"JSGLR2 logging"},{"location":"references/syntax/context-free-syntax/","text":"Context-Free Syntax \u00b6 The context-free syntax describes the more high-level syntactic structure of sentences in a language. A context-free syntax contains a list of productions. Elements of the right-hand side of a context-free production are pre-processed in a normalization step before parser generation that adds the LAYOUT? symbol between any two symbols. Context-free syntax has the form: context-free syntax <Production>* An example production rule: context-free syntax Block.Block = \"{\" Statement* \"}\" SDF3 automatically allows for layout to be present between the symbols of a rule. This means that a fragment such as: { } will still be recognized as a block (assuming that the newline and line-feed characters are defined as layout).","title":"Context-Free Syntax"},{"location":"references/syntax/context-free-syntax/#context-free-syntax","text":"The context-free syntax describes the more high-level syntactic structure of sentences in a language. A context-free syntax contains a list of productions. Elements of the right-hand side of a context-free production are pre-processed in a normalization step before parser generation that adds the LAYOUT? symbol between any two symbols. Context-free syntax has the form: context-free syntax <Production>* An example production rule: context-free syntax Block.Block = \"{\" Statement* \"}\" SDF3 automatically allows for layout to be present between the symbols of a rule. This means that a fragment such as: { } will still be recognized as a block (assuming that the newline and line-feed characters are defined as layout).","title":"Context-Free Syntax"},{"location":"references/syntax/disambiguation/","text":"Disambiguation \u00b6 As we showed before, the semantics of SDF3 can be seen as two-staged. First, the grammar generates all possible derivations. Second, the disambiguation constructs remove a number of derivations that are not valid. Note that SDF3 actually performs some disambiguation when generating the parse table or during parsing. Rejections \u00b6 Rejections filter derivations. The semantics of a rejection is that the set of valid derivations for the left-hand side of the production will not contain the construction described on the right-hand side. In other words, the language defined by the sort on the left-hand side has become smaller, removing all the constructions generated by the rule on the right-hand side. Disambiguation by reject occurs at parse time (mostly). A rule can be marked as rejected by using the attribute {reject} after the rule: <Sort> = ... {reject} The {reject} attribute works well for lexical rejections, especially keyword reservation in the form of productions like: ID = \"keyword\" {reject} Preferences \u00b6 The preferences mechanism is another disambiguation filter that provides a post parse filter to parse forests. The attributes prefer and avoid are the only disambiguation constructs that compare alternative derivations after parsing. Warning prefer and avoid are deprecated and will be removed in a future version of Spoofax. The following definition assumes that derivations are represented using parse forests with \"packaged ambiguity nodes\". This means that whenever in a derivation there is a choice for several sub-derivations, at that point a special choice node (ambiguity constructor) is placed with all alternatives as children. We assume here that the ambiguity constructor is always placed at the location where a choice is needed, and not higher (i.e. a minimal parse forest representation). The preference mechanism compares the top nodes of each alternative: All alternative derivations that have avoid at the top node will be removed, but only if other alternatives derivations are there that do not have avoid at the top node. If there are derivations that have prefer at the top node, all other derivations that do not have prefer at the top node will be removed. The preference attribute can be used to handle the case when two productions can parse the same input. Here is an example:: Exp.FunctionApp = <<Expr> <Expr*>> Exp.Constructor = <<ID> <Expr>> {prefer} Priorities \u00b6 Priorities are one of SDF3's most often used disambiguation constructs. A priority section defines the relative priorities between productions. Priorities are a powerful disambiguation construct because it occurs at parse generation time. The idea behind the semantics of priorities is that productions with a higher priority \"bind stronger\" than productions with a lower priority. The essence of the priority disambiguation construct is that certain parse trees are removed from the \u2018forest\u2019 (the set of all possible parse trees that can be derived from a segment of code). The basic priority syntax looks like this: context-free priorities <ProductionRef> > <ProductionRef> Where <ProductionRef> can either be <Sort>.<Cons> or the entire production itself. Several priorities in a priority grammar are separated by commas. If more productions have the same priority they may be grouped between curly braces on each side of the > sign. context-free priorities {<ProductionRef> <ProductionRef>} > <ProductionRef>, <ProductionRef> > <ProductionRef> By default, the priority relation is automatically transitively closed (i.e. if A > B and B > C then A > C). To specify a non-transitive priority relation it is necessary to include a dot before the > sign ( .> ). SDF3 provides safe disambiguation, meaning that priority relations only remove ambiguous derivations. Furthermore, SDF3 also allows tree filtering by means of indexed priorities such as: context-free priorities <ProductionRef> <idx> > <ProductionRef> where the symbol at position idx (starting with 0) in the first production should not derive the second production. An example defining priorities for the addition, subtraction and multiplication operators is listed below. Because addition and subtraction have the same priority, the are grouped together between brackets. context-free priorities {Exp.Times} > {Exp.Plus Exp.Minus} Associativity \u00b6 Like with priorities, the essence of the associativity attribute is that certain parse trees are removed from the \u2018forest\u2019. The left associativity attribute on a production P filters all occurrences of P as a direct child of P in the right-most argument. This implies that left is only effective on productions that are recursive on the right (as in A B C -> C ). The right associativity attribute on a production P filters all occurrences of P as a direct child of P in the left-most argument. This implies that right is only effective on productions that are recursive on the left ( as in C A B -> C ). The non-assoc associativity attribute on a production P filters all occurrences of P as a direct child of P in any argument. This implement that non-assoc is only effective if a production is indeed recursive (as in A C B -> C ). The assoc attribute means the same as left Associativity declarations occur in two places in SDF3. The first is as production attributes. The second is as associativity declarations in priority groups. An example on how to mention associativity as a production attribute is given below: Exp.Plus = <<Exp> + <Exp>> {left} In priority groups, the associativity has the same semantics as the associativity attributes, except that the filter refers to more nested productions instead of a recursive nesting of one production. The group associativity attribute works pairwise and commutative on all combinations of productions in the group. If there is only one element in the group the attribute is reflexive, otherwise it is not reflexive. context-free priorities {left: Exp.Times} > {left: Exp.Plus Exp.Minus} Restrictions \u00b6 The notion of restrictions enables the formulation of lexical disambiguation strategies. Examples are \"shift before reduce\" and \"longest match\". A restriction filters applications of productions for certain non-terminals if the following character (lookahead) is in a certain class. The result is that specific symbols may not be followed by a character from a given character class. A lookahead may consist of more than one character class (multiple lookahead). Restrictions come in two flavors: lexical restrictions that apply to lexical non-terminals context-free restrictions that apply to context-free non-terminals. The general form of a restriction is: <Symbol>+ -/- <Lookaheads> The semantics of a restriction is to remove all derivations that produce a certain <Symbol> . The condition for this removal is that the derivation tree for that symbol is followed immediately by something that matches the lookahead declaration. Note that to be able to check this condition, one must look past derivations that produce the empty language, until the characters to the right of the filtered symbol are found. Also, for finding multiple lookahead matches, one must ignore nullable sub-trees that may occur in the middle of the matched lookahead. In case of lexical restrictions <Symbol> may be either a literal or sort. In case of context-free restrictions only a sort or symbol is allowed. The restriction operator -/- should be read as may not be followed by. Before the restriction operator -/- a list of symbols is given for which the restriction holds. As an example, the following restriction rule implements the \u201clongest match\u201d policy: an identifier can not be followed by an alpha-numeric character. ID -/- [a-zA-Z0-9\\_]","title":"Disambiguation"},{"location":"references/syntax/disambiguation/#disambiguation","text":"As we showed before, the semantics of SDF3 can be seen as two-staged. First, the grammar generates all possible derivations. Second, the disambiguation constructs remove a number of derivations that are not valid. Note that SDF3 actually performs some disambiguation when generating the parse table or during parsing.","title":"Disambiguation"},{"location":"references/syntax/disambiguation/#rejections","text":"Rejections filter derivations. The semantics of a rejection is that the set of valid derivations for the left-hand side of the production will not contain the construction described on the right-hand side. In other words, the language defined by the sort on the left-hand side has become smaller, removing all the constructions generated by the rule on the right-hand side. Disambiguation by reject occurs at parse time (mostly). A rule can be marked as rejected by using the attribute {reject} after the rule: <Sort> = ... {reject} The {reject} attribute works well for lexical rejections, especially keyword reservation in the form of productions like: ID = \"keyword\" {reject}","title":"Rejections"},{"location":"references/syntax/disambiguation/#preferences","text":"The preferences mechanism is another disambiguation filter that provides a post parse filter to parse forests. The attributes prefer and avoid are the only disambiguation constructs that compare alternative derivations after parsing. Warning prefer and avoid are deprecated and will be removed in a future version of Spoofax. The following definition assumes that derivations are represented using parse forests with \"packaged ambiguity nodes\". This means that whenever in a derivation there is a choice for several sub-derivations, at that point a special choice node (ambiguity constructor) is placed with all alternatives as children. We assume here that the ambiguity constructor is always placed at the location where a choice is needed, and not higher (i.e. a minimal parse forest representation). The preference mechanism compares the top nodes of each alternative: All alternative derivations that have avoid at the top node will be removed, but only if other alternatives derivations are there that do not have avoid at the top node. If there are derivations that have prefer at the top node, all other derivations that do not have prefer at the top node will be removed. The preference attribute can be used to handle the case when two productions can parse the same input. Here is an example:: Exp.FunctionApp = <<Expr> <Expr*>> Exp.Constructor = <<ID> <Expr>> {prefer}","title":"Preferences"},{"location":"references/syntax/disambiguation/#priorities","text":"Priorities are one of SDF3's most often used disambiguation constructs. A priority section defines the relative priorities between productions. Priorities are a powerful disambiguation construct because it occurs at parse generation time. The idea behind the semantics of priorities is that productions with a higher priority \"bind stronger\" than productions with a lower priority. The essence of the priority disambiguation construct is that certain parse trees are removed from the \u2018forest\u2019 (the set of all possible parse trees that can be derived from a segment of code). The basic priority syntax looks like this: context-free priorities <ProductionRef> > <ProductionRef> Where <ProductionRef> can either be <Sort>.<Cons> or the entire production itself. Several priorities in a priority grammar are separated by commas. If more productions have the same priority they may be grouped between curly braces on each side of the > sign. context-free priorities {<ProductionRef> <ProductionRef>} > <ProductionRef>, <ProductionRef> > <ProductionRef> By default, the priority relation is automatically transitively closed (i.e. if A > B and B > C then A > C). To specify a non-transitive priority relation it is necessary to include a dot before the > sign ( .> ). SDF3 provides safe disambiguation, meaning that priority relations only remove ambiguous derivations. Furthermore, SDF3 also allows tree filtering by means of indexed priorities such as: context-free priorities <ProductionRef> <idx> > <ProductionRef> where the symbol at position idx (starting with 0) in the first production should not derive the second production. An example defining priorities for the addition, subtraction and multiplication operators is listed below. Because addition and subtraction have the same priority, the are grouped together between brackets. context-free priorities {Exp.Times} > {Exp.Plus Exp.Minus}","title":"Priorities"},{"location":"references/syntax/disambiguation/#associativity","text":"Like with priorities, the essence of the associativity attribute is that certain parse trees are removed from the \u2018forest\u2019. The left associativity attribute on a production P filters all occurrences of P as a direct child of P in the right-most argument. This implies that left is only effective on productions that are recursive on the right (as in A B C -> C ). The right associativity attribute on a production P filters all occurrences of P as a direct child of P in the left-most argument. This implies that right is only effective on productions that are recursive on the left ( as in C A B -> C ). The non-assoc associativity attribute on a production P filters all occurrences of P as a direct child of P in any argument. This implement that non-assoc is only effective if a production is indeed recursive (as in A C B -> C ). The assoc attribute means the same as left Associativity declarations occur in two places in SDF3. The first is as production attributes. The second is as associativity declarations in priority groups. An example on how to mention associativity as a production attribute is given below: Exp.Plus = <<Exp> + <Exp>> {left} In priority groups, the associativity has the same semantics as the associativity attributes, except that the filter refers to more nested productions instead of a recursive nesting of one production. The group associativity attribute works pairwise and commutative on all combinations of productions in the group. If there is only one element in the group the attribute is reflexive, otherwise it is not reflexive. context-free priorities {left: Exp.Times} > {left: Exp.Plus Exp.Minus}","title":"Associativity"},{"location":"references/syntax/disambiguation/#restrictions","text":"The notion of restrictions enables the formulation of lexical disambiguation strategies. Examples are \"shift before reduce\" and \"longest match\". A restriction filters applications of productions for certain non-terminals if the following character (lookahead) is in a certain class. The result is that specific symbols may not be followed by a character from a given character class. A lookahead may consist of more than one character class (multiple lookahead). Restrictions come in two flavors: lexical restrictions that apply to lexical non-terminals context-free restrictions that apply to context-free non-terminals. The general form of a restriction is: <Symbol>+ -/- <Lookaheads> The semantics of a restriction is to remove all derivations that produce a certain <Symbol> . The condition for this removal is that the derivation tree for that symbol is followed immediately by something that matches the lookahead declaration. Note that to be able to check this condition, one must look past derivations that produce the empty language, until the characters to the right of the filtered symbol are found. Also, for finding multiple lookahead matches, one must ignore nullable sub-trees that may occur in the middle of the matched lookahead. In case of lexical restrictions <Symbol> may be either a literal or sort. In case of context-free restrictions only a sort or symbol is allowed. The restriction operator -/- should be read as may not be followed by. Before the restriction operator -/- a list of symbols is given for which the restriction holds. As an example, the following restriction rule implements the \u201clongest match\u201d policy: an identifier can not be followed by an alpha-numeric character. ID -/- [a-zA-Z0-9\\_]","title":"Restrictions"},{"location":"references/syntax/kernel-syntax/","text":"Kernel Syntax \u00b6 The rules from context-free and lexical syntax are translated into kernel syntax by the SDF3 normalizer. When writing kernel syntax, one has more control over the layout between symbols of a production. As part of normalization, among other things, SDF3 renames each symbol in the lexical syntax to include the suffix -LEX and each symbol in the context-free syntax to include the suffix -CF . For example, the two productions lexical syntax BinaryConst = [0-1]+ context-free syntax Block.Block = \"{\" Statement* \"}\" written in kernel syntax look like syntax Block-CF.Block = \"{\" LAYOUT?-CF Statement*-CF LAYOUT?-CF \"}\" BinaryConst-LEX = [0-1]+ Literals and character classes are lexical by definition, thus they do not need any suffix. Note that each symbol in kernel syntax is uniquely identified by its full name including -CF and -LEX . That is, two symbols named Block-CF and Block are different, if both occur in kernel syntax. However, Block-CF is the same symbol as Block if the latter appears in a context-free syntax section. As mentioned before, layout can only occur in between symbols if explicitly specified. For example, the production syntax Block-CF.Block = \"{\" Statement*-CF LAYOUT?-CF \"}\" does not allow layout to occur in between the opening bracket and the list of statements. This means that a fragment such as: { x = 1; } would not be recognized as a block.","title":"Kernel Syntax"},{"location":"references/syntax/kernel-syntax/#kernel-syntax","text":"The rules from context-free and lexical syntax are translated into kernel syntax by the SDF3 normalizer. When writing kernel syntax, one has more control over the layout between symbols of a production. As part of normalization, among other things, SDF3 renames each symbol in the lexical syntax to include the suffix -LEX and each symbol in the context-free syntax to include the suffix -CF . For example, the two productions lexical syntax BinaryConst = [0-1]+ context-free syntax Block.Block = \"{\" Statement* \"}\" written in kernel syntax look like syntax Block-CF.Block = \"{\" LAYOUT?-CF Statement*-CF LAYOUT?-CF \"}\" BinaryConst-LEX = [0-1]+ Literals and character classes are lexical by definition, thus they do not need any suffix. Note that each symbol in kernel syntax is uniquely identified by its full name including -CF and -LEX . That is, two symbols named Block-CF and Block are different, if both occur in kernel syntax. However, Block-CF is the same symbol as Block if the latter appears in a context-free syntax section. As mentioned before, layout can only occur in between symbols if explicitly specified. For example, the production syntax Block-CF.Block = \"{\" Statement*-CF LAYOUT?-CF \"}\" does not allow layout to occur in between the opening bracket and the list of statements. This means that a fragment such as: { x = 1; } would not be recognized as a block.","title":"Kernel Syntax"},{"location":"references/syntax/layout-sensitivity/","text":"Layout Sensitivity \u00b6 SDF3 supports definition of layout sensitive syntax by means of low-level layout constraints and high-level layout declarations . Note If you want to use layout constraints or layout declarations, you should specify the jsglr-version: layout-sensitive parameter for SDF3, see configuration . Layout Constraints \u00b6 While we haven't covered layout constraints in this documentation, the paper [^1] describes the concepts. Layout Declarations \u00b6 In the paper [^1], the authors describe layout constraints in terms of restrictions involving the position of the subtree involved in the constraint ( 0 , 1 , ...), token selectors ( first , left , last and right ), and position selectors as lines and columns ( line and col ). This mechanism allows writing layout constraints to express alignment, offside and indentation rules, but writing such constraints is rather cumbersome and error prone. Alternatively, one may write layout constraints using layout declarations , which are more declarative specifications and abstract over lines, columns and token selectors as the original layout constraints from [^1]. Tree selectors \u00b6 To specify which trees should be subject to a layout constraint, one may use: tree positions, SDF3 labeled non-terminals, or unique literals that occurs in the production. For example: context-free syntax Stmt.IfElse = \"if\" Exp \"then\" Stmts \"else\" else:Stmts {layout( indent \"if\" 3, else && align 3 else && align \"if\" \"else\" )} In the layout constraint for the production above, else refers to the tree for the labeled non-terminal else:Stmts , \"if\" refers to the tree corresponding to the \"if\" literal and the number 3 correspond to the tree at position 3 in the parse tree (starting at 0, ignoring trees for LAYOUT? ). align \u00b6 The layout constraint layout(align x y1, ..., yn) specifies that the trees indicated by the tree selectors yi should be aligned with the tree indicated by the tree selector x , i.e., all these trees should start in the same column. For example, if we consider the production above, the following program is correct according to the align constraints: if x < 0 then \u00b7\u00b7 x = 0 else \u00b7\u00b7 y = 1 Whereas, the following program is incorrect because neither the if and else keyword align ( align \"if\" \"else\" ), nor the statements in the branches ( align 3 else ): if x < 0 then \u00b7\u00b7 x = 0 \u00b7 else \u00b7\u00b7\u00b7 y = 1 align-list \u00b6 The constraint align-list can be used to indicate that all subtrees within a list should be aligned. That is, a constraint layout(align-list x) , where x is a tree selector for a list subtree, can be used to enforce such constraint. For example, consider the following production and its layout constraint: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( align-list then )} This constraint indicates that statements inside the list should be aligned. Therefore, the following program is correct according to this constraint: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2 And the following program is invalid, as the second statement is misaligned: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2 offside \u00b6 The offside rule is very common in layout-sensitive languages. It states that all lines after the first one should be further to the right compared to the first line. For a description of how the offside rule can be modelled with layout constraints, refer to :cite: s-ErdwegRKO12 . An example of a declarative specification of the offside rule can be seen in the production below: context-free syntax Stmt.Assign = <<ID> = <Exp>> {layout(offside 3)} The layout constraint specifies that when the expression in the statement spams multiple lines, all following lines should be indented with respect to the column where the expression started. For example, the following program is valid according to this constraint: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7\u00b7 + 2 However, the following program is not valid, as the second line of the expression starts at the same column as the first line: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7 + 2 Note that if the expression is written on a single line, the constraint is also verified. That is, the following program successfully parses: x = 4 * 10 + 2 It is also possible to use the offside relation on different trees. For example, consider the constraint in the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( offside \"if\" then )} This constraint states that all lines (except the first) of the statements in the then branch should be indented with respect to the if literal. Thus, the following program is invalid according to this layout constraint, because the statement x = 2 should be indented with relation to the topmost if . if x < 0 then \u00b7\u00b7 if y < 0 then x = 2 In general, an offside constraint involving more than a single tree is combined with indent constraint to enforce that the column of the first and all subsequent lines should be indented. indent \u00b6 An indent constraint indicates that the column of the first line of a certain tree should be further to the right with respect to another tree. For example, consider the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then )} This constraint indicates that the first line of the list of statements should be indented with respect to the if literal. Thus, according to this constraint the following program is valid: if x < 0 then \u00b7\u00b7 x = 2 Note that if the list of statements in the then branch spams multiple lines, the constraint does not apply to its subsequent lines. For example, consider the following program: if x < 0 then \u00b7\u00b7 x = 2 + 10 * 4 y = 3 This program is still valid, since the column of the first line of the first assignment is indented with respect to the if literal. To indicate that the first and all subsequent lines should be indented, an offside constraint should also be included. context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then && offside \"if\" then )} With this constraint, the remainder of the expression * 4 should also be further to the right compared to the \"if\" literal. The following program is correct according to these two constraints, since the second line of the first assignment and the second assignment are also indented with respect to the if literal: if x < 0 then \u00b7\u00b7 x = 2 + 10 \u00b7 * 4 \u00b7 y = 3 Finally, all these layout declarations can be ignored by the parser and used only when generating the pretty-printer. To do that, prefix the constraint with pp- writing, for example, pp-offside or pp-align .","title":"Layout Sensitivity"},{"location":"references/syntax/layout-sensitivity/#layout-sensitivity","text":"SDF3 supports definition of layout sensitive syntax by means of low-level layout constraints and high-level layout declarations . Note If you want to use layout constraints or layout declarations, you should specify the jsglr-version: layout-sensitive parameter for SDF3, see configuration .","title":"Layout Sensitivity"},{"location":"references/syntax/layout-sensitivity/#layout-constraints","text":"While we haven't covered layout constraints in this documentation, the paper [^1] describes the concepts.","title":"Layout Constraints"},{"location":"references/syntax/layout-sensitivity/#layout-declarations","text":"In the paper [^1], the authors describe layout constraints in terms of restrictions involving the position of the subtree involved in the constraint ( 0 , 1 , ...), token selectors ( first , left , last and right ), and position selectors as lines and columns ( line and col ). This mechanism allows writing layout constraints to express alignment, offside and indentation rules, but writing such constraints is rather cumbersome and error prone. Alternatively, one may write layout constraints using layout declarations , which are more declarative specifications and abstract over lines, columns and token selectors as the original layout constraints from [^1].","title":"Layout Declarations"},{"location":"references/syntax/layout-sensitivity/#tree-selectors","text":"To specify which trees should be subject to a layout constraint, one may use: tree positions, SDF3 labeled non-terminals, or unique literals that occurs in the production. For example: context-free syntax Stmt.IfElse = \"if\" Exp \"then\" Stmts \"else\" else:Stmts {layout( indent \"if\" 3, else && align 3 else && align \"if\" \"else\" )} In the layout constraint for the production above, else refers to the tree for the labeled non-terminal else:Stmts , \"if\" refers to the tree corresponding to the \"if\" literal and the number 3 correspond to the tree at position 3 in the parse tree (starting at 0, ignoring trees for LAYOUT? ).","title":"Tree selectors"},{"location":"references/syntax/layout-sensitivity/#align","text":"The layout constraint layout(align x y1, ..., yn) specifies that the trees indicated by the tree selectors yi should be aligned with the tree indicated by the tree selector x , i.e., all these trees should start in the same column. For example, if we consider the production above, the following program is correct according to the align constraints: if x < 0 then \u00b7\u00b7 x = 0 else \u00b7\u00b7 y = 1 Whereas, the following program is incorrect because neither the if and else keyword align ( align \"if\" \"else\" ), nor the statements in the branches ( align 3 else ): if x < 0 then \u00b7\u00b7 x = 0 \u00b7 else \u00b7\u00b7\u00b7 y = 1","title":"align"},{"location":"references/syntax/layout-sensitivity/#align-list","text":"The constraint align-list can be used to indicate that all subtrees within a list should be aligned. That is, a constraint layout(align-list x) , where x is a tree selector for a list subtree, can be used to enforce such constraint. For example, consider the following production and its layout constraint: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( align-list then )} This constraint indicates that statements inside the list should be aligned. Therefore, the following program is correct according to this constraint: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2 And the following program is invalid, as the second statement is misaligned: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2","title":"align-list"},{"location":"references/syntax/layout-sensitivity/#offside","text":"The offside rule is very common in layout-sensitive languages. It states that all lines after the first one should be further to the right compared to the first line. For a description of how the offside rule can be modelled with layout constraints, refer to :cite: s-ErdwegRKO12 . An example of a declarative specification of the offside rule can be seen in the production below: context-free syntax Stmt.Assign = <<ID> = <Exp>> {layout(offside 3)} The layout constraint specifies that when the expression in the statement spams multiple lines, all following lines should be indented with respect to the column where the expression started. For example, the following program is valid according to this constraint: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7\u00b7 + 2 However, the following program is not valid, as the second line of the expression starts at the same column as the first line: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7 + 2 Note that if the expression is written on a single line, the constraint is also verified. That is, the following program successfully parses: x = 4 * 10 + 2 It is also possible to use the offside relation on different trees. For example, consider the constraint in the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( offside \"if\" then )} This constraint states that all lines (except the first) of the statements in the then branch should be indented with respect to the if literal. Thus, the following program is invalid according to this layout constraint, because the statement x = 2 should be indented with relation to the topmost if . if x < 0 then \u00b7\u00b7 if y < 0 then x = 2 In general, an offside constraint involving more than a single tree is combined with indent constraint to enforce that the column of the first and all subsequent lines should be indented.","title":"offside"},{"location":"references/syntax/layout-sensitivity/#indent","text":"An indent constraint indicates that the column of the first line of a certain tree should be further to the right with respect to another tree. For example, consider the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then )} This constraint indicates that the first line of the list of statements should be indented with respect to the if literal. Thus, according to this constraint the following program is valid: if x < 0 then \u00b7\u00b7 x = 2 Note that if the list of statements in the then branch spams multiple lines, the constraint does not apply to its subsequent lines. For example, consider the following program: if x < 0 then \u00b7\u00b7 x = 2 + 10 * 4 y = 3 This program is still valid, since the column of the first line of the first assignment is indented with respect to the if literal. To indicate that the first and all subsequent lines should be indented, an offside constraint should also be included. context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then && offside \"if\" then )} With this constraint, the remainder of the expression * 4 should also be further to the right compared to the \"if\" literal. The following program is correct according to these two constraints, since the second line of the first assignment and the second assignment are also indented with respect to the if literal: if x < 0 then \u00b7\u00b7 x = 2 + 10 \u00b7 * 4 \u00b7 y = 3 Finally, all these layout declarations can be ignored by the parser and used only when generating the pretty-printer. To do that, prefix the constraint with pp- writing, for example, pp-offside or pp-align .","title":"indent"},{"location":"references/syntax/lexical-syntax/","text":"Lexical Syntax \u00b6 The lexical syntax usually describes the low level structure of programs (often referred to as lexical tokens). However, in SDF3, the token concept is not really relevant, since only character classes are terminals. The lexical syntax sections in SDF3 are simply a convenient notation for the low level syntax of a language. The LAYOUT symbol should also be defined in a lexical syntax section. A lexical syntax consists of a list of productions. Lexical syntax is described as follows:: lexical syntax <Production>* An example of a production in lexical syntax: lexical syntax BinaryConst = [0-1]+","title":"Lexical Syntax"},{"location":"references/syntax/lexical-syntax/#lexical-syntax","text":"The lexical syntax usually describes the low level structure of programs (often referred to as lexical tokens). However, in SDF3, the token concept is not really relevant, since only character classes are terminals. The lexical syntax sections in SDF3 are simply a convenient notation for the low level syntax of a language. The LAYOUT symbol should also be defined in a lexical syntax section. A lexical syntax consists of a list of productions. Lexical syntax is described as follows:: lexical syntax <Production>* An example of a production in lexical syntax: lexical syntax BinaryConst = [0-1]+","title":"Lexical Syntax"},{"location":"references/syntax/modules/","text":"Modules \u00b6 An SDF3 specification consists of a number of module declarations. Each module defines sections and may import other modules. Imports \u00b6 Modules may import other modules for reuse or separation of concerns. A module may extend the definition of a non-terminal in another module. A module may compose the definition of a language by importing the parts of the language. The structure of a module is as follows: module <ModuleName> <ImportSection>* <Section>* The module keyword is followed by the module name, then a series of imports can be made, followed by sections that contain the actual definition of the syntax. An import section is structured as follows: imports <ModuleName>* Note that SDF3 does not support parameterized modules. Sections \u00b6 A SDF3 module may constitute of zero or more sections. All sections contribute to the final grammar that defines a language: sorts , lexical sorts , context-free sorts (see Sorts ) lexical syntax (see Lexical Syntax ) context-free syntax (see Context-Free Syntax ) syntax (see Kernel Syntax ) lexical start-symbols , context-free start-symbols , start-symbols (see Start Symbols ) context-free priorities , priorities (see Disambiguation ) template options (see Templates )","title":"Modules"},{"location":"references/syntax/modules/#modules","text":"An SDF3 specification consists of a number of module declarations. Each module defines sections and may import other modules.","title":"Modules"},{"location":"references/syntax/modules/#imports","text":"Modules may import other modules for reuse or separation of concerns. A module may extend the definition of a non-terminal in another module. A module may compose the definition of a language by importing the parts of the language. The structure of a module is as follows: module <ModuleName> <ImportSection>* <Section>* The module keyword is followed by the module name, then a series of imports can be made, followed by sections that contain the actual definition of the syntax. An import section is structured as follows: imports <ModuleName>* Note that SDF3 does not support parameterized modules.","title":"Imports"},{"location":"references/syntax/modules/#sections","text":"A SDF3 module may constitute of zero or more sections. All sections contribute to the final grammar that defines a language: sorts , lexical sorts , context-free sorts (see Sorts ) lexical syntax (see Lexical Syntax ) context-free syntax (see Context-Free Syntax ) syntax (see Kernel Syntax ) lexical start-symbols , context-free start-symbols , start-symbols (see Start Symbols ) context-free priorities , priorities (see Disambiguation ) template options (see Templates )","title":"Sections"},{"location":"references/syntax/productions/","text":"Productions \u00b6 The basic building block of syntax sections is the production. The left-hand side of a regular production rule can be either just a symbol or a symbol followed by . and a constructor name. The right-hand side consists of zero or more symbols. Both sides are separated by = : <Symbol> = <Symbol>* <Symbol>.<Constructor> = <Symbol>* A production is read as the definition. The symbol on the left-hand side is defined by the right-hand side of the production. Productions are used to describe lexical as well as context-free syntax. Productions may also occur in priority sections, but might also be referred to by its <Symbol>.<Constructor> . All productions with the same symbol together define the alternatives for that symbol. Attributes \u00b6 The definition of lexical and context-free productions may be followed by attributes that define additional (syntactic or semantic) properties of that production. The attributes are written between curly brackets after the right-hand side of a production. If a production has more than one attribute they are separated by commas. Attributes have thus the following form: <Sort> = <Symbol>* { <Attribute1>, <Attribute2>, ...} <Sort>.<Constructor> = <Symbol>* { <Attribute1>, <Attribute2>, ...} The following syntax-related attributes exist: bracket is an important attribute in combination with priorities. For example, the sdf2parenthesize tool uses the bracket attribute to find productions to add to a parse tree before pretty printing (when the tree violates priority constraints). Note that most of these tools demand the production with a bracket attribute to have the shape: X = \"(\" X \")\" {bracket} with any kind of bracket syntax but the X being the same symbol on the left-hand side and the right-hand side. The connection with priorities and associativity is that when a non-terminal is disambiguated using either of them, a production rule with the bracket attribute is probably also needed. left , right , non-assoc , assoc are disambiguation constructs used to define the associativity of productions. See Disambiguation . prefer and avoid are deprecated disambiguation constructs to define preference of one derivation over others. See Disambiguation . reject is a disambiguation construct that implements language difference. It is used for keyword reservation. See Disambiguation .","title":"Productions"},{"location":"references/syntax/productions/#productions","text":"The basic building block of syntax sections is the production. The left-hand side of a regular production rule can be either just a symbol or a symbol followed by . and a constructor name. The right-hand side consists of zero or more symbols. Both sides are separated by = : <Symbol> = <Symbol>* <Symbol>.<Constructor> = <Symbol>* A production is read as the definition. The symbol on the left-hand side is defined by the right-hand side of the production. Productions are used to describe lexical as well as context-free syntax. Productions may also occur in priority sections, but might also be referred to by its <Symbol>.<Constructor> . All productions with the same symbol together define the alternatives for that symbol.","title":"Productions"},{"location":"references/syntax/productions/#attributes","text":"The definition of lexical and context-free productions may be followed by attributes that define additional (syntactic or semantic) properties of that production. The attributes are written between curly brackets after the right-hand side of a production. If a production has more than one attribute they are separated by commas. Attributes have thus the following form: <Sort> = <Symbol>* { <Attribute1>, <Attribute2>, ...} <Sort>.<Constructor> = <Symbol>* { <Attribute1>, <Attribute2>, ...} The following syntax-related attributes exist: bracket is an important attribute in combination with priorities. For example, the sdf2parenthesize tool uses the bracket attribute to find productions to add to a parse tree before pretty printing (when the tree violates priority constraints). Note that most of these tools demand the production with a bracket attribute to have the shape: X = \"(\" X \")\" {bracket} with any kind of bracket syntax but the X being the same symbol on the left-hand side and the right-hand side. The connection with priorities and associativity is that when a non-terminal is disambiguated using either of them, a production rule with the bracket attribute is probably also needed. left , right , non-assoc , assoc are disambiguation constructs used to define the associativity of productions. See Disambiguation . prefer and avoid are deprecated disambiguation constructs to define preference of one derivation over others. See Disambiguation . reject is a disambiguation construct that implements language difference. It is used for keyword reservation. See Disambiguation .","title":"Attributes"},{"location":"references/syntax/recovery/","text":"Recovery \u00b6 SDF3 automatically generates permissive grammars for supporting error-recovery parsing. Handwritten recovery rules can be added to tweak the permissive grammar.","title":"Recovery"},{"location":"references/syntax/recovery/#recovery","text":"SDF3 automatically generates permissive grammars for supporting error-recovery parsing. Handwritten recovery rules can be added to tweak the permissive grammar.","title":"Recovery"},{"location":"references/syntax/start-symbols/","text":"Start Symbols \u00b6 The lexical or context-free start symbols sections explicitly define the symbols which will serve as start symbols when parsing terms. If no start symbols are defined it is not possible to recognize terms. This has the effect that input sentences corresponding to these symbols can be parsed. So, if we want to recognize boolean terms we have to define explicitly the sort Boolean as a start symbol in the module Booleans . Any symbol and also lists, optionals, etc., can serve as a start-symbol. A definition of lexical start symbols looks like: lexical start-symbols <Symbol>* While context-free start symbols are defined as: context-free start-symbols <Symbol>* SDF3 also supports kernel start-symbols: start-symbols <Symbol>* In contrast to lexical and kernel start-symbols, context-free start symbols can be surrounded by optional layout. A lexical start-symbol should have been defined by a production in the lexical syntax; a context-free symbol should have been defined in the context-free syntax. Both symbols can also be defined in kernel syntax using the suffix -LEX or -CF .","title":"Start Symbols"},{"location":"references/syntax/start-symbols/#start-symbols","text":"The lexical or context-free start symbols sections explicitly define the symbols which will serve as start symbols when parsing terms. If no start symbols are defined it is not possible to recognize terms. This has the effect that input sentences corresponding to these symbols can be parsed. So, if we want to recognize boolean terms we have to define explicitly the sort Boolean as a start symbol in the module Booleans . Any symbol and also lists, optionals, etc., can serve as a start-symbol. A definition of lexical start symbols looks like: lexical start-symbols <Symbol>* While context-free start symbols are defined as: context-free start-symbols <Symbol>* SDF3 also supports kernel start-symbols: start-symbols <Symbol>* In contrast to lexical and kernel start-symbols, context-free start symbols can be surrounded by optional layout. A lexical start-symbol should have been defined by a production in the lexical syntax; a context-free symbol should have been defined in the context-free syntax. Both symbols can also be defined in kernel syntax using the suffix -LEX or -CF .","title":"Start Symbols"},{"location":"references/syntax/symbols/","text":"Symbols \u00b6 The building block of SDF3 productions is a symbol. SDF3 symbols can be compared to terminals and non-terminals in other grammar formalisms. The elementary symbols are character classes, literals, and sorts. Intrinsically, only character classes are real terminal symbols. All other symbols represent non-terminals. SDF3 also support symbols that capture BNF-like notation such as lists, optionals, alternatives, and sequences. Note that these symbols are also non-terminals, and are just shorthands for common structures present in context-free grammars. Character classes \u00b6 Character classes occur only in lexical syntax and are enclosed by [ and ] . A character class consists of a list of zero or more characters (which stand for themselves) such as [x] to represent the character x , or character ranges, as an abbreviation for all the characters in the range such as [0-9] representing 0 , 1 , ..., 9 . A valid range consists of [c1-c2] , where the character c2 has a higher ASCII code than c1 . Note that nested character classes can also be concatenated within the same character class symbol, for example [c1c2-c3c4-c5] includes the characters c1 and the ranges c2-c3 , c4-c5 . In this case, the nested character classes do not need to be ordered, as SDF3 orders them when performing a normalization step. Escaped Characters : SDF3 uses a backslash ( \\ ) as a escape for the quoting of special characters. One should use \\c whenever c is not a digit or a letter in a character class. Arbitrary Unicode code points can be included in a character class by writing an escaped integer, which is particularly useful for representing characters outside the printable ASCII range. The integer can be a binary, octal, decimal, or hexadecimal number, for example: \\0b101010 , \\052 , \\42 , and \\0x2A all represent the code point 42, or the '*' character. Additionally, special ASCII characters are represented by: \\t : horizontal tabulation \\n : newline character \\v : vertical tabulation \\f : form feed \\r : carriage return Character Class Operators : SDF3 provides the following operators for character classes: (complement) ~ : Accepts all the characters that are not in the original class. (difference) / : Accepts all the characters in the first class unless they are in a second class. (union) \\/ : Accepts all the characters in either character classes. (intersection) /\\ : Accepts all the characters that are accepted by both character classes. Note that the first operator is unary and the other ones are left associative binary operators. Furthermore, such operators are not applicable to other symbols in general. Literals \u00b6 A literal symbol defines a fixed length word. This usually corresponds to a terminal symbol in ordinary context-free grammars, for example \"true\" or \"+\" . Literals must always be quoted and consist of (possibly escaped) ASCII characters. As literals are also regular non-terminals, SDF3 automatically generates productions for them in terms of terminal symbols. \"definition\" = [d][e][f][i][n][i][t][i][o][n] Note that the production above defines a case-sensitive implementation of the defined literal. Case-insensitive literals are defined using single-quoted strings as in 'true' or 'else' . SDF3 generates a different production for case-insensitive literals as 'definition' = [dD][eE][fF][iI][nN][iI][tT][iI][oO][nN] The literal above accepts case-insensitive inputs such as definition , DEFINITION , DeFiNiTiOn or defINITION . Sorts \u00b6 A sort corresponds to a plain non-terminal, e.g. Statement or Exp . Sort names start with a capital letter and may be followed by letters, digits, hyphens, or underscores. Note that unlike SDF2, SDF3 does not support parameterized sorts (yet!). Sorts are declared by listing their name in the appropriate sorts section, which have the following forms. For context-free sorts: context-free sorts <Sort>* For lexical sorts: lexical sorts <Sort>* SDF3 also supports kernel sorts: sorts <Sort>* Note Kernel sorts should be suffixed with -CF or -LEX , depending on whether they are context-free sorts or lexical sorts. When a sort in a sorts block does not have a suffix, it is treated as a context-free sort. Writing a sort in these sections only indicates that a sort has been declared, even if it does not have any explicit production visible. Optionals \u00b6 SDF3 provides a shorthand for describing zero or exactly one occurrence of a sort by appending the sort with ? . For example, the sort Extends? can be parsed as Extends or without consuming any input. Internally, SDF3 generates the following productions after normalizing the grammar:: Extends?.None = Extends?.Some = Extends Note that using ? adds the constructors None and Some to the final abstract syntax tree. Lists \u00b6 Lists symbols as the name says, indicate that a symbol should occur several times. In this way, it is also possible to construct flat structures to represent them. SDF3 provides support for two types of lists, with and without separators. Furthermore, it is also possible to indicate whether a list can be empty ( * ) or should have at least one element ( + ). For example, a list Statement* indicates zero or more Statement , whereas a list with separator {ID \",\"}+ indicates one or more ID separated by , . Note that SDF3 only supports literal symbols as separators. Again, SDF3 generates the following productions to represent lists, when normalizing the grammar. Statement* = Statement* = Statement+ Statement+ = Statement+ Statement Statement+ = Statement {ID \",\"}* = {ID \",\"}* = {ID \",\"}+ {ID \",\"}+ = {ID \",\"}+ \",\" {ID \",\"} {ID \",\"}+ = {ID \",\"} When parsing a context-free list, SDF3 produces a flattened list as an AST node such as [Statement, ..., Statement] or [ID, ..., ID] . Note that because the separator is a literal, it does not appear in the AST. Alternative \u00b6 Alternative symbols express the choice between two symbols, for example, ID | INT . That is, the symbol ID | INT can be parsed as either ID or INT . For that reason, SDF3 normalizes alternatives by generating the following productions: ID | INT = ID ID | INT = INT Note that SDF3 only allow alternative symbols to occur in lexical syntax. Furthermore, note that the alternative operator is right associative and binds stronger than any operator. That is, ID \",\" | ID \";\" expresses ID (\",\" | ID) \";\" . To express (ID \",\") | (ID \";\") , we can use a sequence symbol. Sequence \u00b6 A sequence operator allows grouping of two or more symbols. Sequences are useful when combined with other symbols such, lists or optionals, for example (\"e\" [0-9]+)? . Like alternative symbols, sequences can only occur in lexical syntax. A sequence symbol is normalized as (\"e\" [0-9]+) = \"e\" [0-9]+ Labeled symbols \u00b6 SDF3 supports decorating symbols with labels, such as myList:{elem:Stmt \";\"}* . The labels have no semantics but can be used by other tools that use SDF3 grammars as input. LAYOUT \u00b6 The LAYOUT symbol is a reserved sort name. It is used to indicate the whitespace that can appear in between context-free symbols. The user must define the symbol LAYOUT such as: LAYOUT = [\\ \\t\\n] Note that the production above should be defined in the lexical syntax.","title":"Symbols"},{"location":"references/syntax/symbols/#symbols","text":"The building block of SDF3 productions is a symbol. SDF3 symbols can be compared to terminals and non-terminals in other grammar formalisms. The elementary symbols are character classes, literals, and sorts. Intrinsically, only character classes are real terminal symbols. All other symbols represent non-terminals. SDF3 also support symbols that capture BNF-like notation such as lists, optionals, alternatives, and sequences. Note that these symbols are also non-terminals, and are just shorthands for common structures present in context-free grammars.","title":"Symbols"},{"location":"references/syntax/symbols/#character-classes","text":"Character classes occur only in lexical syntax and are enclosed by [ and ] . A character class consists of a list of zero or more characters (which stand for themselves) such as [x] to represent the character x , or character ranges, as an abbreviation for all the characters in the range such as [0-9] representing 0 , 1 , ..., 9 . A valid range consists of [c1-c2] , where the character c2 has a higher ASCII code than c1 . Note that nested character classes can also be concatenated within the same character class symbol, for example [c1c2-c3c4-c5] includes the characters c1 and the ranges c2-c3 , c4-c5 . In this case, the nested character classes do not need to be ordered, as SDF3 orders them when performing a normalization step. Escaped Characters : SDF3 uses a backslash ( \\ ) as a escape for the quoting of special characters. One should use \\c whenever c is not a digit or a letter in a character class. Arbitrary Unicode code points can be included in a character class by writing an escaped integer, which is particularly useful for representing characters outside the printable ASCII range. The integer can be a binary, octal, decimal, or hexadecimal number, for example: \\0b101010 , \\052 , \\42 , and \\0x2A all represent the code point 42, or the '*' character. Additionally, special ASCII characters are represented by: \\t : horizontal tabulation \\n : newline character \\v : vertical tabulation \\f : form feed \\r : carriage return Character Class Operators : SDF3 provides the following operators for character classes: (complement) ~ : Accepts all the characters that are not in the original class. (difference) / : Accepts all the characters in the first class unless they are in a second class. (union) \\/ : Accepts all the characters in either character classes. (intersection) /\\ : Accepts all the characters that are accepted by both character classes. Note that the first operator is unary and the other ones are left associative binary operators. Furthermore, such operators are not applicable to other symbols in general.","title":"Character classes"},{"location":"references/syntax/symbols/#literals","text":"A literal symbol defines a fixed length word. This usually corresponds to a terminal symbol in ordinary context-free grammars, for example \"true\" or \"+\" . Literals must always be quoted and consist of (possibly escaped) ASCII characters. As literals are also regular non-terminals, SDF3 automatically generates productions for them in terms of terminal symbols. \"definition\" = [d][e][f][i][n][i][t][i][o][n] Note that the production above defines a case-sensitive implementation of the defined literal. Case-insensitive literals are defined using single-quoted strings as in 'true' or 'else' . SDF3 generates a different production for case-insensitive literals as 'definition' = [dD][eE][fF][iI][nN][iI][tT][iI][oO][nN] The literal above accepts case-insensitive inputs such as definition , DEFINITION , DeFiNiTiOn or defINITION .","title":"Literals"},{"location":"references/syntax/symbols/#sorts","text":"A sort corresponds to a plain non-terminal, e.g. Statement or Exp . Sort names start with a capital letter and may be followed by letters, digits, hyphens, or underscores. Note that unlike SDF2, SDF3 does not support parameterized sorts (yet!). Sorts are declared by listing their name in the appropriate sorts section, which have the following forms. For context-free sorts: context-free sorts <Sort>* For lexical sorts: lexical sorts <Sort>* SDF3 also supports kernel sorts: sorts <Sort>* Note Kernel sorts should be suffixed with -CF or -LEX , depending on whether they are context-free sorts or lexical sorts. When a sort in a sorts block does not have a suffix, it is treated as a context-free sort. Writing a sort in these sections only indicates that a sort has been declared, even if it does not have any explicit production visible.","title":"Sorts"},{"location":"references/syntax/symbols/#optionals","text":"SDF3 provides a shorthand for describing zero or exactly one occurrence of a sort by appending the sort with ? . For example, the sort Extends? can be parsed as Extends or without consuming any input. Internally, SDF3 generates the following productions after normalizing the grammar:: Extends?.None = Extends?.Some = Extends Note that using ? adds the constructors None and Some to the final abstract syntax tree.","title":"Optionals"},{"location":"references/syntax/symbols/#lists","text":"Lists symbols as the name says, indicate that a symbol should occur several times. In this way, it is also possible to construct flat structures to represent them. SDF3 provides support for two types of lists, with and without separators. Furthermore, it is also possible to indicate whether a list can be empty ( * ) or should have at least one element ( + ). For example, a list Statement* indicates zero or more Statement , whereas a list with separator {ID \",\"}+ indicates one or more ID separated by , . Note that SDF3 only supports literal symbols as separators. Again, SDF3 generates the following productions to represent lists, when normalizing the grammar. Statement* = Statement* = Statement+ Statement+ = Statement+ Statement Statement+ = Statement {ID \",\"}* = {ID \",\"}* = {ID \",\"}+ {ID \",\"}+ = {ID \",\"}+ \",\" {ID \",\"} {ID \",\"}+ = {ID \",\"} When parsing a context-free list, SDF3 produces a flattened list as an AST node such as [Statement, ..., Statement] or [ID, ..., ID] . Note that because the separator is a literal, it does not appear in the AST.","title":"Lists"},{"location":"references/syntax/symbols/#alternative","text":"Alternative symbols express the choice between two symbols, for example, ID | INT . That is, the symbol ID | INT can be parsed as either ID or INT . For that reason, SDF3 normalizes alternatives by generating the following productions: ID | INT = ID ID | INT = INT Note that SDF3 only allow alternative symbols to occur in lexical syntax. Furthermore, note that the alternative operator is right associative and binds stronger than any operator. That is, ID \",\" | ID \";\" expresses ID (\",\" | ID) \";\" . To express (ID \",\") | (ID \";\") , we can use a sequence symbol.","title":"Alternative"},{"location":"references/syntax/symbols/#sequence","text":"A sequence operator allows grouping of two or more symbols. Sequences are useful when combined with other symbols such, lists or optionals, for example (\"e\" [0-9]+)? . Like alternative symbols, sequences can only occur in lexical syntax. A sequence symbol is normalized as (\"e\" [0-9]+) = \"e\" [0-9]+","title":"Sequence"},{"location":"references/syntax/symbols/#labeled-symbols","text":"SDF3 supports decorating symbols with labels, such as myList:{elem:Stmt \";\"}* . The labels have no semantics but can be used by other tools that use SDF3 grammars as input.","title":"Labeled symbols"},{"location":"references/syntax/symbols/#layout","text":"The LAYOUT symbol is a reserved sort name. It is used to indicate the whitespace that can appear in between context-free symbols. The user must define the symbol LAYOUT such as: LAYOUT = [\\ \\t\\n] Note that the production above should be defined in the lexical syntax.","title":"LAYOUT"},{"location":"references/syntax/templates/","text":"Templates \u00b6 Templates are a major change in SDF3 when comparing to SDF2. They are essential when aiming to generate a nice pretty printer or generate proper syntactic code completion templates. When generating such artifacts, a general production simply introduces a whitespace in between symbols. For example, when writing a grammar rule Statement.If = \"if\" \"(\" Exp \")\" Exp \"else\" Exp and pretty printing a valid program, we would get the text in a single line separated by spaces, as: Furthermore, code completion would consider the same indentation when inserting code snippets. However, when using template productions such as Statement.If = < if (<Exp>) <Exp> else <Exp>> We would get the following program: Again, code completion would also consider this indentation for proposals. That is, in template productions, the surrounding layout is used to nicely pretty print programs and its code completion suggestions. Template Productions \u00b6 Template productions are an alternative way of defining productions. Similarly, they consist of a left-hand side and a right-hand side separated by = . The left-hand side is the same as for productive rules. The right-hand side is a template delimited by < and > . The template can contain zero or more symbols: <Sort> = < <Symbol>* > <Sort>.<Constructor> = < <Symbol>* > Alternatively, square brackets can be used to delimit a template: <Sort> = [ <Symbol>* ] <Sort>.<Constructor> = [ <Symbol>* ] The symbols in a template can either be placeholders or literal strings. It is worth noting that: placeholders need to be enclosed within the same delimiters (either <...> or [...] ) as the template; literal strings need not not be enclosed within quotation marks; literal strings are tokenized on space characters (whitespace, tab); additionally, literal strings are tokenized on boundaries between characters from the set given by the tokenize option, see the tokenize template option; placeholders translate literally. If a separator containing any layout characters is given, the placeholder maps to a list with separator that strips the layout. An example of a template rule: Exp.Addition = < <Exp> + <Exp> > Here, the + symbol is a literal string and <Exp> is a placeholder for sort Exp . Placeholders are of the form: <Sort?> : optional placeholder <Sort*> : repetition (0...n) <Sort+> : repetition (1...n) <{Sort \",\"}*> : repetition with separator Case-insensitive Literals \u00b6 As we showed before, SDF3 allows defining case-insensitive literals as single-quoted strings in regular productions. For example: Exp.If = 'if' \"(\" Exp \")\" Exp 'else' Exp accepts case-insensitive keywords for if and else such as if , IF , If , else , ELSE or ELsE . However, to generate case-insensitive literals from template productions, it is necessary to add annotate these productions as case-insensitive. For example, a template production: Exp.If = < if(<Exp>) <Exp> else <Exp> > {case-insensitive} accepts the same input as the regular production mentioned before. Moreover, lexical symbols can also be annotated as case-insensitive to parse as such. The constructed abstract syntax tree contains lower-case symbols, but the original term is preserved via origin-tracking. For example: ID = [a-zA-z][a-zA-Z0-9]* {case-insensitive} can parse foo , Foo , FOo , fOo , foO , fOO or FOO . Whichever option generates a node \"foo\" in the abstract syntax tree. By consulting the origin information on this node, it is possible to know which term was used as input to the parser. Template options \u00b6 Template options are options that are applied to the current file. A template options section is structured as follows: template options <TemplateOption*> Multiple template option sections are not supported. If multiple template option sections are specified, the last one is used. There are three kinds of template options. keyword \u00b6 Convenient way for setting up lexical follow restrictions for keywords. See the section on follow restrictions for more information. The structure of the keyword option is as follows: keyword -/- <Pattern> This will add a follow restriction on the pattern for each keyword in the language. Keywords are automatically detected, any terminal that ends with an alphanumeric character is considered a keyword. Multiple keyword options are not supported. If multiple keyword options are specified, the last one is used. Note that this only sets up follow restrictions, rejection of keywords as identifiers still needs to be written manually. tokenize \u00b6 Specifies which characters may have layout around them. The structure of a tokenize option is as follows: tokenize : \"<Character*>\" Consider the following grammar specification: template options tokenize : \"(\" context-free syntax Exp.Call = <<ID>();> Because layout is allowed around the ( and ) characters, there may be layout between () and ; in the template rule. If no tokenize option is specified, it defaults to the default value of () . Multiple tokenize options are not supported. If multiple tokenize options are specified, the last one is used. reject Convenient way for setting up reject rules for keywords. See the section on rejections_ for more information. The structure of the reject option is as follows: Symbol = keyword {attrs} where Symbol is the symbol to generate the rules for. Note that attrs can be include any attribute, but by using reject , reject rules such as ID = \"true\" {reject} are generated for all keywords that appear in the templates. Multiple reject template options are not supported. If multiple reject template options are specified, the last one is used.","title":"Templates"},{"location":"references/syntax/templates/#templates","text":"Templates are a major change in SDF3 when comparing to SDF2. They are essential when aiming to generate a nice pretty printer or generate proper syntactic code completion templates. When generating such artifacts, a general production simply introduces a whitespace in between symbols. For example, when writing a grammar rule Statement.If = \"if\" \"(\" Exp \")\" Exp \"else\" Exp and pretty printing a valid program, we would get the text in a single line separated by spaces, as: Furthermore, code completion would consider the same indentation when inserting code snippets. However, when using template productions such as Statement.If = < if (<Exp>) <Exp> else <Exp>> We would get the following program: Again, code completion would also consider this indentation for proposals. That is, in template productions, the surrounding layout is used to nicely pretty print programs and its code completion suggestions.","title":"Templates"},{"location":"references/syntax/templates/#template-productions","text":"Template productions are an alternative way of defining productions. Similarly, they consist of a left-hand side and a right-hand side separated by = . The left-hand side is the same as for productive rules. The right-hand side is a template delimited by < and > . The template can contain zero or more symbols: <Sort> = < <Symbol>* > <Sort>.<Constructor> = < <Symbol>* > Alternatively, square brackets can be used to delimit a template: <Sort> = [ <Symbol>* ] <Sort>.<Constructor> = [ <Symbol>* ] The symbols in a template can either be placeholders or literal strings. It is worth noting that: placeholders need to be enclosed within the same delimiters (either <...> or [...] ) as the template; literal strings need not not be enclosed within quotation marks; literal strings are tokenized on space characters (whitespace, tab); additionally, literal strings are tokenized on boundaries between characters from the set given by the tokenize option, see the tokenize template option; placeholders translate literally. If a separator containing any layout characters is given, the placeholder maps to a list with separator that strips the layout. An example of a template rule: Exp.Addition = < <Exp> + <Exp> > Here, the + symbol is a literal string and <Exp> is a placeholder for sort Exp . Placeholders are of the form: <Sort?> : optional placeholder <Sort*> : repetition (0...n) <Sort+> : repetition (1...n) <{Sort \",\"}*> : repetition with separator","title":"Template Productions"},{"location":"references/syntax/templates/#case-insensitive-literals","text":"As we showed before, SDF3 allows defining case-insensitive literals as single-quoted strings in regular productions. For example: Exp.If = 'if' \"(\" Exp \")\" Exp 'else' Exp accepts case-insensitive keywords for if and else such as if , IF , If , else , ELSE or ELsE . However, to generate case-insensitive literals from template productions, it is necessary to add annotate these productions as case-insensitive. For example, a template production: Exp.If = < if(<Exp>) <Exp> else <Exp> > {case-insensitive} accepts the same input as the regular production mentioned before. Moreover, lexical symbols can also be annotated as case-insensitive to parse as such. The constructed abstract syntax tree contains lower-case symbols, but the original term is preserved via origin-tracking. For example: ID = [a-zA-z][a-zA-Z0-9]* {case-insensitive} can parse foo , Foo , FOo , fOo , foO , fOO or FOO . Whichever option generates a node \"foo\" in the abstract syntax tree. By consulting the origin information on this node, it is possible to know which term was used as input to the parser.","title":"Case-insensitive Literals"},{"location":"references/syntax/templates/#template-options","text":"Template options are options that are applied to the current file. A template options section is structured as follows: template options <TemplateOption*> Multiple template option sections are not supported. If multiple template option sections are specified, the last one is used. There are three kinds of template options.","title":"Template options"},{"location":"references/syntax/templates/#keyword","text":"Convenient way for setting up lexical follow restrictions for keywords. See the section on follow restrictions for more information. The structure of the keyword option is as follows: keyword -/- <Pattern> This will add a follow restriction on the pattern for each keyword in the language. Keywords are automatically detected, any terminal that ends with an alphanumeric character is considered a keyword. Multiple keyword options are not supported. If multiple keyword options are specified, the last one is used. Note that this only sets up follow restrictions, rejection of keywords as identifiers still needs to be written manually.","title":"keyword"},{"location":"references/syntax/templates/#tokenize","text":"Specifies which characters may have layout around them. The structure of a tokenize option is as follows: tokenize : \"<Character*>\" Consider the following grammar specification: template options tokenize : \"(\" context-free syntax Exp.Call = <<ID>();> Because layout is allowed around the ( and ) characters, there may be layout between () and ; in the template rule. If no tokenize option is specified, it defaults to the default value of () . Multiple tokenize options are not supported. If multiple tokenize options are specified, the last one is used. reject Convenient way for setting up reject rules for keywords. See the section on rejections_ for more information. The structure of the reject option is as follows: Symbol = keyword {attrs} where Symbol is the symbol to generate the rules for. Note that attrs can be include any attribute, but by using reject , reject rules such as ID = \"true\" {reject} are generated for all keywords that appear in the templates. Multiple reject template options are not supported. If multiple reject template options are specified, the last one is used.","title":"tokenize"},{"location":"references/testing/","text":"SPT: Spoofax Testing Language \u00b6","title":"SPT: Spoofax Testing Language"},{"location":"references/testing/#spt-spoofax-testing-language","text":"","title":"SPT: Spoofax Testing Language"},{"location":"tutorials/","text":"Tutorials \u00b6 This page lists tutorials that take you step-by-step through a project to learn a variety of concepts and aspects of Spoofax in a specific scope. For guides on achieving specific tasks, see the How To's section. For the Spoofax language reference, see the References section. No tutorials yet.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"This page lists tutorials that take you step-by-step through a project to learn a variety of concepts and aspects of Spoofax in a specific scope. For guides on achieving specific tasks, see the How To's section. For the Spoofax language reference, see the References section. No tutorials yet.","title":"Tutorials"}]}